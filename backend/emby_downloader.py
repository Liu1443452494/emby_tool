# backend/emby_downloader.py (ä¿®æ”¹å)

import os
import requests
import json
import logging
import threading
import re
from datetime import datetime
from xml.sax.saxutils import escape
from typing import Dict, Any, Optional

from log_manager import ui_logger
from models import AppConfig, BatchDownloadRequest, DownloadConfig
from task_manager import TaskManager

def create_nfo_from_details(details: dict, download_config: DownloadConfig) -> str:
    """
    æ ¹æ®ä» Emby è·å–çš„ã€ç»è¿‡å¤„ç†çš„è¯¦ç»†ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„ NFO æ–‡ä»¶ã€‚
    (æ­¤å‡½æ•°ä¿æŒä¸å˜)
    """
    item_type = details.get("Type", "Movie")
    root_tag = "tvshow" if item_type == "Series" else "movie"
    
    xml_parts = [
        '<?xml version="1.0" encoding="UTF-8" standalone="yes" ?>',
        f'<{root_tag}>'
    ]

    def append_if_exists(tag, value, is_cdata=False):
        if value:
            if is_cdata:
                xml_parts.append(f"    <{tag}><![CDATA[{value}]]></{tag}>")
            else:
                xml_parts.append(f"    <{tag}>{escape(str(value))}</{tag}>")

    taglines = details.get("Taglines") or []
    first_tagline = taglines[0] if taglines else None
    
    overview = details.get("Overview", "")
    outline = first_tagline or overview
    append_if_exists("plot", overview, is_cdata=True)
    append_if_exists("outline", outline, is_cdata=True)
    
    xml_parts.append("    <lockdata>false</lockdata>")

    if date_added_str := details.get('DateCreated'):
        try:
            date_added_obj = datetime.fromisoformat(date_added_str.replace('Z', '+00:00'))
            append_if_exists("dateadded", date_added_obj.strftime('%Y-%m-%d %H:%M:%S'))
        except (ValueError, TypeError): pass

    append_if_exists("title", details.get("Name", ""))
    append_if_exists("originaltitle", details.get("OriginalTitle", ""))
    if item_type == "Movie":
        append_if_exists("sorttitle", details.get("SortName", ""))
    else:
        append_if_exists("showtitle", details.get("Name", ""))

    all_people = details.get('People', []) or []
    all_actors = [p for p in all_people if p and p.get('Type') == 'Actor']
    other_people = [p for p in all_people if p and p.get('Type') != 'Actor']
    
    limited_actors = all_actors[:download_config.nfo_actor_limit]
    logging.info(f"     -- NFOç”Ÿæˆï¼šå…± {len(all_actors)} ä½æ¼”å‘˜ï¼Œå°†å†™å…¥å‰ {len(limited_actors)} ä½ã€‚")
    people_to_process = limited_actors + other_people

    for person in people_to_process:
        if not (person and person.get('Name')): continue
        p_type = person.get('Type')
        person_ids = person.get('ProviderIds', {}) or {}
        
        tmdbid = next((v for k, v in person_ids.items() if k.lower() == 'tmdb'), None)
        imdbid = next((v for k, v in person_ids.items() if k.lower() == 'imdb'), None)

        if p_type == 'Actor':
            actor_parts = ["    <actor>", f"        <name>{escape(person['Name'])}</name>", f"        <role>{escape(person.get('Role', ''))}</role>", "        <type>Actor</type>"]
            if tmdbid: actor_parts.append(f"        <tmdbid>{tmdbid}</tmdbid>")
            if imdbid: actor_parts.append(f"        <imdbid>{imdbid}</imdbid>")
            actor_parts.append("    </actor>")
            xml_parts.append('\n'.join(actor_parts))
        elif p_type == 'Director':
            director_attrs = []
            if tmdbid: director_attrs.append(f'tmdbid="{tmdbid}"')
            if imdbid: director_attrs.append(f'imdbid="{imdbid}"')
            attr_str = " " + " ".join(director_attrs) if director_attrs else ""
            xml_parts.append(f"    <director{attr_str}>{escape(person['Name'])}</director>")
        elif p_type == 'Writer':
            xml_parts.append(f"    <writer>{escape(person['Name'])}</writer>")

    append_if_exists("rating", details.get("CommunityRating"))
    append_if_exists("year", details.get("ProductionYear"))
    append_if_exists("mpaa", details.get("OfficialRating"))

    if premiere_date_str := details.get("PremiereDate"):
        try:
            premiere_date_obj = datetime.fromisoformat(premiere_date_str.replace('Z', '+00:00'))
            append_if_exists("premiered", premiere_date_obj.strftime('%Y-%m-%d'))
            append_if_exists("releasedate", premiere_date_obj.strftime('%Y-%m-%d'))
        except (ValueError, TypeError): pass
    
    append_if_exists("tagline", first_tagline)

    provider_ids = details.get('ProviderIds', {}) or {}
    imdb_id = next((v for k, v in provider_ids.items() if k.lower() == 'imdb'), None)
    tmdb_id = next((v for k, v in provider_ids.items() if k.lower() == 'tmdb'), None)
    douban_id = next((v for k, v in provider_ids.items() if k.lower() == 'douban'), None)

    if imdb_id:
        append_if_exists("id", imdb_id)
        xml_parts.append(f'    <uniqueid type="imdb">{imdb_id}</uniqueid>')
    if tmdb_id:
        xml_parts.append(f'    <uniqueid type="tmdb">{tmdb_id}</uniqueid>')
    if douban_id:
        xml_parts.append(f'    <uniqueid type="douban">{douban_id}</uniqueid>')
        append_if_exists("doubanid", douban_id)

    for genre in (details.get('Genres', []) or []):
        append_if_exists("genre", genre)
    for country in (details.get('ProductionLocations', []) or []):
        append_if_exists("country", country)

    if collection_info := details.get('CollectionInfo'):
        if collection_info.get("tmdbcolid") and collection_info.get("name"):
            set_parts = [f'    <set tmdbcolid="{collection_info["tmdbcolid"]}">']
            set_parts.append(f'        <name>{escape(collection_info["name"])}</name>')
            set_parts.append('    </set>')
            xml_parts.append('\n'.join(set_parts))

    xml_parts.append(f'</{root_tag}>')
    return '\n'.join(xml_parts)


class EmbyDownloader:
    def __init__(self, app_config: AppConfig, task_category: str = "åª’ä½“ä¸‹è½½"):
        self.server_config = app_config.server_config
        self.download_config = app_config.download_config
        self.base_url = self.server_config.server
        self.api_key = self.server_config.api_key
        self.user_id = self.server_config.user_id
        self.params = {"api_key": self.api_key}
        self.session = requests.Session()
        self.task_category = task_category


    def _get_item_details(self, item_id: str, fields: str) -> Optional[Dict[str, Any]]:
        """è·å–åª’ä½“é¡¹çš„åŸºç¡€ä¿¡æ¯"""
        url = f"{self.base_url}/Users/{self.user_id}/Items/{item_id}"
        params = {**self.params, "Fields": fields}
        try:
            response = self.session.get(url, params=params, timeout=20)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            # --- æ—¥å¿—ä¿®æ”¹ ---
            ui_logger.error(f"âŒ è·å–åª’ä½“é¡¹(ID:{item_id})ä¿¡æ¯å¤±è´¥: {e}", task_category=self.task_category)
            return None


    def _get_full_item_details(self, item_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä¸€ä¸ªåª’ä½“é¡¹çš„æ‰€æœ‰éœ€è¦ç”¨äºç”ŸæˆNFOçš„è¯¦ç»†ä¿¡æ¯"""
        # --- æ—¥å¿—ä¿®æ”¹ ---
        ui_logger.debug(f"â¡ï¸ æ­£åœ¨è·å–åª’ä½“é¡¹ (ID: {item_id}) çš„å®Œæ•´è¯¦æƒ…...", task_category=self.task_category)
        
        fields = "ProviderIds,People,Genres,Overview,OriginalTitle,SortName,DateCreated,ProductionYear,CommunityRating,Path,OfficialRating,PremiereDate,Taglines,ProductionLocations"
        details = self._get_item_details(item_id, fields)
        if not details:
            ui_logger.error(f"âŒ è·å–åª’ä½“é¡¹åŸºç¡€ä¿¡æ¯å¤±è´¥ã€‚ä»»åŠ¡ä¸­æ­¢ã€‚", task_category=self.task_category)
            return None

        if 'People' in details and details['People']:
            all_people = details.get('People', [])
            all_actors = [p for p in all_people if p and p.get('Type') == 'Actor']
            other_people = [p for p in all_people if p and p.get('Type') != 'Actor']
            
            actor_limit = self.download_config.nfo_actor_limit
            limited_actors = all_actors[:actor_limit]
            
            people_to_fetch_ids = limited_actors + other_people
            
            ui_logger.debug(f"  - æ¼”èŒå‘˜æ€»æ•°: {len(all_people)} (æ¼”å‘˜: {len(all_actors)})ã€‚æ ¹æ®é…ç½®ï¼Œå°†ä¸º {len(people_to_fetch_ids)} ä½å…³é”®æ¼”èŒå‘˜è·å–å¤–éƒ¨ID...", task_category=self.task_category)

            for person in people_to_fetch_ids:
                person_id = person.get('Id')
                if not person_id: continue
                
                person_details = self._get_item_details(person_id, "ProviderIds")
                if person_details:
                    person['ProviderIds'] = person_details.get('ProviderIds', {})
                else:
                    ui_logger.warning(f"  - âš ï¸ è·å–æ¼”èŒå‘˜(ID: {person_id})çš„è¯¦æƒ…å¤±è´¥ã€‚è·³è¿‡æ­¤äººã€‚", task_category=self.task_category)
                    person['ProviderIds'] = {}
            
            details['People'] = people_to_fetch_ids

        ui_logger.debug(f"â¡ï¸ æ­£åœ¨æŸ¥è¯¢é¡¹ç›® (ID: {item_id}) æ‰€å±çš„åˆé›†...", task_category=self.task_category)
        collections_url = f"{self.base_url}/Users/{self.user_id}/Items"
        collection_params = {
            **self.params,
            "ListItemIds": item_id,
            "IncludeItemTypes": "BoxSet",
            "Recursive": "true",
            "Fields": "ProviderIds"
        }
        try:
            c_resp = self.session.get(collections_url, params=collection_params, timeout=15)
            c_resp.raise_for_status()
            collections_summary = c_resp.json().get("Items", [])
            
            if not collections_summary:
                ui_logger.debug("  - è¯¥é¡¹ç›®ä¸å±äºä»»ä½•åˆé›†ã€‚", task_category=self.task_category)
            else:
                boxset_summary = collections_summary[0]
                collection_id = boxset_summary.get('Id')
                collection_name = boxset_summary.get('Name', 'ç”µå½±åˆé›†')
                
                if collection_id:
                    ui_logger.debug(f"  - æ‰¾åˆ°æ‰€å±åˆé›†: '{collection_name}' (ID: {collection_id})ã€‚æ­£åœ¨è·å–å…¶ProviderIds...", task_category=self.task_category)
                    
                    collection_details = self._get_item_details(collection_id, "ProviderIds")
                    if collection_details:
                        collection_ids = collection_details.get('ProviderIds', {})
                        tmdb_col_id = next((v for k, v in collection_ids.items() if k.lower() == 'tmdb'), None)

                        if tmdb_col_id:
                            ui_logger.debug(f"  - âœ… æˆåŠŸæ‰¾åˆ° TMDB åˆé›† ID: {tmdb_col_id}", task_category=self.task_category)
                            details['CollectionInfo'] = {"name": collection_name, "tmdbcolid": tmdb_col_id}
                        else:
                            ui_logger.warning("  - âš ï¸ æœªåœ¨è¯¥åˆé›†çš„ ProviderIds ä¸­æ‰¾åˆ° TMDB åˆé›† IDã€‚", task_category=self.task_category)
        except requests.RequestException as e:
            ui_logger.warning(f"  - âš ï¸ è·å–åˆé›†åˆ—è¡¨å¤±è´¥ï¼Œå°†ä¸å†™å…¥åˆé›†ä¿¡æ¯: {e}", task_category=self.task_category)

        ui_logger.debug(f"â¡ï¸ å®Œæ•´è¯¦æƒ…è·å–å®Œæ¯•ã€‚", task_category=self.task_category)
        return details


    def _download_file(self, url: str, save_path: str, item_name: str, content_name: str):
        if os.path.exists(save_path) and self.download_config.download_behavior == "skip":
            return "skipped"
        response = requests.get(url, params=self.params, stream=True, timeout=60)
        response.raise_for_status()
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        return "success"

    def _sanitize_path(self, path: str) -> str:
        return re.sub(r'[<>:"/\\|?*]', '_', path)

    def _get_relative_path(self, path: str) -> str:
        path = re.sub(r'^[a-zA-Z]:', '', path)
        return path.lstrip('/\\')



    def download_for_item(self, item_id: str, content_types: list[str]):
        if 'nfo' in content_types:
            details = self._get_full_item_details(item_id)
        else:
            fields_for_images = "ProviderIds,Path,Name,Type,ImageTags,BackdropImageTags"
            details = self._get_item_details(item_id, fields_for_images)

        if not details:
            ui_logger.error(f"âŒ æ— æ³•è·å–é¡¹ç›® {item_id} çš„ä¿¡æ¯ï¼Œä¸‹è½½ä»»åŠ¡ä¸­æ­¢ã€‚", task_category=self.task_category)
            return {"error": "Failed to get item details"}

        item_name = details.get("Name", f"Item {item_id}")
        
        save_dir = ""
        rule = self.download_config.directory_naming_rule

        if rule == "media_path":
            ui_logger.debug(f"[{item_name}] ä½¿ç”¨ 'media_path' è§„åˆ™åˆ›å»ºç›®å½•ã€‚", task_category=self.task_category)
            media_path = details.get("Path")
            if not media_path:
                raise ValueError(f"é¡¹ç›® '{item_name}' (ID: {item_id}) ç¼ºå°‘ Path å­—æ®µï¼Œæ— æ³•æŒ‰åª’ä½“è·¯å¾„åˆ›å»ºç›®å½•ã€‚")
            
            relative_dir = os.path.dirname(media_path) if details.get("Type") == "Movie" else media_path
            
            uncleaned_relative_path = self._get_relative_path(relative_dir)
            
            normalized_path = uncleaned_relative_path.replace('\\', '/')
            
            path_parts = normalized_path.split('/')
            
            safe_path_parts = [self._sanitize_path(part) for part in path_parts]
            
            save_dir = os.path.join(self.download_config.download_directory, *safe_path_parts)
            
        else: # tmdb_id è§„åˆ™
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šå¢åŠ åª’ä½“ç±»å‹å‰ç¼€ ---
            provider_ids = details.get("ProviderIds", {})
            tmdb_id = next((v for k, v in provider_ids.items() if k.lower() == 'tmdb'), None)
            if not tmdb_id:
                raise ValueError(f"é¡¹ç›® '{item_name}' (ID: {item_id}) ç¼ºå°‘ TMDB IDï¼Œæ— æ³•åˆ›å»ºç›®å½•ã€‚")

            item_type = details.get("Type")
            prefix = 'tv-' if item_type == 'Series' else 'movie-'
            folder_name = f"{prefix}{tmdb_id}"
            
            ui_logger.debug(f"[{item_name}] ä½¿ç”¨ 'tmdb_id' è§„åˆ™åˆ›å»ºç›®å½•ï¼Œç±»å‹: {item_type}, æ–‡ä»¶å¤¹å: {folder_name}", task_category=self.task_category)
            save_dir = os.path.join(self.download_config.download_directory, folder_name)

        os.makedirs(save_dir, exist_ok=True)
        ui_logger.debug(f"[{item_name}] æœ€ç»ˆæ–‡ä»¶ä¿å­˜ç›®å½•ä¸º: {save_dir}", task_category=self.task_category)

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ—¥å¿—èšåˆé€»è¾‘ ---
        results = {}
        log_details = []
        has_error = False

        if 'nfo' in content_types:
            try:
                nfo_content = create_nfo_from_details(details, self.download_config)
                filename = "movie.nfo" if details.get("Type") == "Movie" else "tvshow.nfo"
                save_path = os.path.join(save_dir, filename)
                if os.path.exists(save_path) and self.download_config.download_behavior == "skip":
                    results['nfo'] = 'skipped'
                    log_details.append(f"  - NFO æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
                else:
                    with open(save_path, 'w', encoding='utf-8') as f: f.write(nfo_content)
                    results['nfo'] = 'success'
                    log_details.append(f"  - NFO æ–‡ä»¶åˆ›å»ºæˆåŠŸ: {filename}")
            except Exception as e: 
                results['nfo'] = f"Error: {e}"
                log_details.append(f"  - âŒ NFO æ–‡ä»¶åˆ›å»ºå¤±è´¥: {e}")
                has_error = True
        
        image_map = {
            "poster": ("Primary", "æµ·æŠ¥", "poster.jpg"), 
            "logo": ("Logo", "Logo", "clearlogo.png"), 
            "backdrop": ("Backdrop", "èƒŒæ™¯å›¾", "fanart.jpg")
        }
        for content_type, (emby_type, content_name, save_filename) in image_map.items():
            if content_type not in content_types: continue
            try:
                if emby_type == "Backdrop":
                    if not details.get("BackdropImageTags"):
                        results[content_type] = 'not_found'
                        log_details.append(f"  - {content_name}ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
                        continue
                    img_url = f"{self.base_url}/Items/{item_id}/Images/Backdrop/0"
                else:
                    if not details.get("ImageTags", {}).get(emby_type):
                        results[content_type] = 'not_found'
                        log_details.append(f"  - {content_name}ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
                        continue
                    img_url = f"{self.base_url}/Items/{item_id}/Images/{emby_type}"
                
                save_path = os.path.join(save_dir, save_filename)
                status = self._download_file(img_url, save_path, item_name, content_name)
                results[content_type] = status
                
                if status == 'success':
                    log_details.append(f"  - {content_name}ä¸‹è½½æˆåŠŸ: {save_filename}")
                elif status == 'skipped':
                    log_details.append(f"  - {content_name}å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")

            except Exception as e: 
                results[content_type] = f"Error: {e}"
                log_details.append(f"  - âŒ {content_name}ä¸‹è½½å¤±è´¥: {e}")
                has_error = True
        
        # ç»Ÿä¸€è¾“å‡ºæ—¥å¿—
        if log_details:
            log_icon = "âš ï¸" if has_error else "âœ…"
            log_message = f"{log_icon} [{item_name}]\n" + "\n".join(log_details)
            ui_logger.info(log_message, task_category=self.task_category)

        return results


def batch_download_task(config: AppConfig, request: BatchDownloadRequest, cancellation_event: threading.Event, task_id: str, task_manager: TaskManager):
    task_cat = f"æ‰¹é‡ä¸‹è½½({request.mode})"
    downloader = EmbyDownloader(config, task_category=task_cat)
    server_conf = config.server_config
    all_items_to_process = []
    
    # --- æ—¥å¿—ä¿®æ”¹ ---
    ui_logger.info(f"ğŸ‰ æ‰¹é‡ä¸‹è½½ä»»åŠ¡å¯åŠ¨ï¼Œæ¨¡å¼: {request.mode}", task_category=task_cat)

    fields_to_get = "Id,ParentId,Name"
    if request.mode == "byLibrary":
        if not request.library_ids: 
            ui_logger.error("âŒ æ¨¡å¼ 'byLibrary' éœ€è¦æä¾› library_idsï¼Œä»»åŠ¡ä¸­æ­¢ã€‚", task_category=task_cat)
            return
        ui_logger.info(f"â¡ï¸ å°†åœ¨åª’ä½“åº“ {request.library_ids} ä¸­ä¸‹è½½æ‰€æœ‰ç”µå½±å’Œç”µè§†å‰§", task_category=task_cat)
        for lib_id in request.library_ids:
            url = f"{server_conf.server}/Items"
            params = {"api_key": server_conf.api_key, "UserId": server_conf.user_id, "Recursive": "true", "ParentId": lib_id, "IncludeItemTypes": "Movie,Series", "Fields": fields_to_get}
            start_index = 0
            while True:
                if cancellation_event.is_set(): 
                    ui_logger.info("âš ï¸ ä»»åŠ¡åœ¨è·å–åˆ—è¡¨é˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                    return
                params["StartIndex"] = start_index
                response = requests.get(url, params=params, timeout=30)
                response.raise_for_status()
                items_page = response.json().get("Items", [])
                if not items_page: break
                all_items_to_process.extend(items_page)
                start_index += len(items_page)
    elif request.mode == "byType":
        if not request.media_type: 
            ui_logger.error("âŒ æ¨¡å¼ 'byType' éœ€è¦æä¾› media_typeï¼Œä»»åŠ¡ä¸­æ­¢ã€‚", task_category=task_cat)
            return
        ui_logger.info(f"â¡ï¸ å°†åœ¨æ‰€æœ‰åª’ä½“åº“ä¸­ä¸‹è½½ {request.media_type} ç±»å‹", task_category=task_cat)
        url = f"{server_conf.server}/Items"
        params = {"api_key": server_conf.api_key, "UserId": server_conf.user_id, "Recursive": "true", "IncludeItemTypes": request.media_type, "Fields": fields_to_get}
        start_index = 0
        while True:
            if cancellation_event.is_set(): 
                ui_logger.info("âš ï¸ ä»»åŠ¡åœ¨è·å–åˆ—è¡¨é˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                return
            params["StartIndex"] = start_index
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            items_page = response.json().get("Items", [])
            if not items_page: break
            all_items_to_process.extend(items_page)
            start_index += len(items_page)
    elif request.mode == "all":
        ui_logger.info("â¡ï¸ å°†ä¸‹è½½æ‰€æœ‰åª’ä½“åº“ä¸­çš„ 'Movie' å’Œ 'Series' ç±»å‹", task_category=task_cat)
        url = f"{server_conf.server}/Items"
        params = {"api_key": server_conf.api_key, "UserId": server_conf.user_id, "Recursive": "true", "IncludeItemTypes": "Movie,Series", "Fields": fields_to_get}
        all_items = []
        start_index = 0
        while True:
            if cancellation_event.is_set(): 
                ui_logger.info("âš ï¸ ä»»åŠ¡åœ¨è·å–åˆ—è¡¨é˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                return
            params["StartIndex"] = start_index
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            items_page = response.json().get("Items", [])
            if not items_page: break
            all_items.extend(items_page)
            start_index += len(items_page)
        if request.blacklist:
            views_url = f"{server_conf.server}/Users/{server_conf.user_id}/Views"
            views_resp = requests.get(views_url, params={"api_key": server_conf.api_key})
            views = views_resp.json().get("Items", [])
            blacklist_names = {name.strip() for name in request.blacklist.split(',') if name.strip()}
            blacklisted_ids = {view['Id'] for view in views if view['Name'] in blacklist_names}
            ui_logger.info(f"  - åº”ç”¨é»‘åå•åª’ä½“åº“: {blacklist_names}, å¯¹åº”ID: {blacklisted_ids}", task_category=task_cat)
            all_items_to_process = [item for item in all_items if item.get("ParentId") not in blacklisted_ids]
        else:
            all_items_to_process = all_items

    total_count = len(all_items_to_process)
    ui_logger.info(f"âœ… ä»»åŠ¡å‡†å¤‡å°±ç»ªï¼Œå…±éœ€å¤„ç† {total_count} ä¸ªé¡¹ç›®ã€‚", task_category=task_cat)
    task_manager.update_task_progress(task_id, 0, total_count)

    for i, item in enumerate(all_items_to_process):
        if cancellation_event.is_set():
            ui_logger.info(f"âš ï¸ ä»»åŠ¡åœ¨å¤„ç†ç¬¬ {i+1} ä¸ªé¡¹ç›®æ—¶è¢«å–æ¶ˆã€‚", task_category=task_cat)
            return
        
        item_name_log = item.get('Name', f"Item {item['Id']}")
        ui_logger.debug(f"â¡ï¸ è¿›åº¦ {i+1}/{total_count}: æ­£åœ¨å¤„ç† [{item_name_log}]", task_category=task_cat)
        
        try:
            downloader.download_for_item(item['Id'], request.content_types)
        except Exception as e:
            ui_logger.error(f"âŒ å¤„ç†é¡¹ç›® [{item_name_log}] (ID: {item['Id']}) æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}", task_category=task_cat)
        
        task_manager.update_task_progress(task_id, i + 1, total_count)
    
    if not cancellation_event.is_set():
        ui_logger.info("ğŸ‰ æ‰¹é‡ä¸‹è½½ä»»åŠ¡æ­£å¸¸å®Œæˆã€‚", task_category=task_cat)