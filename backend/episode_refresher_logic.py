import logging
import threading
import time
import requests
import base64
import re
import subprocess
import json
import os
import shutil
from typing import List, Iterable, Optional, Dict, Tuple
from collections import defaultdict
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from filelock import FileLock, Timeout
from datetime import datetime 

try:
    import cv2
    import numpy as np
    from PIL import Image
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False
    logging.warning("ã€å‰§é›†åˆ·æ–°ã€‘OpenCV æˆ– NumPy æœªå®‰è£…ï¼Œé«˜è´¨é‡æˆªå›¾åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ 'pip install opencv-python-headless numpy Pillow'")

from log_manager import ui_logger
from models import AppConfig, EpisodeRefresherConfig
from task_manager import TaskManager
from tmdb_logic import TmdbLogic

# --- æ–°å¢å¸¸é‡ ---
GITHUB_DB_CACHE_FILE = os.path.join('/app/data', 'github_database_cache.json')
GITHUB_DB_CACHE_DURATION = 3600  # ç¼“å­˜1å°æ—¶

class EpisodeRefresherLogic:
    @staticmethod
    def _sanitize_filename(name: str) -> str:
        """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        if not name:
            return "Unknown"
        return re.sub(r'[\\/*?:"<>|]', '_', name)
    
    def _find_screenshot_cache_dir_by_tmdbid(self, series_tmdb_id: str) -> Optional[str]:
        """é€šè¿‡ TMDB ID æŸ¥æ‰¾å®é™…å­˜åœ¨çš„ç¼“å­˜ç›®å½•è·¯å¾„ï¼Œå¿½ç•¥æ ‡é¢˜ã€‚"""
        douban_data_root = self.app_config.douban_config.directory
        if not douban_data_root:
            return None
        
        base_cache_dir = os.path.join(douban_data_root, "EpisodeScreenshots")
        if not os.path.isdir(base_cache_dir):
            return None

        id_pattern = re.compile(r'\[(\d+)\]$')

        try:
            for dirname in os.listdir(base_cache_dir):
                dirpath = os.path.join(base_cache_dir, dirname)
                if os.path.isdir(dirpath):
                    match = id_pattern.search(dirname)
                    if match and match.group(1) == str(series_tmdb_id):
                        return dirpath
        except OSError as e:
            ui_logger.error(f"ã€æˆªå›¾ç¼“å­˜ã€‘æ‰«æç¼“å­˜ç›®å½•æ—¶å‡ºé”™: {e}", task_category="æˆªå›¾ç¼“å­˜")
        
        return None
    
    def __init__(self, app_config: AppConfig):
        self.app_config = app_config
        self.server_config = app_config.server_config
        self.base_url = self.server_config.server
        self.api_key = self.server_config.api_key
        self.user_id = self.server_config.user_id
        self.params = {"api_key": self.api_key}
        self.session = requests.Session()
        self.tmdb_logic = TmdbLogic(app_config)
        self.ffmpeg_available = shutil.which('ffmpeg') is not None and shutil.which('ffprobe') is not None

    @staticmethod
    def _is_generic_episode_title(text: str) -> bool:
        if not text: return True
        pattern = re.compile(r'^(ç¬¬\s*\d+\s*é›†|Episode\s*\d+)$', re.IGNORECASE)
        return bool(pattern.match(text.strip()))
    
    def _get_emby_item_details(self, item_id: str, fields: str) -> Optional[Dict]:
        """è·å–åª’ä½“é¡¹çš„è¯¦ç»†ä¿¡æ¯"""
        try:
            url = f"{self.base_url}/Users/{self.user_id}/Items/{item_id}"
            params = {**self.params, "Fields": fields}
            response = self.session.get(url, params=params, timeout=15)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logging.error(f"ã€å‰§é›†åˆ·æ–°ã€‘è·å–åª’ä½“è¯¦æƒ… (ID: {item_id}) å¤±è´¥: {e}")
            return None

    def _unlock_item(self, item_id: str, task_category: str) -> bool:
        try:
            details_url = f"{self.base_url}/Users/{self.user_id}/Items/{item_id}"
            details_resp = self.session.get(details_url, params=self.params, timeout=15)
            details_resp.raise_for_status()
            item_details = details_resp.json()
            if item_details.get("LockedFields") and len(item_details["LockedFields"]) > 0:
                ui_logger.debug(f"     - [è§£é”] æ£€æµ‹åˆ°é”å®šçš„å­—æ®µ: {item_details['LockedFields']}ï¼Œæ­£åœ¨è§£é”...", task_category=task_category)
                item_details["LockedFields"] = []
                update_url = f"{self.base_url}/Items/{item_id}"
                headers = {'Content-Type': 'application/json'}
                update_resp = self.session.post(update_url, params=self.params, json=item_details, headers=headers, timeout=20)
                update_resp.raise_for_status()
                ui_logger.debug(f"     - [è§£é”] åª’ä½“é¡¹ (ID: {item_id}) å·²æˆåŠŸè§£é”ã€‚", task_category=task_category)
            else:
                ui_logger.debug(f"     - [è§£é”] åª’ä½“é¡¹ (ID: {item_id}) æ— éœ€è§£é”ã€‚", task_category=task_category)
            return True
        except requests.RequestException as e:
            ui_logger.error(f"     - [è§£é”] è§£é”åª’ä½“é¡¹ (ID: {item_id}) æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}", task_category=task_category)
            return False
        except Exception as e:
            ui_logger.error(f"     - [è§£é”] è§£é”åª’ä½“é¡¹ (ID: {item_id}) æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", task_category=task_category, exc_info=True)
            return False

    def _refresh_single_episode_by_emby(self, episode_id: str, config: EpisodeRefresherConfig, task_category: str) -> bool:
        try:
            if not self._unlock_item(episode_id, task_category):
                ui_logger.warning(f"     - è§£é”åª’ä½“é¡¹ (ID: {episode_id}) å¤±è´¥ï¼Œåˆ·æ–°å¯èƒ½ä¸ä¼šç”Ÿæ•ˆã€‚", task_category=task_category)
            
            url = f"{self.base_url}/Items/{episode_id}/Refresh"
            params = {
                **self.params,
                "Recursive": "true",
                "MetadataRefreshMode": "FullRefresh",
                "ImageRefreshMode": "Default",
                "ReplaceAllMetadata": str(config.overwrite_metadata).lower(),
                "ReplaceAllImages": "false"
            }
            ui_logger.debug(f"     - [Embyæ¨¡å¼] å‘é€åˆ·æ–°è¯·æ±‚åˆ°: {url} (å›¾ç‰‡æ¨¡å¼: Default)", task_category=task_category)
            
            response = self.session.post(url, params=params, timeout=30)
            response.raise_for_status()
            
            if response.status_code == 204:
                ui_logger.debug(f"     - [Embyæ¨¡å¼] Emby æœåŠ¡å™¨å·²æˆåŠŸæ¥æ”¶åˆ·æ–°è¯·æ±‚ (ID: {episode_id})ã€‚", task_category=task_category)
                return True
            else:
                ui_logger.warning(f"     - [Embyæ¨¡å¼] Emby æœåŠ¡å™¨è¿”å›å¼‚å¸¸çŠ¶æ€ç  {response.status_code} (ID: {episode_id})ã€‚", task_category=task_category)
                return False

        except requests.RequestException as e:
            ui_logger.error(f"     - [Embyæ¨¡å¼] åˆ·æ–°åˆ†é›† (ID: {episode_id}) æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}", task_category=task_category)
            return False

    def _upload_image_from_url(self, item_id: str, image_url: str, task_category: str) -> bool:
        try:
            ui_logger.debug(f"     - [æ‰§è¡Œ] æ­£åœ¨ä»URLä¸‹è½½å›¾ç‰‡: {image_url}", task_category=task_category)
            proxies = self.tmdb_logic.proxy_manager.get_proxies(image_url)
            image_response = self.tmdb_logic.session.get(image_url, timeout=30, proxies=proxies)
            image_response.raise_for_status()
            image_data = image_response.content
            content_type = image_response.headers.get('Content-Type', 'image/jpeg')
            return self._upload_image_bytes(item_id, image_data, content_type, task_category)
        except Exception as e:
            ui_logger.error(f"     - [å¤±è´¥âŒ] ä»URLä¸‹è½½å¹¶ä¸Šä¼ å›¾ç‰‡åˆ° Emby (ID: {item_id}) å¤±è´¥: {e}", task_category=task_category, exc_info=True)
            return False

    def _upload_image_bytes(self, item_id: str, image_data: bytes, content_type: str, task_category: str) -> bool:
        try:
            upload_url = f"{self.base_url}/Items/{item_id}/Images/Primary"
            
            try:
                self.session.delete(upload_url, params=self.params, timeout=20)
            except requests.RequestException as e:
                ui_logger.debug(f"     - [æ‰§è¡Œ] åˆ é™¤æ—§ä¸»å›¾æ—¶å‘ç”Ÿé”™è¯¯ï¼ˆå¯èƒ½æ˜¯æ­£å¸¸çš„ï¼‰: {e}", task_category=task_category)

            base64_encoded_data = base64.b64encode(image_data).decode('utf-8')
            
            headers = {'Content-Type': content_type}
            
            ui_logger.debug(f"     - [æ‰§è¡Œ] æ­£åœ¨ä¸Šä¼ å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®åˆ° Emby...", task_category=task_category)
            upload_response = self.session.post(
                upload_url, 
                params=self.params, 
                data=base64_encoded_data,
                headers=headers, 
                timeout=60
            )
            upload_response.raise_for_status()
            return True
        except Exception as e:
            ui_logger.error(f"     - [å¤±è´¥âŒ] ä¸Šä¼ å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®åˆ° Emby (ID: {item_id}) å¤±è´¥: {e}", task_category=task_category, exc_info=True)
            return False

    def _get_video_url_from_item(self, episode_id: str, task_category: str) -> Optional[str]:
        try:
            details_url = f"{self.base_url}/Users/{self.user_id}/Items/{episode_id}"
            details_params = {**self.params, "Fields": "MediaSources"}
            details_resp = self.session.get(details_url, params=details_params, timeout=15)
            details_resp.raise_for_status()
            item_details = details_resp.json()

            media_sources = item_details.get("MediaSources")
            if not media_sources or not isinstance(media_sources, list) or len(media_sources) == 0:
                ui_logger.warning(f"     - [æˆªå›¾] åª’ä½“é¡¹ (ID: {episode_id}) çš„ MediaSources å­—æ®µä¸ºç©ºæˆ–æ— æ•ˆï¼Œæ— æ³•è·å–æµåª’ä½“åœ°å€ã€‚", task_category=task_category)
                return None

            first_source = media_sources[0]
            video_url = first_source.get("DirectStreamUrl") or first_source.get("Path")

            if not video_url:
                ui_logger.warning(f"     - [æˆªå›¾] åœ¨ MediaSources ä¸­æ—¢æœªæ‰¾åˆ° DirectStreamUrl ä¹Ÿæœªæ‰¾åˆ° Path å­—æ®µï¼Œæ— æ³•æˆªå›¾ã€‚", task_category=task_category)
                return None
            
            if not video_url.startswith(('http://', 'https://')):
                ui_logger.error(f"     - [æˆªå›¾] ä» MediaSources è·å–åˆ°çš„æµåª’ä½“åœ°å€ä¸æ˜¯æœ‰æ•ˆçš„URL: {video_url}", task_category=task_category)
                return None
            
            ui_logger.debug(f"     - [æˆªå›¾] æˆåŠŸè·å–åˆ°æµåª’ä½“ URL: {video_url}", task_category=task_category)
            return video_url
            
        except Exception as e:
            ui_logger.error(f"     - [æˆªå›¾] è·å–åª’ä½“é¡¹æµåª’ä½“ URL æ—¶å‡ºé”™: {e}", task_category=task_category, exc_info=True)
            return None

    def _get_video_duration(self, video_url: str, task_category: str) -> Optional[float]:
        try:
            command = [
                'ffprobe',
                '-v', 'quiet',
                '-print_format', 'json',
                '-show_format',
                '-i', video_url
            ]
            ui_logger.debug(f"     - [æˆªå›¾] æ­£åœ¨æ‰§è¡Œ ffprobe å‘½ä»¤è·å–è§†é¢‘æ—¶é•¿...", task_category=task_category)
            result = subprocess.run(command, capture_output=True, text=True, check=True, timeout=60)
            format_info = json.loads(result.stdout).get('format', {})
            duration_str = format_info.get('duration')
            if duration_str:
                return float(duration_str)
            ui_logger.warning("     - [æˆªå›¾] ffprobe æœªè¿”å›æ—¶é•¿ä¿¡æ¯ã€‚", task_category=task_category)
            return None
        except subprocess.TimeoutExpired:
            ui_logger.error("     - [æˆªå›¾] ffprobe è·å–æ—¶é•¿è¶…æ—¶ã€‚", task_category=task_category)
            return None
        except (subprocess.CalledProcessError, json.JSONDecodeError, ValueError) as e:
            ui_logger.error(f"     - [æˆªå›¾] ffprobe è·å–æ—¶é•¿å¤±è´¥: {e}", task_category=task_category)
            return None
        
    def _get_local_screenshot_path(self, series_tmdb_id: str, season_number: int, episode_number: int, series_name: str) -> Optional[str]:
        """æ ¹æ®å‰§é›†ä¿¡æ¯ç”Ÿæˆæœ¬åœ°æˆªå›¾ç¼“å­˜çš„å®Œæ•´è·¯å¾„ (ç”¨äºå†™å…¥å’Œé‡å‘½å)"""
        douban_data_root = self.app_config.douban_config.directory
        if not douban_data_root:
            ui_logger.warning("     - [æœ¬åœ°ç¼“å­˜] æœªé…ç½®è±†ç“£æ•°æ®æ ¹ç›®å½•ï¼Œæ— æ³•ä½¿ç”¨æœ¬åœ°æˆªå›¾ç¼“å­˜åŠŸèƒ½ã€‚", task_category="æˆªå›¾ç¼“å­˜")
            return None
        
        sanitized_name = self._sanitize_filename(series_name)
        folder_name = f"{sanitized_name} [{series_tmdb_id}]"
        
        cache_dir = os.path.join(douban_data_root, "EpisodeScreenshots", folder_name)
        filename = f"season-{season_number}-episode-{episode_number}.jpg"
        return os.path.join(cache_dir, filename)

    def _save_screenshot_to_local(self, image_bytes: bytes, series_tmdb_id: str, season_number: int, episode_number: int, series_name: str, task_category: str) -> bool:
        """å°†æˆªå›¾äºŒè¿›åˆ¶æ•°æ®ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜ï¼Œå¹¶æ™ºèƒ½å¤„ç†æ–‡ä»¶å¤¹é‡å‘½åã€‚"""
        new_filepath = self._get_local_screenshot_path(series_tmdb_id, season_number, episode_number, series_name)
        if not new_filepath:
            return False
        
        new_dir = os.path.dirname(new_filepath)
        old_dir = self._find_screenshot_cache_dir_by_tmdbid(series_tmdb_id)
        final_dir = new_dir
        
        try:
            if old_dir:
                if old_dir != new_dir:
                    ui_logger.info(f"     - [æœ¬åœ°ç¼“å­˜] æ£€æµ‹åˆ°å‰§é›†æ ‡é¢˜å˜æ›´ï¼Œæ­£åœ¨é‡å‘½åç¼“å­˜æ–‡ä»¶å¤¹: '{os.path.basename(old_dir)}' -> '{os.path.basename(new_dir)}'", task_category=task_category)
                    os.rename(old_dir, new_dir)
                final_dir = new_dir
            else:
                os.makedirs(new_dir, exist_ok=True)
            
            final_filepath = os.path.join(final_dir, os.path.basename(new_filepath))
            with open(final_filepath, 'wb') as f:
                f.write(image_bytes)
            ui_logger.debug(f"     - [æœ¬åœ°ç¼“å­˜] æˆåŠŸå°†æˆªå›¾ä¿å­˜åˆ°: {final_filepath}", task_category=task_category)
            return True
            
        except OSError as e:
            ui_logger.error(f"     - [æœ¬åœ°ç¼“å­˜] ä¿å­˜æˆ–é‡å‘½åç¼“å­˜æ—¶å‘ç”Ÿæ–‡ä»¶ç³»ç»Ÿé”™è¯¯: {e}", task_category=task_category, exc_info=True)
            return False
        except Exception as e:
            ui_logger.error(f"     - [æœ¬åœ°ç¼“å­˜] ä¿å­˜æˆªå›¾åˆ°æœ¬åœ°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", task_category=task_category, exc_info=True)
            return False

    def _read_screenshot_from_local(self, series_tmdb_id: str, season_number: int, episode_number: int, task_category: str) -> Optional[bytes]:
        """ä»æœ¬åœ°ç¼“å­˜è¯»å–æˆªå›¾äºŒè¿›åˆ¶æ•°æ® (é€šè¿‡TMDB IDæŸ¥æ‰¾)"""
        cache_dir = self._find_screenshot_cache_dir_by_tmdbid(series_tmdb_id)
        if not cache_dir:
            return None
        
        filename = f"season-{season_number}-episode-{episode_number}.jpg"
        filepath = os.path.join(cache_dir, filename)

        if not os.path.exists(filepath):
            return None
        
        try:
            with open(filepath, 'rb') as f:
                return f.read()
        except Exception as e:
            ui_logger.error(f"     - [æœ¬åœ°ç¼“å­˜] ä»æœ¬åœ°è¯»å–æˆªå›¾å¤±è´¥: {e}", task_category=task_category, exc_info=True)
            return None

    def _delete_local_screenshot(self, series_tmdb_id: str, season_number: int, episode_number: int, task_category: str) -> bool:
        """ä»æœ¬åœ°ç¼“å­˜ä¸­åˆ é™¤æŒ‡å®šçš„æˆªå›¾æ–‡ä»¶ (é€šè¿‡TMDB IDæŸ¥æ‰¾)"""
        cache_dir = self._find_screenshot_cache_dir_by_tmdbid(series_tmdb_id)
        if not cache_dir:
            ui_logger.debug(f"     - [æœ¬åœ°ç¼“å­˜] æ— éœ€åˆ é™¤ï¼Œæœªæ‰¾åˆ°TMDB IDä¸º {series_tmdb_id} çš„ç¼“å­˜ç›®å½•ã€‚", task_category=task_category)
            return True

        filename = f"season-{season_number}-episode-{episode_number}.jpg"
        filepath = os.path.join(cache_dir, filename)

        if not os.path.exists(filepath):
            ui_logger.debug(f"     - [æœ¬åœ°ç¼“å­˜] æ— éœ€åˆ é™¤ï¼Œæ–‡ä»¶ä¸å­˜åœ¨: {filepath}", task_category=task_category)
            return True
        
        try:
            os.remove(filepath)
            ui_logger.info(f"     - [æœ¬åœ°ç¼“å­˜] TMDBå·²æœ‰å®˜æ–¹å›¾ï¼ŒæˆåŠŸåˆ é™¤ä½œåºŸçš„æœ¬åœ°æˆªå›¾: {filepath}", task_category=task_category)
            return True
        except Exception as e:
            ui_logger.error(f"     - [æœ¬åœ°ç¼“å­˜] åˆ é™¤æœ¬åœ°æˆªå›¾å¤±è´¥: {e}", task_category=task_category, exc_info=True)
            return False
    
    def _split_image_stream(self, stream: bytes) -> List[bytes]:
        """å°† ffmpeg image2pipe è¾“å‡ºçš„äºŒè¿›åˆ¶æµåˆ†å‰²æˆç‹¬ç«‹çš„å›¾ç‰‡åˆ—è¡¨"""
        images = []
        start_marker = b'\xff\xd8'
        end_marker = b'\xff\xd9'
        
        start = 0
        while True:
            start_pos = stream.find(start_marker, start)
            if start_pos == -1: break
            end_pos = stream.find(end_marker, start_pos)
            if end_pos == -1: break
            
            images.append(stream[start_pos:end_pos+2])
            start = end_pos + 2
        return images

    def _get_best_image_by_variance(self, images: List[bytes], task_category: str) -> Optional[bytes]:
        """é€šè¿‡æ‹‰æ™®æ‹‰æ–¯æ–¹å·®è®¡ç®—ï¼Œä»å›¾ç‰‡åˆ—è¡¨ä¸­é€‰æ‹©æœ€æ¸…æ™°çš„ä¸€å¼ """
        if not images:
            return None
        if not OPENCV_AVAILABLE:
            ui_logger.warning("     - [æˆªå›¾-æ™ºèƒ½æ¨¡å¼] OpenCV æœªåŠ è½½ï¼Œå°†éšæœºé€‰æ‹©ä¸€å¼ å›¾ç‰‡ã€‚", task_category=task_category)
            return images[len(images) // 2]

        max_variance = -1
        best_image = None

        for img_bytes in images:
            try:
                pil_img = Image.open(BytesIO(img_bytes))
                cv_img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2GRAY)
                variance = cv2.Laplacian(cv_img, cv2.CV_64F).var()
                if variance > max_variance:
                    max_variance = variance
                    best_image = img_bytes
            except Exception as e:
                ui_logger.debug(f"     - [æˆªå›¾-æ™ºèƒ½æ¨¡å¼] åˆ†æå•å¼ å›¾ç‰‡æ—¶å‡ºé”™: {e}", task_category=task_category)
                continue
        
        ui_logger.info(f"     - [æˆªå›¾-æ™ºèƒ½æ¨¡å¼] åˆ†æäº† {len(images)} å¸§ï¼Œé€‰æ‹©äº†æ¸…æ™°åº¦å¾—åˆ†æœ€é«˜çš„ä¸€å¼  (æ–¹å·®: {max_variance:.2f})ã€‚", task_category=task_category)
        return best_image

    def _capture_screenshot(self, video_url: str, seek_time: float, config: EpisodeRefresherConfig, task_category: str) -> Optional[bytes]:
        """ä½¿ç”¨ ffmpeg ä»è§†é¢‘æµæˆªå›¾ï¼Œå¹¶æ ¹æ®é…ç½®å¤„ç†é»‘è¾¹å’Œæ¯”ä¾‹"""
        try:
            crop_filter = ""
            try:
                detect_seek_time = max(0, seek_time - 3)
                detect_cmd = [
                    'ffmpeg', '-ss', str(detect_seek_time),
                    '-i', video_url, '-t', '2', '-vf', 'cropdetect',
                    '-f', 'null', '-'
                ]
                ui_logger.debug(f"     - [æˆªå›¾] æ­£åœ¨æ‰§è¡Œ cropdetect å‘½ä»¤æ£€æµ‹é»‘è¾¹...", task_category=task_category)
                detect_result = subprocess.run(detect_cmd, capture_output=True, text=True, timeout=60)
                
                crop_match = re.search(r'crop=(\d+:\d+:\d+:\d+)', detect_result.stderr)
                if crop_match:
                    detected_crop_params = crop_match.group(1)
                    w, h, x, y = map(int, detected_crop_params.split(':'))
                    ui_logger.debug(f"     - [æˆªå›¾] æ£€æµ‹åˆ°æœ‰æ•ˆç”»é¢åŒºåŸŸ: {detected_crop_params}", task_category=task_category)

                    crop_filter = f"crop={detected_crop_params}"
                    if config.crop_widescreen_to_16_9 and w / h > 1.8:
                        target_w = round(h * 16 / 9)
                        if target_w < w:
                            offset_x = round((w - target_w) / 2)
                            crop_filter += f",crop={target_w}:{h}:{offset_x}:0"
                            ui_logger.info(f"     - [æˆªå›¾] å°†åº”ç”¨å®½å±è£å‰ªï¼Œæœ€ç»ˆæ»¤é•œ: {crop_filter}", task_category=task_category)
                else:
                    ui_logger.warning("     - [æˆªå›¾] æœªèƒ½æ£€æµ‹åˆ°é»‘è¾¹ä¿¡æ¯ï¼Œå°†ä¸è¿›è¡Œè£å‰ªã€‚", task_category=task_category)
            except Exception as e:
                ui_logger.warning(f"     - [æˆªå›¾] é»‘è¾¹æ£€æµ‹å¤±è´¥ï¼Œå°†ä¸è¿›è¡Œè£å‰ªã€‚åŸå› : {e}", task_category=task_category)

            if config.use_smart_screenshot:
                ui_logger.info("     - [æˆªå›¾] å¯åŠ¨æ™ºèƒ½æˆªå›¾æ¨¡å¼ï¼Œå°†è·å–1ç§’å†…å¤šå¸§è¿›è¡Œç­›é€‰...", task_category=task_category)
                capture_cmd = [
                    'ffmpeg', '-ss', str(seek_time),
                    '-i', video_url, '-t', '1',
                    '-q:v', '2', '-f', 'image2pipe', '-'
                ]
                if crop_filter:
                    capture_cmd.insert(7, '-vf')
                    capture_cmd.insert(8, crop_filter)
                
                capture_result = subprocess.run(capture_cmd, capture_output=True, check=True, timeout=60)
                all_frames = self._split_image_stream(capture_result.stdout)
                return self._get_best_image_by_variance(all_frames, task_category)
            else:
                ui_logger.info("     - [æˆªå›¾] å¯åŠ¨å¿«é€Ÿæˆªå›¾æ¨¡å¼ï¼Œå°†æˆªå–å•å¸§å›¾ç‰‡...", task_category=task_category)
                capture_cmd = [
                    'ffmpeg', '-ss', str(seek_time),
                    '-i', video_url, '-vframes', '1',
                    '-q:v', '2', '-f', 'image2pipe', '-'
                ]
                if crop_filter:
                    capture_cmd.insert(5, '-vf')
                    capture_cmd.insert(6, crop_filter)
                
                capture_result = subprocess.run(capture_cmd, capture_output=True, check=True, timeout=60)
                return capture_result.stdout

        except Exception as e:
            ui_logger.error(f"     - [å¤±è´¥âŒ] æˆªå›¾è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", task_category=task_category, exc_info=True)
            return None


    def _get_remote_db(self, config: EpisodeRefresherConfig, force_refresh: bool = False) -> Tuple[Optional[Dict], Optional[str]]:
        task_cat = "è¿œç¨‹å›¾åºŠ"
        github_conf = config.github_config
        if not github_conf.repo_url:
            return None, None

        lock_path = GITHUB_DB_CACHE_FILE + ".lock"
        try:
            with FileLock(lock_path, timeout=5):
                if not force_refresh and os.path.exists(GITHUB_DB_CACHE_FILE):
                    mtime = os.path.getmtime(GITHUB_DB_CACHE_FILE)
                    if time.time() - mtime < GITHUB_DB_CACHE_DURATION:
                        with open(GITHUB_DB_CACHE_FILE, 'r', encoding='utf-8') as f:
                            ui_logger.debug("     - [è¿œç¨‹å›¾åºŠ] å‘½ä¸­æœ¬åœ°çš„æ•°æ®åº“æ–‡ä»¶ç¼“å­˜ã€‚", task_category=task_cat)
                            return json.load(f), None # ä»ç¼“å­˜åŠ è½½æ—¶ï¼Œshaä¸ºNone
        except (Timeout, IOError, json.JSONDecodeError) as e:
            ui_logger.warning(f"     - [è¿œç¨‹å›¾åºŠ] è¯»å–æœ¬åœ°æ•°æ®åº“ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†å¼ºåˆ¶ä»è¿œç¨‹è·å–ã€‚", task_category=task_cat)

        try:
            match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", github_conf.repo_url)
            if not match:
                raise ValueError("æ— æ•ˆçš„ GitHub ä»“åº“ URL æ ¼å¼ã€‚")
            owner, repo = match.groups()
            
            # --- æ–°å¢ï¼šä¸‹è½½å†·å´ ---
            if github_conf.download_cooldown > 0:
                ui_logger.debug(f"     - [è¿œç¨‹å›¾åºŠ] â±ï¸ ä¸‹è½½å†·å´ {github_conf.download_cooldown} ç§’...", task_category=task_cat)
                time.sleep(github_conf.download_cooldown)
            # --- ç»“æŸæ–°å¢ ---

            # ä¼˜å…ˆå°è¯• Raw URL
            raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/{github_conf.branch}/database.json"
            ui_logger.debug(f"     - [è¿œç¨‹å›¾åºŠ] æ­£åœ¨ä» Raw URL ä¸‹è½½æ•°æ®åº“: {raw_url}", task_category=task_cat)
            proxies = self.tmdb_logic.proxy_manager.get_proxies(raw_url)
            response = self.session.get(raw_url, timeout=30, proxies=proxies)
            
            if response.status_code == 200:
                db_content = response.json()
                with FileLock(lock_path, timeout=5):
                    with open(GITHUB_DB_CACHE_FILE, 'w', encoding='utf-8') as f:
                        json.dump(db_content, f, ensure_ascii=False)
                return db_content, None # é€šè¿‡ Raw URL è·å–æ—¶ï¼Œæ— æ³•å¾—åˆ° sha
            
            ui_logger.warning(f"     - [è¿œç¨‹å›¾åºŠ] ä» Raw URL ä¸‹è½½å¤±è´¥ (çŠ¶æ€ç : {response.status_code})ï¼Œå°†å°è¯•ä½¿ç”¨ API è·å–...", task_category=task_cat)
            
            # ä½¿ç”¨ API ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
            api_url = f"https://api.github.com/repos/{owner}/{repo}/contents/database.json?ref={github_conf.branch}"
            headers = {"Accept": "application/vnd.github.v3+json"}
            if github_conf.personal_access_token:
                headers["Authorization"] = f"token {github_conf.personal_access_token}"
            
            proxies = self.tmdb_logic.proxy_manager.get_proxies(api_url)
            api_response = self.session.get(api_url, headers=headers, timeout=30, proxies=proxies)
            api_response.raise_for_status()
            
            api_data = api_response.json()
            content = base64.b64decode(api_data['content']).decode('utf-8')
            db_content = json.loads(content)
            sha = api_data.get('sha')

            with FileLock(lock_path, timeout=5):
                with open(GITHUB_DB_CACHE_FILE, 'w', encoding='utf-8') as f:
                    json.dump(db_content, f, ensure_ascii=False)
            
            return db_content, sha

        except Exception as e:
            ui_logger.error(f"     - [å¤±è´¥âŒ] è·å–è¿œç¨‹å›¾åºŠæ•°æ®åº“å¤±è´¥: {e}", task_category=task_cat)
            return None, None

    # --- æ ¸å¿ƒé‡æ„ï¼š_handle_screenshot_flow ---
    def _handle_screenshot_flow(self, series_tmdb_id: str, episode_id: str, episode_details: Dict, config: EpisodeRefresherConfig, task_category: str) -> bool:
        log_prefix = f"     - S{episode_details.get('ParentIndexNumber', 0):02d}E{episode_details.get('IndexNumber', 0):02d}:"
        
        if not self.ffmpeg_available:
            ui_logger.warning(f"{log_prefix} [è·³è¿‡] ffmpeg æˆ– ffprobe æœªå®‰è£…ï¼Œæ— æ³•æ‰§è¡Œæˆªå›¾åŠŸèƒ½ã€‚", task_category=task_category)
            return False

        if config.force_overwrite_screenshots:
            ui_logger.info(f"{log_prefix} [è­¦å‘Šâš ï¸] ç”¨æˆ·å¼€å¯å¼ºåˆ¶è¦†ç›–æ¨¡å¼ï¼å°†è·³è¿‡æ‰€æœ‰ç¼“å­˜ï¼Œç›´æ¥è¿›è¡Œå®æ—¶æˆªå›¾...", task_category=task_category)
            return self._trigger_realtime_screenshot(series_tmdb_id, episode_id, episode_details, config, task_category, log_prefix)

        # æ£€æŸ¥è¿œç¨‹å›¾åºŠç¼“å­˜
        if config.screenshot_cache_mode == 'remote':
            ui_logger.debug(f"{log_prefix} [å†³ç­–] æ£€æŸ¥ç¼“å­˜ (æ¨¡å¼: è¿œç¨‹å›¾åºŠä¼˜å…ˆ)...", task_category=task_category)
            remote_db, _ = self._get_remote_db(config)
            if remote_db:
                s_key = str(episode_details.get("ParentIndexNumber"))
                e_key = str(episode_details.get("IndexNumber"))
                series_data = remote_db.get("series", {}).get(str(series_tmdb_id), {})
                image_url = series_data.get(f"{s_key}-{e_key}")
                
                if image_url:
                    ui_logger.info(f"{log_prefix} [å‘½ä¸­âœ…] å‘ç°è¿œç¨‹å›¾åºŠç¼“å­˜ï¼Œå‡†å¤‡æ›´æ–°ã€‚", task_category=task_category)
                    if self._upload_image_from_url(episode_id, image_url, task_category):
                        return True
                    else:
                        ui_logger.warning(f"{log_prefix} [è­¦å‘Šâš ï¸] è¿œç¨‹å›¾åºŠå›¾ç‰‡ä¸‹è½½æˆ–ä¸Šä¼ å¤±è´¥ï¼Œå°†ç»§ç»­é™çº§æ£€æŸ¥ã€‚", task_category=task_category)
                else:
                    ui_logger.debug(f"{log_prefix} [è·³è¿‡] è¿œç¨‹å›¾åºŠæ— ç¼“å­˜ã€‚", task_category=task_category)
            else:
                ui_logger.warning(f"{log_prefix} [è­¦å‘Šâš ï¸] è·å–è¿œç¨‹å›¾åºŠæ•°æ®åº“å¤±è´¥ï¼Œæ— æ³•æ£€æŸ¥è¿œç¨‹ç¼“å­˜ã€‚", task_category=task_category)

        # é™çº§æ£€æŸ¥æœ¬åœ°æ–‡ä»¶ç¼“å­˜
        if config.screenshot_cache_mode in ['remote', 'local']:
            ui_logger.debug(f"{log_prefix} [å†³ç­–] é™çº§æˆ–ä¼˜å…ˆæ£€æŸ¥æœ¬åœ°æ–‡ä»¶ç¼“å­˜...", task_category=task_category)
            cached_image_bytes = self._read_screenshot_from_local(series_tmdb_id, episode_details.get("ParentIndexNumber"), episode_details.get("IndexNumber"), task_category)
            if cached_image_bytes:
                ui_logger.info(f"{log_prefix} [å‘½ä¸­âœ…] å‘ç°æœ¬åœ°æ–‡ä»¶ç¼“å­˜ï¼Œå‡†å¤‡æ›´æ–°ã€‚", task_category=task_category)
                if self._upload_image_bytes(episode_id, cached_image_bytes, 'image/jpeg', task_category):
                    return True
                else:
                     ui_logger.warning(f"{log_prefix} [è­¦å‘Šâš ï¸] æœ¬åœ°ç¼“å­˜å›¾ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œå°†ç»§ç»­é™çº§æ£€æŸ¥ã€‚", task_category=task_category)
            else:
                ui_logger.debug(f"{log_prefix} [è·³è¿‡] æœ¬åœ°æ— ç¼“å­˜ã€‚", task_category=task_category)

        # é™çº§åˆ°å®æ—¶æˆªå›¾
        return self._trigger_realtime_screenshot(series_tmdb_id, episode_id, episode_details, config, task_category, log_prefix)

    # --- æ–°å¢ï¼šå®æ—¶æˆªå›¾çš„ç‹¬ç«‹è§¦å‘å‡½æ•° ---
    def _trigger_realtime_screenshot(self, series_tmdb_id: str, episode_id: str, episode_details: Dict, config: EpisodeRefresherConfig, task_category: str, log_prefix: str) -> bool:
        if config.screenshot_cache_mode == 'remote' and not config.github_config.allow_fallback:
            ui_logger.info(f"{log_prefix} [è·³è¿‡] æ‰€æœ‰ç¼“å­˜å‡æœªå‘½ä¸­ï¼Œä¸”ç”¨æˆ·ç¦æ­¢é™çº§ä¸ºå®æ—¶æˆªå›¾ï¼Œæˆªå›¾æµç¨‹ä¸­æ­¢ã€‚", task_category=task_category)
            return False

        ui_logger.info(f"{log_prefix} [å†³ç­–] æ‰€æœ‰ç¼“å­˜å‡æœªå‘½ä¸­æˆ–è¢«è·³è¿‡ï¼Œå‡†å¤‡å®æ—¶æˆªå›¾...", task_category=task_category)
        
        duration = None
        if (runtime_ticks := episode_details.get("RunTimeTicks")) and runtime_ticks > 0:
            duration = runtime_ticks / 10_000_000
            ui_logger.debug(f"     - [æˆªå›¾] è§†é¢‘æ—¶é•¿: {duration:.2f}s (æ¥è‡ªEmbyå…ƒæ•°æ®)ã€‚", task_category=task_category)
        
        video_url = self._get_video_url_from_item(episode_id, task_category)
        if not video_url:
            ui_logger.error(f"{log_prefix} [å¤±è´¥âŒ] æœªèƒ½è·å–åˆ°è§†é¢‘æµ URLï¼Œæ— æ³•æˆªå›¾ã€‚", task_category=task_category)
            return False
            
        if duration is None:
            duration = self._get_video_duration(video_url, task_category)

        seek_time = duration * (config.screenshot_percentage / 100) if duration else config.screenshot_fallback_seconds
        if duration:
            ui_logger.debug(f"     - [æˆªå›¾] æˆªå›¾ä½ç½®: {seek_time:.2f}s (åŸºäº {config.screenshot_percentage}%)ã€‚", task_category=task_category)
        else:
            ui_logger.warning(f"     - [æˆªå›¾] è·å–è§†é¢‘æ—¶é•¿å¤±è´¥ï¼Œå°†ä½¿ç”¨ä¿åº•ç§’æ•°è¿›è¡Œæˆªå›¾: {seek_time}sã€‚", task_category=task_category)

        image_bytes = self._capture_screenshot(video_url, seek_time, config, task_category)
        
        if image_bytes:
            if self._upload_image_bytes(episode_id, image_bytes, 'image/jpeg', task_category):
                ui_logger.info(f"{log_prefix} [æˆåŠŸğŸ‰] æˆªå›¾ç”Ÿæˆå¹¶ä¸Šä¼ æˆåŠŸï¼", task_category=task_category)
                
                # å›å†™ç¼“å­˜
                if config.screenshot_cache_mode == 'local':
                    self._save_screenshot_to_local(image_bytes, series_tmdb_id, episode_details.get("ParentIndexNumber"), episode_details.get("IndexNumber"), episode_details.get("SeriesName"), task_category)
                    ui_logger.info(f"{log_prefix} [å›å†™] æ–°æˆªå›¾å·²ä¿å­˜è‡³æœ¬åœ°ç¼“å­˜ã€‚", task_category=task_category)
                elif config.screenshot_cache_mode == 'remote' and config.github_config.personal_access_token:
                    self._save_screenshot_to_local(image_bytes, series_tmdb_id, episode_details.get("ParentIndexNumber"), episode_details.get("IndexNumber"), episode_details.get("SeriesName"), task_category)
                    ui_logger.info(f"{log_prefix} [å›å†™] æ–°æˆªå›¾å·²æš‚å­˜è‡³æœ¬åœ° (ç­‰å¾…æ‰‹åŠ¨å¤‡ä»½)ã€‚", task_category=task_category)

                self._set_image_source_tag(episode_id, "screenshot", task_category)
                return True
        
        ui_logger.error(f"{log_prefix} [å¤±è´¥âŒ] å®æ—¶æˆªå›¾å¤±è´¥ã€‚", task_category=task_category)
        return False

    def _set_image_source_tag(self, item_id: str, source: str, task_category: str):
        try:
            item_details = self.tmdb_logic._get_emby_item_details(item_id, fields="ProviderIds")
            if not item_details: return

            item_details.setdefault("ProviderIds", {})['ToolboxImageSource'] = source
            
            update_url = f"{self.base_url}/Items/{item_id}"
            headers = {'Content-Type': 'application/json'}
            self.session.post(update_url, params=self.params, json=item_details, headers=headers, timeout=20).raise_for_status()
            ui_logger.debug(f"     - [æ ‡è®°] æˆåŠŸå°†å›¾ç‰‡æ¥æºæ ‡è®° '{source}' å†™å…¥ Emby (ID: {item_id})ã€‚", task_category=task_category)
        except Exception as e:
            ui_logger.error(f"     - [å¤±è´¥âŒ] å†™å…¥å›¾ç‰‡æ¥æºæ ‡è®°å¤±è´¥ (ID: {item_id}): {e}", task_category=task_category)

    def _clear_image_source_tag(self, item_id: str, task_category: str):
        try:
            item_details = self.tmdb_logic._get_emby_item_details(item_id, fields="ProviderIds")
            if not item_details: return

            provider_ids = item_details.get("ProviderIds", {})
            if 'ToolboxImageSource' in provider_ids:
                del provider_ids['ToolboxImageSource']
                update_url = f"{self.base_url}/Items/{item_id}"
                headers = {'Content-Type': 'application/json'}
                self.session.post(update_url, params=self.params, json=item_details, headers=headers, timeout=20).raise_for_status()
                ui_logger.debug(f"     - [æ ‡è®°] æˆåŠŸä» Emby (ID: {item_id}) ç§»é™¤å›¾ç‰‡æ¥æºæ ‡è®°ã€‚", task_category=task_category)
        except Exception as e:
            ui_logger.error(f"     - [å¤±è´¥âŒ] ç§»é™¤å›¾ç‰‡æ¥æºæ ‡è®°å¤±è´¥ (ID: {item_id}): {e}", task_category=task_category)

    # åœ¨ backend/episode_refresher_logic.py æ–‡ä»¶ä¸­

    def _refresh_season_by_toolbox(self, series_tmdb_id: str, season_number: int, emby_episodes: List[Dict], config: EpisodeRefresherConfig, task_category: str) -> int:
        updated_count = 0
        try:
            series_name_for_log = emby_episodes[0].get("SeriesName", f"å‰§é›† {series_tmdb_id}")
            ui_logger.info(f"  -> [å·¥å…·ç®±æ¨¡å¼] æ­£åœ¨ä¸ºã€Š{series_name_for_log}ã€‹S{season_number:02d} è·å–æ•´å­£TMDBæ•°æ®...", task_category=task_category)
            tmdb_season_details = self.tmdb_logic.get_season_details(int(series_tmdb_id), season_number)

            if not tmdb_season_details or not tmdb_season_details.get("episodes"):
                ui_logger.warning(f"     - æœªèƒ½ä»TMDBè·å–åˆ° S{season_number:02d} çš„æœ‰æ•ˆåˆ†é›†åˆ—è¡¨ã€‚", task_category=task_category)
                return 0
            
            tmdb_episodes_map = {ep.get("episode_number"): ep for ep in tmdb_season_details["episodes"]}

            for emby_episode in emby_episodes:
                episode_num = emby_episode.get("IndexNumber")
                if episode_num is None:
                    continue
                
                # --- æ—¥å¿—ä¼˜åŒ–ï¼šå°†é›†æ•°ä¿¡æ¯æ”¾åˆ° log_prefix ä¸­ ---
                log_prefix = f"     - S{season_number:02d}E{episode_num:02d}:"
                ui_logger.info(f"â¡ï¸ å¼€å§‹å¤„ç†ã€Š{series_name_for_log}ã€‹S{season_number:02d}E{episode_num:02d}: {emby_episode.get('Name')}", task_category=task_category)

                tmdb_episode = tmdb_episodes_map.get(episode_num)
                
                potential_changes = {}
                image_update_action = None
                
                emby_name = emby_episode.get("Name", "")
                tmdb_name = tmdb_episode.get("name") if tmdb_episode else None
                if tmdb_name and not self._is_generic_episode_title(tmdb_name) and tmdb_name != emby_name:
                    potential_changes["Name"] = tmdb_name

                emby_overview = emby_episode.get("Overview", "")
                tmdb_overview = tmdb_episode.get("overview") if tmdb_episode else None
                if tmdb_overview and tmdb_overview != emby_overview:
                    potential_changes["Overview"] = tmdb_overview

                emby_premiere_date = emby_episode.get("PremiereDate", "")
                tmdb_air_date = tmdb_episode.get("air_date") if tmdb_episode else None
                if tmdb_air_date:
                    if not emby_premiere_date or tmdb_air_date != emby_premiere_date[:10]:
                        potential_changes["PremiereDate"] = tmdb_air_date + "T00:00:00.000Z"

                current_image_source = emby_episode.get("ProviderIds", {}).get("ToolboxImageSource")
                emby_has_image = bool(emby_episode.get("ImageTags", {}).get("Primary"))
                tmdb_still_path = tmdb_episode.get("still_path") if tmdb_episode else None

                # --- æ ¸å¿ƒ Bug ä¿®å¤ï¼šé‡æ„å›¾ç‰‡æ›´æ–°å†³ç­–é€»è¾‘ ---
                ui_logger.debug(f"{log_prefix} [å†³ç­–] æ£€æŸ¥å¤–éƒ¨æ•°æ®æº...")
                if tmdb_still_path:
                    if not emby_has_image or current_image_source == "screenshot":
                        image_update_action = "tmdb"
                    else:
                        ui_logger.info(f"{log_prefix} [ä¿æŠ¤ğŸ›¡ï¸] Emby å·²æœ‰ç”¨æˆ·è‡ªå®šä¹‰å›¾ç‰‡ï¼Œè·³è¿‡å›¾ç‰‡æ›´æ–°ã€‚", task_category=task_category)
                elif config.screenshot_enabled:
                    # åªæœ‰åœ¨ Emby æ²¡å›¾ï¼Œæˆ–è€…æœ‰å›¾ä½†ç”¨æˆ·è¦æ±‚å¼ºåˆ¶è¦†ç›–æ—¶ï¼Œæ‰è€ƒè™‘æˆªå›¾
                    if not emby_has_image:
                        image_update_action = "screenshot"
                    elif current_image_source == "screenshot" and config.force_overwrite_screenshots:
                        image_update_action = "screenshot"
                    else:
                        # åŒ…å«äº†â€œæœ‰ç”¨æˆ·å›¾â€å’Œâ€œæœ‰æˆªå›¾ä½†æœªå¼ºåˆ¶è¦†ç›–â€ä¸¤ç§æƒ…å†µ
                        ui_logger.info(f"{log_prefix} [è·³è¿‡] Emby ä¸­å·²æœ‰å·¥å…·æˆªå›¾ä¸”æœªå¼€å¯å¼ºåˆ¶è¦†ç›–ï¼Œæ— éœ€æˆªå›¾ã€‚", task_category=task_category)
                # --- ä¿®å¤ç»“æŸ ---

                if not potential_changes and not image_update_action:
                    ui_logger.info(f"{log_prefix} [è·³è¿‡] å…ƒæ•°æ®å’Œå›¾ç‰‡å‡æ— éœ€æ›´æ–°ã€‚", task_category=task_category)
                    continue

                final_changes_log = []
                
                if potential_changes:
                    if self._unlock_item(emby_episode["Id"], task_category):
                        update_payload = emby_episode.copy()
                        update_payload.update(potential_changes)
                        update_url = f"{self.base_url}/Items/{emby_episode['Id']}"
                        headers = {'Content-Type': 'application/json'}
                        try:
                            self.session.post(update_url, params=self.params, json=update_payload, headers=headers, timeout=20).raise_for_status()
                            field_map = {"Name": "æ ‡é¢˜", "Overview": "ç®€ä»‹", "PremiereDate": "é¦–æ’­æ—¥æœŸ"}
                            for key in potential_changes.keys():
                                final_changes_log.append(field_map.get(key, key))
                        except Exception as e:
                            ui_logger.error(f"{log_prefix} [å¤±è´¥âŒ] åº”ç”¨å…ƒæ•°æ®æ›´æ–°æ—¶å¤±è´¥: {e}", task_category=task_category)
                    else:
                        ui_logger.error(f"{log_prefix} [å¤±è´¥âŒ] è§£é”å¤±è´¥ï¼Œè·³è¿‡å…ƒæ•°æ®æ›´æ–°ã€‚", task_category=task_category)

                if image_update_action == "tmdb":
                    ui_logger.info(f"{log_prefix} [å‘½ä¸­âœ…] å‘ç° TMDB å®˜æ–¹å›¾ï¼Œå‡†å¤‡æ›´æ–°ã€‚", task_category=task_category)
                    image_url = f"https://image.tmdb.org/t/p/original{tmdb_still_path}"
                    if self._upload_image_from_url(emby_episode["Id"], image_url, task_category):
                        final_changes_log.append("å›¾ç‰‡(TMDB)")
                        if current_image_source == "screenshot":
                            self._clear_image_source_tag(emby_episode["Id"], task_category)
                            if config.screenshot_cache_mode != 'none':
                                self._delete_local_screenshot(series_tmdb_id, season_number, episode_num, task_category)
                
                elif image_update_action == "screenshot":
                    if self._handle_screenshot_flow(series_tmdb_id, emby_episode["Id"], emby_episode, config, task_category):
                        final_changes_log.append("å›¾ç‰‡(æˆªå›¾)")
                        if config.screenshot_cooldown > 0:
                            ui_logger.debug(f"     - [æˆªå›¾] æ“ä½œå†·å´ï¼Œç­‰å¾… {config.screenshot_cooldown} ç§’...", task_category=task_category)
                            time.sleep(config.screenshot_cooldown)

                if final_changes_log:
                    ui_logger.info(f"{log_prefix} [æˆåŠŸğŸ‰] æœ¬æ¬¡æ›´æ–°å†…å®¹: [{', '.join(final_changes_log)}]", task_category=task_category)
                    updated_count += 1
                else:
                    ui_logger.warning(f"{log_prefix} [è­¦å‘Šâš ï¸] æ£€æµ‹åˆ°éœ€è¦æ›´æ–°ï¼Œä½†æ‰€æœ‰æ›´æ–°æ“ä½œå‡å¤±è´¥ã€‚", task_category=task_category)

        except Exception as e:
            ui_logger.error(f"     - [å¤±è´¥âŒ] å¤„ç† S{season_number:02d} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", task_category=task_category, exc_info=True)
        
        return updated_count

    def run_refresh_for_episodes(self, episode_ids: Iterable[str], config: EpisodeRefresherConfig, cancellation_event: threading.Event, task_id: Optional[str] = None, task_manager: Optional[TaskManager] = None, task_category: str = "å‰§é›†åˆ·æ–°"):
        
        episode_ids_list = list(episode_ids)
        total_episodes = len(episode_ids_list)
        
        ui_logger.info(f"ä»»åŠ¡å¯åŠ¨ï¼Œå…±éœ€å¤„ç† {total_episodes} ä¸ªå‰§é›†åˆ†é›†ã€‚", task_category=task_category)
        ui_logger.info(f"  - åˆ·æ–°æ¨¡å¼: {'å·¥å…·ç®±ä»£ç†åˆ·æ–°' if config.refresh_mode == 'toolbox' else 'é€šçŸ¥Embyåˆ·æ–°'}", task_category=task_category)
        if config.refresh_mode == 'emby':
            ui_logger.info(f"  - å…ƒæ•°æ®å†™å…¥æ–¹å¼: {'è¦†ç›–æ‰€æœ‰å…ƒæ•°æ®' if config.overwrite_metadata else 'ä»…è¡¥å……ç¼ºå¤±'}", task_category=task_category)
        else:
            ui_logger.info(f"  - æˆªå›¾åŠŸèƒ½: {'å¼€å¯' if config.screenshot_enabled else 'å…³é—­'}", task_category=task_category)
            if config.screenshot_enabled:
                mode_map = {'none': 'æ— ç¼“å­˜', 'local': 'æœ¬åœ°æ–‡ä»¶ç¼“å­˜', 'remote': 'è¿œç¨‹å›¾åºŠä¼˜å…ˆ'}
                ui_logger.info(f"  - æˆªå›¾ç¼“å­˜æ¨¡å¼: {mode_map.get(config.screenshot_cache_mode, 'æœªçŸ¥')}", task_category=task_category)
                ui_logger.info(f"  - æ™ºèƒ½æˆªå›¾: {'å¼€å¯' if config.use_smart_screenshot else 'å…³é—­'}", task_category=task_category)
        ui_logger.info(f"  - æ™ºèƒ½è·³è¿‡: {'å¼€å¯' if config.skip_if_complete else 'å…³é—­'}", task_category=task_category)
        
        if task_manager and task_id:
            task_manager.update_task_progress(task_id, 0, total_episodes)

        if total_episodes == 0:
            ui_logger.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„å‰§é›†åˆ†é›†ï¼Œä»»åŠ¡ç»“æŸã€‚", task_category=task_category)
            return {"refreshed_count": 0}

        ui_logger.info("æ­¥éª¤ 1/2: æ­£åœ¨è·å–å¹¶ç­›é€‰éœ€è¦å¤„ç†çš„åˆ†é›†...", task_category=task_category)
        
        all_episode_details = []
        for index, episode_id in enumerate(episode_ids_list):
            if cancellation_event.is_set(): break
            if task_manager and task_id:
                task_manager.update_task_progress(task_id, index + 1, total_episodes)
            try:
                fields_to_get = "SeriesId,SeriesName,Name,Overview,ImageTags,IndexNumber,ParentIndexNumber,PremiereDate,ProviderIds,LockedFields,RunTimeTicks"
                details = self._get_emby_item_details(episode_id, fields_to_get)
                if details:
                    all_episode_details.append(details)
            except requests.RequestException as e:
                logging.error(f"ã€å‰§é›†åˆ·æ–°ã€‘è·å–åˆ†é›†è¯¦æƒ… (ID: {episode_id}) å¤±è´¥: {e}ï¼Œè·³è¿‡æ­¤åˆ†é›†ã€‚")
        
        if cancellation_event.is_set():
            ui_logger.warning("ä»»åŠ¡åœ¨é¢„å¤„ç†é˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_category)
            return

        episodes_to_process = []
        if config.skip_if_complete:
            for ep in all_episode_details:
                is_title_ok = bool(ep.get("Name")) and not self._is_generic_episode_title(ep.get("Name"))
                is_overview_ok = bool(ep.get("Overview"))
                
                has_primary_image = bool(ep.get("ImageTags", {}).get("Primary"))
                image_source = ep.get("ProviderIds", {}).get("ToolboxImageSource")
                is_image_ok = has_primary_image and image_source != "screenshot"
                
                if not (is_title_ok and is_overview_ok and is_image_ok):
                    episodes_to_process.append(ep)
            ui_logger.info(f"æ™ºèƒ½è·³è¿‡å·²å¼€å¯ï¼Œå…±ç­›é€‰å‡º {len(episodes_to_process)} / {len(all_episode_details)} ä¸ªéœ€è¦å¤„ç†çš„åˆ†é›†ã€‚", task_category=task_category)
        else:
            episodes_to_process = all_episode_details

        refreshed_count = 0
        
        if config.refresh_mode == 'toolbox':
            grouped_seasons = defaultdict(lambda: defaultdict(list))
            for ep in episodes_to_process:
                if ep.get("SeriesId") and ep.get("ParentIndexNumber") is not None:
                    grouped_seasons[ep["SeriesId"]][ep["ParentIndexNumber"]].append(ep)

            ui_logger.info(f"æ­¥éª¤ 2/2: [å·¥å…·ç®±æ¨¡å¼] å¼€å§‹æŒ‰å­£è·å–TMDBæ•°æ®å¹¶æ›´æ–°...", task_category=task_category)
            series_tmdb_id_cache = {}
            processed_ep_count = 0
            total_ep_to_process = len(episodes_to_process)
            for series_id, seasons in grouped_seasons.items():
                if cancellation_event.is_set(): break
                
                series_tmdb_id = series_tmdb_id_cache.get(series_id)
                if not series_tmdb_id:
                    series_details = self.tmdb_logic._get_emby_item_details(series_id, fields="ProviderIds")
                    series_tmdb_id = next((v for k, v in series_details.get("ProviderIds", {}).items() if k.lower() == 'tmdb'), None)
                    if series_tmdb_id:
                        series_tmdb_id_cache[series_id] = series_tmdb_id
                    else:
                        ui_logger.warning(f"å‰§é›†(ID: {series_id})ç¼ºå°‘TMDB IDï¼Œè·³è¿‡è¯¥å‰§é›†çš„æ‰€æœ‰åˆ†é›†ã€‚", task_category=task_category)
                        processed_ep_count += sum(len(eps) for eps in seasons.values())
                        if task_manager and task_id:
                            task_manager.update_task_progress(task_id, processed_ep_count, total_ep_to_process)
                        continue
                
                for season_number, emby_episodes in seasons.items():
                    if cancellation_event.is_set(): break
                    refreshed_count += self._refresh_season_by_toolbox(series_tmdb_id, season_number, emby_episodes, config, task_category)
                    processed_ep_count += len(emby_episodes)
                    if task_manager and task_id:
                        task_manager.update_task_progress(task_id, processed_ep_count, total_ep_to_process)
        else:
            ui_logger.info(f"æ­¥éª¤ 2/2: [Embyæ¨¡å¼] å¼€å§‹é€ä¸ªé€šçŸ¥Embyåˆ·æ–°...", task_category=task_category)
            for index, episode in enumerate(episodes_to_process):
                if cancellation_event.is_set(): break
                if task_manager and task_id:
                    task_manager.update_task_progress(task_id, index + 1, len(episodes_to_process))
                
                series_name = episode.get("SeriesName", "æœªçŸ¥å‰§é›†")
                episode_number = episode.get("IndexNumber")
                episode_title = episode.get("Name", f"åˆ†é›† {episode_number}" if episode_number else f"ID {episode['Id']}")
                log_message = f"{series_name} - "
                if episode_number:
                    log_message += f"ç¬¬{episode_number}é›† - "
                log_message += episode_title
                ui_logger.info(f"è¿›åº¦ {index + 1}/{len(episodes_to_process)}: æ­£åœ¨åˆ·æ–°: {log_message}", task_category=task_category)

                if self._refresh_single_episode_by_emby(episode['Id'], config, task_category):
                    refreshed_count += 1
                time.sleep(0.5)

        ui_logger.info(f"ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚å…±æˆåŠŸå¤„ç† {refreshed_count} ä¸ªåˆ†é›†ã€‚", task_category=task_category)
        if config.refresh_mode == 'emby':
            ui_logger.warning("è¯·æ³¨æ„ï¼šEmby çš„åˆ·æ–°æ˜¯åœ¨åå°è¿›è¡Œçš„ï¼Œå®é™…å…ƒæ•°æ®æ›´æ–°å¯èƒ½ä¼šæœ‰å»¶è¿Ÿã€‚è¯·ç¨ååœ¨ Emby ä¸­æŸ¥çœ‹ç»“æœã€‚", task_category=task_category)
        return {"refreshed_count": refreshed_count}
    
    def backup_screenshots_from_emby_task(self, series_ids: Iterable[str], config: EpisodeRefresherConfig, cancellation_event: threading.Event, task_id: Optional[str] = None, task_manager: Optional[TaskManager] = None):
        """
        ä» Emby å¤‡ä»½å·²å­˜åœ¨çš„ã€ç”±å·¥å…·ç®±ç”Ÿæˆçš„æˆªå›¾åˆ°æœ¬åœ°ç¼“å­˜ç›®å½•ã€‚
        """
        task_cat = "æˆªå›¾å¤‡ä»½"
        ui_logger.info(f"ã€{task_cat}ã€‘ä»»åŠ¡å¯åŠ¨ï¼Œå¼€å§‹ä» {len(series_ids)} ä¸ªå‰§é›†ä¸­æŸ¥æ‰¾å¹¶å¤‡ä»½å·²æœ‰æˆªå›¾...", task_category=task_cat)
        ui_logger.info(f"  - å¤‡ä»½é…ç½®ï¼šè¦†ç›–æœ¬åœ°æ–‡ä»¶={'å¼€å¯' if config.backup_overwrite_local else 'å…³é—­'}", task_category=task_cat)

        all_episode_ids = []
        if task_manager and task_id:
            task_manager.update_task_progress(task_id, 0, len(series_ids))
        
        for i, series_id in enumerate(series_ids):
            if cancellation_event.is_set():
                ui_logger.warning(f"ã€{task_cat}ã€‘ä»»åŠ¡åœ¨è·å–åˆ†é›†åˆ—è¡¨é˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                return
            try:
                episodes_url = f"{self.base_url}/Items"
                episodes_params = {**self.params, "ParentId": series_id, "IncludeItemTypes": "Episode", "Recursive": "true", "Fields": "Id"}
                episodes_resp = self.session.get(episodes_url, params=episodes_params, timeout=30)
                episodes_resp.raise_for_status()
                episodes = episodes_resp.json().get("Items", [])
                all_episode_ids.extend([ep['Id'] for ep in episodes])
            except Exception as e:
                ui_logger.error(f"ã€{task_cat}ã€‘è·å–å‰§é›† {series_id} çš„åˆ†é›†æ—¶å¤±è´¥: {e}", task_category=task_cat)
            if task_manager and task_id:
                task_manager.update_task_progress(task_id, i + 1, len(series_ids))
        
        ui_logger.info(f"ã€{task_cat}ã€‘åˆ†é›†åˆ—è¡¨è·å–å®Œæ¯•ï¼Œå…±æ‰¾åˆ° {len(all_episode_ids)} ä¸ªåˆ†é›†éœ€è¦æ£€æŸ¥ã€‚", task_category=task_cat)

        backed_up_count = 0
        skipped_count = 0
        failed_count = 0
        total_episodes = len(all_episode_ids)
        if task_manager and task_id:
            task_manager.update_task_progress(task_id, 0, total_episodes)

        series_tmdb_id_cache = {}

        for i, episode_id in enumerate(all_episode_ids):
            if cancellation_event.is_set():
                ui_logger.warning(f"ã€{task_cat}ã€‘ä»»åŠ¡åœ¨å¤„ç†åˆ†é›†æ—¶è¢«å–æ¶ˆã€‚", task_category=task_cat)
                break
            
            try:
                ep_details = self._get_emby_item_details(episode_id, fields="ProviderIds,SeriesId,ParentIndexNumber,IndexNumber,Name,SeriesName")
                if not ep_details:
                    ui_logger.warning(f"  -> æ— æ³•è·å–åˆ†é›† {episode_id} çš„è¯¦æƒ…ï¼Œè·³è¿‡ã€‚", task_category=task_cat)
                    failed_count += 1
                    continue

                log_prefix = f"  -> æ­£åœ¨å¤„ç†ã€Š{ep_details.get('SeriesName', 'æœªçŸ¥å‰§é›†')}ã€‹S{ep_details.get('ParentIndexNumber', 0):02d}E{ep_details.get('IndexNumber', 0):02d}:"

                provider_ids = ep_details.get("ProviderIds", {})
                if provider_ids.get("ToolboxImageSource") != "screenshot":
                    ui_logger.debug(f"{log_prefix} éå·¥å…·ç®±æˆªå›¾ï¼Œè·³è¿‡ã€‚", task_category=task_cat)
                    skipped_count += 1
                    continue

                series_id = ep_details.get("SeriesId")
                series_tmdb_id = series_tmdb_id_cache.get(series_id)
                if not series_tmdb_id:
                    series_details = self._get_emby_item_details(series_id, fields="ProviderIds")
                    series_tmdb_id = next((v for k, v in series_details.get("ProviderIds", {}).items() if k.lower() == 'tmdb'), None)
                    if not series_tmdb_id:
                        ui_logger.warning(f"{log_prefix} æ‰€å±å‰§é›†(ID: {series_id})ç¼ºå°‘TMDB IDï¼Œæ— æ³•æ„å»ºç¼“å­˜è·¯å¾„ï¼Œè·³è¿‡ã€‚", task_category=task_cat)
                        failed_count += 1
                        continue
                    series_tmdb_id_cache[series_id] = series_tmdb_id
                
                local_path = self._get_local_screenshot_path(
                    series_tmdb_id, 
                    ep_details.get("ParentIndexNumber"), 
                    ep_details.get("IndexNumber"),
                    ep_details.get("SeriesName")
                )
                if not local_path:
                    failed_count += 1
                    continue
                
                if os.path.exists(local_path) and not config.backup_overwrite_local:
                    ui_logger.info(f"{log_prefix} æœ¬åœ°ç¼“å­˜å·²å­˜åœ¨ä¸”æœªå¼€å¯è¦†ç›–ï¼Œè·³è¿‡ã€‚", task_category=task_cat)
                    skipped_count += 1
                    continue

                image_url = f"{self.base_url}/Items/{episode_id}/Images/Primary?api_key={self.api_key}"
                image_resp = self.session.get(image_url, timeout=30)
                image_resp.raise_for_status()
                image_bytes = image_resp.content

                if self._save_screenshot_to_local(image_bytes, series_tmdb_id, ep_details.get("ParentIndexNumber"), ep_details.get("IndexNumber"), ep_details.get("SeriesName"), task_cat):
                    ui_logger.info(f"{log_prefix} æˆåŠŸå¤‡ä»½åˆ°æœ¬åœ°ã€‚", task_category=task_cat)
                    backed_up_count += 1
                else:
                    ui_logger.error(f"{log_prefix} å¤‡ä»½å¤±è´¥ã€‚", task_category=task_cat)
                    failed_count += 1

            except Exception as e:
                ui_logger.error(f"  -> å¤„ç†åˆ†é›† {episode_id} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", task_category=task_cat, exc_info=True)
                failed_count += 1
            finally:
                if task_manager and task_id:
                    task_manager.update_task_progress(task_id, i + 1, total_episodes)

        ui_logger.info(f"ã€{task_cat}ã€‘ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚æˆåŠŸå¤‡ä»½: {backed_up_count} å¼ , è·³è¿‡: {skipped_count} å¼ , å¤±è´¥: {failed_count} å¼ ã€‚", task_category=task_cat)
        return {"backed_up_count": backed_up_count, "skipped_count": skipped_count, "failed_count": failed_count}

    # --- æ–°å¢ï¼šå¤‡ä»½åˆ° GitHub çš„ä»»åŠ¡ ---
    def backup_screenshots_to_github_task(self, config: EpisodeRefresherConfig, cancellation_event: threading.Event, task_id: str, task_manager: TaskManager):
        task_cat = "å¤‡ä»½åˆ°GitHub"
        github_conf = config.github_config
        
        ui_logger.info(f"ã€{task_cat}ã€‘ä»»åŠ¡å¯åŠ¨...", task_category=task_cat)
        
        if not github_conf.repo_url or not github_conf.personal_access_token:
            ui_logger.error(f"ã€{task_cat}ã€‘[å¤±è´¥âŒ] æœªé…ç½®å®Œæ•´çš„ GitHub ä»“åº“ URL å’Œä¸ªäººè®¿é—®ä»¤ç‰Œ (PAT)ï¼Œä»»åŠ¡ä¸­æ­¢ã€‚", task_category=task_cat)
            raise ValueError("GitHub ä»“åº“ URL å’Œ PAT ä¸èƒ½ä¸ºç©ºã€‚")

        douban_data_root = self.app_config.douban_config.directory
        if not douban_data_root:
            ui_logger.error(f"ã€{task_cat}ã€‘[å¤±è´¥âŒ] æœªé…ç½®è±†ç“£æ•°æ®æ ¹ç›®å½•ï¼Œæ— æ³•æ‰¾åˆ°æœ¬åœ°æˆªå›¾æ–‡ä»¶å¤¹ï¼Œä»»åŠ¡ä¸­æ­¢ã€‚", task_category=task_cat)
            raise ValueError("è±†ç“£æ•°æ®æ ¹ç›®å½•æœªé…ç½®ã€‚")

        local_screenshots_dir = os.path.join(douban_data_root, "EpisodeScreenshots")
        if not os.path.isdir(local_screenshots_dir):
            ui_logger.warning(f"ã€{task_cat}ã€‘[è·³è¿‡] æœ¬åœ°æˆªå›¾ç›®å½• '{local_screenshots_dir}' ä¸å­˜åœ¨ï¼Œæ²¡æœ‰å¯å¤‡ä»½çš„æ–‡ä»¶ã€‚", task_category=task_cat)
            return {"uploaded_count": 0, "skipped_count": 0, "failed_count": 0}

        # æ­¥éª¤ 1: è·å–è¿œç¨‹çŠ¶æ€
        ui_logger.info(f"ã€{task_cat}ã€‘[æ­¥éª¤ 1/5] æ­£åœ¨ä» GitHub è·å–æœ€æ–°çš„è¿œç¨‹æ•°æ®åº“...", task_category=task_cat)
        remote_db, remote_sha = self._get_remote_db(config, force_refresh=True)
        if remote_db is None:
            ui_logger.info(f"ã€{task_cat}ã€‘è¿œç¨‹ä»“åº“ä¼¼ä¹æ²¡æœ‰ 'database.json' æ–‡ä»¶ï¼Œå°†åˆ›å»ºä¸€ä¸ªæ–°çš„ã€‚", task_category=task_cat)
            remote_db = {"version": 2, "last_updated": "", "series": {}}
            remote_sha = None # æ–°å»ºæ–‡ä»¶æ²¡æœ‰ sha

        # æ­¥éª¤ 2: æ‰«ææœ¬åœ°æ–‡ä»¶
        ui_logger.info(f"ã€{task_cat}ã€‘[æ­¥éª¤ 2/5] æ­£åœ¨æ‰«ææœ¬åœ°æˆªå›¾æ–‡ä»¶å¤¹...", task_category=task_cat)
        upload_queue = []
        id_pattern = re.compile(r'\[(\d+)\]$')
        file_pattern = re.compile(r'season-(\d+)-episode-(\d+)\.jpg')

        for series_dir_name in os.listdir(local_screenshots_dir):
            series_dir_path = os.path.join(local_screenshots_dir, series_dir_name)
            if not os.path.isdir(series_dir_path): continue
            
            match_id = id_pattern.search(series_dir_name)
            if not match_id: continue
            tmdb_id = match_id.group(1)

            for filename in os.listdir(series_dir_path):
                match_file = file_pattern.match(filename)
                if not match_file: continue
                
                s_num, e_num = match_file.groups()
                episode_key = f"{s_num}-{e_num}"
                
                remote_series_data = remote_db.get("series", {}).get(tmdb_id, {})
                
                if episode_key not in remote_series_data or github_conf.overwrite_remote:
                    upload_queue.append({
                        "local_path": os.path.join(series_dir_path, filename),
                        "github_path": f"EpisodeScreenshots/{tmdb_id}/{filename}",
                        "tmdb_id": tmdb_id,
                        "episode_key": episode_key
                    })
        
        total_to_upload = len(upload_queue)
        ui_logger.info(f"ã€{task_cat}ã€‘[æ­¥éª¤ 2/5] æœ¬åœ°æ‰«æå®Œæˆï¼Œå…±å‘ç° {total_to_upload} ä¸ªéœ€è¦ä¸Šä¼ æˆ–æ›´æ–°çš„æˆªå›¾ã€‚", task_category=task_cat)
        if total_to_upload == 0:
            ui_logger.info(f"ã€{task_cat}ã€‘æœ¬åœ°ä¸è¿œç¨‹æ²¡æœ‰å·®å¼‚ï¼Œä»»åŠ¡å®Œæˆã€‚", task_category=task_cat)
            return {"uploaded_count": 0, "skipped_count": 0, "failed_count": 0}

        # æ­¥éª¤ 3: å¹¶å‘ä¸Šä¼ 
        ui_logger.info(f"ã€{task_cat}ã€‘[æ­¥éª¤ 3/5] å¼€å§‹å¹¶å‘ä¸Šä¼ æˆªå›¾ï¼Œè¯·ç¨å€™...", task_category=task_cat)
        task_manager.update_task_progress(task_id, 0, total_to_upload)
        
        successful_uploads = []
        failed_uploads = []
        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_item = {executor.submit(self._upload_file_to_github, item, github_conf): item for item in upload_queue}
            for i, future in enumerate(as_completed(future_to_item)):
                if cancellation_event.is_set():
                    ui_logger.warning(f"ã€{task_cat}ã€‘ä»»åŠ¡åœ¨ä¸Šä¼ é˜¶æ®µè¢«ç”¨æˆ·å–æ¶ˆã€‚", task_category=task_cat)
                    executor.shutdown(wait=False, cancel_futures=True)
                    return
                
                item = future_to_item[future]
                try:
                    download_url = future.result()
                    if download_url:
                        item["download_url"] = download_url
                        successful_uploads.append(item)
                    else:
                        failed_uploads.append(item)
                except Exception as e:
                    ui_logger.error(f"ã€{task_cat}ã€‘ä¸Šä¼ æ–‡ä»¶ '{item['local_path']}' å¤±è´¥: {e}", task_category=task_cat)
                    failed_uploads.append(item)
                
                task_manager.update_task_progress(task_id, i + 1, total_to_upload)

        ui_logger.info(f"ã€{task_cat}ã€‘[æ­¥éª¤ 3/5] æˆªå›¾ä¸Šä¼ å®Œæˆã€‚æˆåŠŸ: {len(successful_uploads)}, å¤±è´¥: {len(failed_uploads)}ã€‚", task_category=task_cat)
        if failed_uploads:
            ui_logger.error(f"ã€{task_cat}ã€‘ç”±äºå­˜åœ¨ä¸Šä¼ å¤±è´¥çš„æˆªå›¾ï¼Œä»»åŠ¡ä¸­æ­¢ï¼Œç´¢å¼•æ–‡ä»¶å°†ä¸ä¼šæ›´æ–°ã€‚", task_category=task_cat)
            return {"uploaded_count": len(successful_uploads), "skipped_count": 0, "failed_count": len(failed_uploads)}

        # æ­¥éª¤ 4: åˆå¹¶ç´¢å¼•
        ui_logger.info(f"ã€{task_cat}ã€‘[æ­¥éª¤ 4/5] æ­£åœ¨åˆå¹¶ç´¢å¼•æ–‡ä»¶...", task_category=task_cat)
        final_db = remote_db
        for item in successful_uploads:
            tmdb_id = item["tmdb_id"]
            episode_key = item["episode_key"]
            if tmdb_id not in final_db["series"]:
                final_db["series"][tmdb_id] = {}
            final_db["series"][tmdb_id][episode_key] = item["download_url"]
        
        final_db["last_updated"] = datetime.utcnow().isoformat() + "Z"

        # æ­¥éª¤ 5: æäº¤ç´¢å¼•
        ui_logger.info(f"ã€{task_cat}ã€‘[æ­¥éª¤ 5/5] æ­£åœ¨å°†æ›´æ–°åçš„ç´¢å¼•æäº¤åˆ° GitHub...", task_category=task_cat)
        if self._upload_db_to_github(final_db, remote_sha, github_conf):
            ui_logger.info(f"ã€{task_cat}ã€‘[æˆåŠŸğŸ‰] ç´¢å¼•æ–‡ä»¶æˆåŠŸæ›´æ–°ï¼å¤‡ä»½ä»»åŠ¡å…¨éƒ¨å®Œæˆã€‚", task_category=task_cat)
        else:
            ui_logger.error(f"ã€{task_cat}ã€‘[å¤±è´¥âŒ] ç´¢å¼•æ–‡ä»¶æäº¤å¤±è´¥ï¼å›¾ç‰‡å·²ä¸Šä¼ ä½†æœªè®°å½•ï¼Œè¯·é‡æ–°è¿è¡Œå¤‡ä»½ä»»åŠ¡ä»¥ä¿®å¤ç´¢å¼•ã€‚", task_category=task_cat)
            return {"uploaded_count": len(successful_uploads), "skipped_count": 0, "failed_count": len(failed_uploads) + 1}

        return {"uploaded_count": len(successful_uploads), "skipped_count": 0, "failed_count": 0}


    def _upload_file_to_github(self, item: Dict, github_conf) -> Optional[str]:
        task_cat = "å¤‡ä»½åˆ°GitHub"
        try:
            if github_conf.upload_cooldown > 0:
                ui_logger.debug(f"     - [GitHubä¸Šä¼ ] â±ï¸ ä¸Šä¼ å†·å´ {github_conf.upload_cooldown} ç§’...", task_category=task_cat)
                time.sleep(github_conf.upload_cooldown)

            with open(item["local_path"], "rb") as f:
                content_bytes = f.read()
            
            content_b64 = base64.b64encode(content_bytes).decode('utf-8')
            
            match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", github_conf.repo_url)
            owner, repo = match.groups()
            
            api_url = f"https://api.github.com/repos/{owner}/{repo}/contents/{item['github_path']}"
            
            sha = None
            try:
                get_headers = {
                    "Accept": "application/vnd.github.v3+json",
                    "Authorization": f"token {github_conf.personal_access_token}"
                }
                proxies = self.tmdb_logic.proxy_manager.get_proxies(api_url)
                get_resp = self.session.get(api_url, headers=get_headers, timeout=20, proxies=proxies)
                if get_resp.status_code == 200:
                    sha = get_resp.json().get('sha')
            except Exception:
                pass

            payload_dict = {
                "message": f"feat: Add screenshot for {item['github_path']}",
                "content": content_b64,
                "branch": github_conf.branch
            }
            if sha:
                payload_dict["sha"] = sha

            payload_json_str = json.dumps(payload_dict)

            command = [
                'curl',
                '-L',
                '-X', 'PUT',
                '-H', 'Accept: application/vnd.github.v3+json',
                '-H', f'Authorization: token {github_conf.personal_access_token}',
                '-H', 'Content-Type: application/json',
                '--data-binary', '@-',
                api_url
            ]
            
            proxies = self.tmdb_logic.proxy_manager.get_proxies(api_url)
            if proxies.get('https'):
                command.extend(['--proxy', proxies['https']])

            result = subprocess.run(command, input=payload_json_str, capture_output=True, text=True, check=False)
            
            response_data = {}
            try:
                response_data = json.loads(result.stdout)
            except json.JSONDecodeError:
                raise Exception(f"cURL è¿”å›äº†éJSONå“åº”: {result.stdout} | é”™è¯¯: {result.stderr}")

            if result.returncode != 0 or 'download_url' not in response_data.get('content', {}):
                 raise Exception(f"cURL ä¸Šä¼ å¤±è´¥ã€‚è¿”å›ç : {result.returncode}, è¾“å‡º: {result.stdout}, é”™è¯¯: {result.stderr}")

            return response_data["content"]["download_url"]

        except Exception as e:
            ui_logger.error(f"ã€{task_cat}ã€‘ä¸Šä¼ æ–‡ä»¶ '{item['local_path']}' æ—¶å‘ç”Ÿé”™è¯¯: {e}", task_category=task_cat)
            return None


    def _upload_db_to_github(self, db_content: Dict, sha: Optional[str], github_conf) -> bool:
        task_cat = "å¤‡ä»½åˆ°GitHub"
        try:
            # --- æ–°å¢ï¼šä¸Šä¼ å†·å´ ---
            if github_conf.upload_cooldown > 0:
                ui_logger.debug(f"     - [GitHubä¸Šä¼ ] â±ï¸ ä¸Šä¼ å†·å´ {github_conf.upload_cooldown} ç§’...", task_category=task_cat)
                time.sleep(github_conf.upload_cooldown)
            # --- ç»“æŸæ–°å¢ ---

            content_b64 = base64.b64encode(json.dumps(db_content, indent=2, ensure_ascii=False).encode('utf-8')).decode('utf-8')
            
            match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", github_conf.repo_url)
            owner, repo = match.groups()
            
            api_url = f"https://api.github.com/repos/{owner}/{repo}/contents/database.json"
            
            payload_dict = {
                "message": f"feat: Update database.json - {datetime.utcnow().isoformat()}",
                "content": content_b64,
                "branch": github_conf.branch
            }
            if sha:
                payload_dict["sha"] = sha

            payload_json_str = json.dumps(payload_dict)

            command = [
                'curl',
                '-L',
                '-X', 'PUT',
                '-H', 'Accept: application/vnd.github.v3+json',
                '-H', f'Authorization: token {github_conf.personal_access_token}',
                '-H', 'Content-Type: application/json',
                '--data-binary', '@-',
                api_url
            ]

            proxies = self.tmdb_logic.proxy_manager.get_proxies(api_url)
            if proxies.get('https'):
                command.extend(['--proxy', proxies['https']])

            result = subprocess.run(command, input=payload_json_str, capture_output=True, text=True, check=False)

            if result.returncode != 0:
                raise Exception(f"cURL ä¸Šä¼ å¤±è´¥ã€‚è¿”å›ç : {result.returncode}, è¾“å‡º: {result.stdout}, é”™è¯¯: {result.stderr}")
            
            try:
                response_json = json.loads(result.stdout)
                if "message" in response_json and "documentation_url" in response_json:
                     raise Exception(f"GitHub API è¿”å›é”™è¯¯: {response_json['message']}")
            except json.JSONDecodeError:
                pass

            return True
        except Exception as e:
            ui_logger.error(f"ã€{task_cat}ã€‘ä¸Šä¼  database.json æ—¶å‘ç”Ÿé”™è¯¯: {e}", task_category=task_cat)
            return False