# backend/upcoming_logic.py (æ–°æ–‡ä»¶)

import logging
import os
import json
import time
from typing import Dict, Any, Optional, List, Literal
from datetime import datetime, timedelta, timezone
from filelock import FileLock, Timeout

from models import AppConfig, UpcomingSubscriptionItem
from log_manager import ui_logger
from trakt_manager import TraktManager
from tmdb_logic import TmdbLogic
from notification_manager import notification_manager, escape_markdown

UPCOMING_SUBSCRIPTIONS_FILE = os.path.join('/app/data', 'upcoming_subscriptions.json')
UPCOMING_CACHE_FILE = os.path.join('/app/data', 'upcoming_cache.json')
CACHE_DURATION_HOURS = 2

class UpcomingLogic:
    def __init__(self, app_config: AppConfig):
        self.app_config = app_config
        self.config = app_config.upcoming_config
        self.trakt_manager = TraktManager(app_config)
        self.tmdb_logic = TmdbLogic(app_config)

    # backend/upcoming_logic.py (å‡½æ•°æ›¿æ¢)
    def _get_cached_list(self) -> Optional[List[Dict]]:
        task_cat = "å³å°†ä¸Šæ˜ -ç¼“å­˜"
        if not os.path.exists(UPCOMING_CACHE_FILE):
            return None
        try:
            with open(UPCOMING_CACHE_FILE, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            timestamp_str = cache_data.get("timestamp")
            if not timestamp_str:
                return None
            
            timestamp = datetime.fromisoformat(timestamp_str)
            age = datetime.now(timezone.utc) - timestamp
            
            # --- ä¿®æ”¹ ---
            if age < timedelta(hours=CACHE_DURATION_HOURS):
                remaining_time = timedelta(hours=CACHE_DURATION_HOURS) - age
                remaining_minutes = int(remaining_time.total_seconds() / 60)
                ui_logger.info(f"âœ… å‘½ä¸­ç¼“å­˜ï¼æ•°æ®å°†åœ¨çº¦ {remaining_minutes} åˆ†é’Ÿåè¿‡æœŸã€‚", task_category=task_cat)
                return cache_data.get("data")
            else:
                ui_logger.info(f"âš ï¸ ç¼“å­˜å·²è¿‡æœŸ (å­˜åœ¨æ—¶é•¿: {age})ï¼Œå°†é‡æ–°è·å–ã€‚", task_category=task_cat)
                return None
            # --- ä¿®æ”¹ç»“æŸ ---
        except (IOError, json.JSONDecodeError) as e:
            ui_logger.error(f"âŒ è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}", task_category=task_cat)
            return None

    
    def _save_to_cache(self, data: List[Dict]):
        task_cat = "å³å°†ä¸Šæ˜ -ç¼“å­˜"
        try:
            cache_data = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "data": data
            }
            with open(UPCOMING_CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=4)
            # --- ä¿®æ”¹ ---
            ui_logger.info(f"âœ… æˆåŠŸå°† {len(data)} æ¡æ•°æ®å†™å…¥ç¼“å­˜ï¼Œæœ‰æ•ˆæœŸ {CACHE_DURATION_HOURS} å°æ—¶ã€‚", task_category=task_cat)
            # --- ä¿®æ”¹ç»“æŸ ---
        except IOError as e:
            ui_logger.error(f"âŒ å†™å…¥ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}", task_category=task_cat)

    def _apply_3d_filtering(self, raw_items: List[Dict], filters: Dict) -> List[Dict]:
        task_cat = "å³å°†ä¸Šæ˜ -ç­›é€‰"
        ui_logger.info("â¡ï¸ [æ­¥éª¤ 2/3] å¼€å§‹åº”ç”¨ä¸‰ç»´åˆ†å±‚é¢„ç­›é€‰ç­–ç•¥...", task_category=task_cat)
        
        verified_items = []
        processed_ids = set()

        for item in raw_items:
            media_type = item.get('media_type')
            media_info = item.get('movie') if media_type == 'movie' else item.get('show')
            if not media_info: continue

            tmdb_id = media_info.get('ids', {}).get('tmdb')
            if not tmdb_id or tmdb_id in processed_ids: continue
            
            title = media_info.get('original_title') or media_info.get('title', 'N/A')
            
            # 1. ç±»å‹è¿‡æ»¤
            genres = media_info.get('genres', [])
            if any(genre in filters['genre_blacklist'] for genre in genres):
                logging.debug(f"  - [ä¸¢å¼ƒ] {title}: ç±»å‹åœ¨é»‘åå•ä¸­ ({genres})")
                continue

            # 2. å¸‚åœºä¼˜å…ˆçº§è¿‡æ»¤
            country = media_info.get('country', '')
            language = media_info.get('language', '')
            translations = media_info.get('available_translations', [])

            is_p0 = (country in filters['p0_countries']) or (language in filters['p0_languages'])
            is_p1 = (country in filters['p1_countries']) and ('zh' in translations)

            if is_p0 or is_p1:
                reason = "æ ¸å¿ƒå¸‚åœº" if is_p0 else "æ½œåŠ›å¸‚åœº"
                logging.debug(f"  - [ä¿ç•™] {title}: {reason}")
                
                release_date = item.get('released') or item.get('first_aired')
                if release_date:
                    release_date_str = datetime.fromisoformat(release_date.replace('Z', '+00:00')).strftime('%Y-%m-%d')
                    verified_items.append({'tmdb_id': tmdb_id, 'media_type': media_type, 'release_date': release_date_str})
                    processed_ids.add(tmdb_id)
            else:
                logging.debug(f"  - [ä¸¢å¼ƒ] {title}: ä½ä¼˜å…ˆçº§ (å›½å®¶: {country}, è¯­è¨€: {language})")

        ui_logger.info(f"âœ… [æ­¥éª¤ 2/3] é¢„ç­›é€‰å®Œæˆã€‚å€™é€‰æ¡ç›®ä» {len(raw_items)} ä¸ªå‡å°‘åˆ° {len(verified_items)} ä¸ªã€‚", task_category=task_cat)
        return verified_items

    def get_upcoming_list(self, dynamic_filters: Optional[Dict] = None) -> List[Dict]:
        task_cat = "å³å°†ä¸Šæ˜ -è·å–"
        
        # 1. å‚æ•°åˆå¹¶
        filters = self.config.filters.model_dump()
        if dynamic_filters and not dynamic_filters.get('use_defaults', True):
            ui_logger.info("ğŸ” æ£€æµ‹åˆ°åŠ¨æ€ç­›é€‰æ¡ä»¶ï¼Œå°†è¦†ç›–é»˜è®¤é…ç½®ã€‚", task_category=task_cat)
            for key, value in dynamic_filters.items():
                if key in filters:
                    filters[key] = value
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_list = self._get_cached_list()
        if cached_list:
            return cached_list

        # 2. ä» Trakt è·å–åŸå§‹æ•°æ®
        ui_logger.info(f"â¡ï¸ [æ­¥éª¤ 1/3] å¼€å§‹ä» Trakt è·å–æœªæ¥ {filters['fetch_days']} å¤©çš„æ—¥å†æ•°æ®...", task_category=task_cat)
        start_date = datetime.now().strftime('%Y-%m-%d')
        raw_items = []
        
        movies = self.trakt_manager.get_upcoming_calendar_raw('movies', start_date, filters['fetch_days'])
        if movies:
            for item in movies: item['media_type'] = 'movie'
            raw_items.extend(movies)
        
        shows = self.trakt_manager.get_upcoming_calendar_raw('shows', start_date, filters['fetch_days'])
        if shows:
            # ä»…ä¿ç•™æ–°å‰§é¦–æ’­
            new_shows = [item for item in shows if item.get('episode', {}).get('episode_type') == 'series_premiere']
            for item in new_shows: item['media_type'] = 'tv'
            raw_items.extend(new_shows)
        
        ui_logger.info(f"âœ… [æ­¥éª¤ 1/3] å®Œæˆã€‚å…±è·å–åˆ° {len(raw_items)} æ¡åŸå§‹è®°å½•ã€‚", task_category=task_cat)

        # 3. åº”ç”¨ç­›é€‰
        filtered_items = self._apply_3d_filtering(raw_items, filters)

        # 4. ä» TMDB è·å–æœ€ç»ˆè¯¦æƒ…
        ui_logger.info(f"â¡ï¸ [æ­¥éª¤ 3/3] å¼€å§‹ä» TMDB è·å– {len(filtered_items)} ä¸ªé¡¹ç›®çš„è¯¦ç»†ä¸­æ–‡ä¿¡æ¯...", task_category=task_cat)
        final_list = []
        for item in filtered_items:
            try:
                endpoint = f"{item['media_type']}/{item['tmdb_id']}"
                params = {'language': 'zh-CN', 'append_to_response': 'images'}
                details = self.tmdb_logic._tmdb_request(endpoint, params)
                
                final_list.append({
                    "tmdb_id": details['id'],
                    "media_type": item['media_type'],
                    "title": details.get('title') or details.get('name'),
                    "overview": details.get('overview'),
                    "poster_path": details.get('poster_path'),
                    "release_date": item['release_date']
                })
                time.sleep(0.1) # API ç¤¼è²Œæ€§å»¶è¿Ÿ
            except Exception as e:
                logging.error(f"è·å– TMDB è¯¦æƒ…å¤±è´¥ (ID: {item['tmdb_id']}): {e}")
        
        ui_logger.info(f"âœ… [æ­¥éª¤ 3/3] å®Œæˆã€‚æœ€ç»ˆç”Ÿæˆ {len(final_list)} æ¡é«˜è´¨é‡ç»“æœåˆ—è¡¨ã€‚", task_category=task_cat)
        
        # 5. å†™å…¥ç¼“å­˜å¹¶è¿”å›
        self._save_to_cache(final_list)
        return final_list

    def get_subscriptions(self) -> Dict[str, Any]:
        if not os.path.exists(UPCOMING_SUBSCRIPTIONS_FILE):
            return {}
        try:
            with open(UPCOMING_SUBSCRIPTIONS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (IOError, json.JSONDecodeError):
            return {}

    def add_subscription(self, item_data: Dict) -> bool:
        task_cat = "å³å°†ä¸Šæ˜ -è®¢é˜…"
        try:
            with FileLock(UPCOMING_SUBSCRIPTIONS_FILE + ".lock", timeout=5):
                subs = self.get_subscriptions()
                tmdb_id_str = str(item_data['tmdb_id'])
                if tmdb_id_str in subs:
                    ui_logger.warning(f"âš ï¸ ã€Š{item_data['title']}ã€‹å·²åœ¨è®¢é˜…åˆ—è¡¨ä¸­ã€‚", task_category=task_cat)
                    return True
                
                subs[tmdb_id_str] = UpcomingSubscriptionItem(**item_data).model_dump()
                
                with open(UPCOMING_SUBSCRIPTIONS_FILE, 'w', encoding='utf-8') as f:
                    json.dump(subs, f, ensure_ascii=False, indent=4)
                
                ui_logger.info(f"âœ… æˆåŠŸè®¢é˜…ã€Š{item_data['title']}ã€‹ï¼", task_category=task_cat)
                return True
        except Timeout:
            ui_logger.error("âŒ æ·»åŠ è®¢é˜…å¤±è´¥ï¼šè·å–æ–‡ä»¶é”è¶…æ—¶ã€‚", task_category=task_cat)
            return False
        except Exception as e:
            ui_logger.error(f"âŒ æ·»åŠ è®¢é˜…å¤±è´¥: {e}", task_category=task_cat)
            return False

    def remove_subscription(self, tmdb_id: int) -> bool:
        task_cat = "å³å°†ä¸Šæ˜ -è®¢é˜…"
        try:
            with FileLock(UPCOMING_SUBSCRIPTIONS_FILE + ".lock", timeout=5):
                subs = self.get_subscriptions()
                tmdb_id_str = str(tmdb_id)
                if tmdb_id_str not in subs:
                    return True
                
                title = subs[tmdb_id_str].get('title', f"ID {tmdb_id}")
                del subs[tmdb_id_str]
                
                with open(UPCOMING_SUBSCRIPTIONS_FILE, 'w', encoding='utf-8') as f:
                    json.dump(subs, f, ensure_ascii=False, indent=4)
                
                ui_logger.info(f"âœ… å·²å–æ¶ˆå¯¹ã€Š{title}ã€‹çš„è®¢é˜…ã€‚", task_category=task_cat)
                return True
        except Timeout:
            ui_logger.error("âŒ å–æ¶ˆè®¢é˜…å¤±è´¥ï¼šè·å–æ–‡ä»¶é”è¶…æ—¶ã€‚", task_category=task_cat)
            return False
        except Exception as e:
            ui_logger.error(f"âŒ å–æ¶ˆè®¢é˜…å¤±è´¥: {e}", task_category=task_cat)
            return False

    def check_and_notify(self):
        task_cat = "å®šæ—¶ä»»åŠ¡-è®¢é˜…é€šçŸ¥"
        ui_logger.info("â¡ï¸ å¼€å§‹æ£€æŸ¥è®¢é˜…åˆ—è¡¨å¹¶å‘é€é€šçŸ¥...", task_category=task_cat)
        
        if not self.app_config.telegram_config.enabled:
            ui_logger.warning("âš ï¸ Telegram é€šçŸ¥æœªå¯ç”¨ï¼Œä»»åŠ¡è·³è¿‡ã€‚", task_category=task_cat)
            return

        subs = self.get_subscriptions()
        if not subs:
            ui_logger.info("âœ… è®¢é˜…åˆ—è¡¨ä¸ºç©ºï¼Œæ— éœ€å‘é€é€šçŸ¥ã€‚", task_category=task_cat)
            return

        today = datetime.now().date()
        notifications = {0: [], 1: [], 2: [], 3: []} # 0:ä»Šå¤©, 1:æ˜å¤©, ...

        for tmdb_id, item_info in subs.items():
            try:
                release_date = datetime.strptime(item_info['release_date'], '%Y-%m-%d').date()
                delta_days = (release_date - today).days
                
                if 0 <= delta_days <= 3:
                    notifications[delta_days].append(item_info['title'])
            except (ValueError, KeyError):
                continue
        
        message_parts = []
        if notifications[0]:
            titles = "ã€".join([f"ã€Š{escape_markdown(t)}ã€‹" for t in notifications[0]])
            message_parts.append(f"ğŸ‰ *ä»Šæ—¥é¦–æ˜ *\n{titles}")
        
        upcoming_parts = []
        if notifications[1]:
            titles = "ã€".join([f"ã€Š{escape_markdown(t)}ã€‹" for t in notifications[1]])
            upcoming_parts.append(f"æ˜å¤©: {titles}")
        if notifications[2]:
            titles = "ã€".join([f"ã€Š{escape_markdown(t)}ã€‹" for t in notifications[2]])
            upcoming_parts.append(f"åå¤©: {titles}")
        if notifications[3]:
            titles = "ã€".join([f"ã€Š{escape_markdown(t)}ã€‹" for t in notifications[3]])
            upcoming_parts.append(f"3å¤©å: {titles}")
        
        if upcoming_parts:
            message_parts.append(f"ğŸ“… *å³å°†ä¸Šæ˜ *\n- " + "\n- ".join(upcoming_parts))

        if not message_parts:
            ui_logger.info("âœ… æ£€æŸ¥å®Œæ¯•ï¼Œæœªæ¥3å¤©å†…æ²¡æœ‰å³å°†ä¸Šæ˜ çš„è®¢é˜…é¡¹ç›®ã€‚", task_category=task_cat)
            return
            
        final_message = "ğŸ”” *è®¢é˜…æ—¥å†æé†’*\n\n" + "\n\n".join(message_parts)
        notification_manager.send_telegram_message(final_message, self.app_config)
        ui_logger.info("âœ… æˆåŠŸå‘é€è®¢é˜…é€šçŸ¥ï¼", task_category=task_cat)