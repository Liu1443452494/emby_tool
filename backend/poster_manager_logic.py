# backend/poster_manager_logic.py

import logging
import os
import json
import threading
import time
import re
import base64
import subprocess
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
from filelock import FileLock, Timeout
from datetime import datetime, timedelta

from log_manager import ui_logger
from models import AppConfig, PosterManagerConfig, ScheduledTasksTargetScope
from task_manager import TaskManager
from media_selector import MediaSelector
from proxy_manager import ProxyManager
import config as app_config_module

# --- å¸¸é‡å®šä¹‰ ---
AGGREGATED_INDEX_CACHE_FILE = os.path.join('/app/data', 'poster_manager_aggregated_index.json')
AGGREGATED_INDEX_CACHE_DURATION = 3600  # ç¼“å­˜1å°æ—¶ (3600ç§’)

class PosterManagerLogic:
    def __init__(self, config: AppConfig):
        self.config = config
        self.pm_config = config.poster_manager_config
        self.proxy_manager = ProxyManager(config)
        self.session = self._create_session()

    def _create_session(self):
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        return session
    
    def _get_emby_item_details(self, item_id: str, fields: str) -> Dict:
        """ä» Emby è·å–åª’ä½“é¡¹çš„è¯¦ç»†ä¿¡æ¯"""
        import requests
        try:
            url = f"{self.config.server_config.server}/Users/{self.config.server_config.user_id}/Items/{item_id}"
            params = {"api_key": self.config.server_config.api_key, "Fields": fields}
            proxies = self.proxy_manager.get_proxies(url)
            response = self.session.get(url, params=params, timeout=15, proxies=proxies)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logging.error(f"ã€æµ·æŠ¥ç®¡ç†ã€‘è·å–åª’ä½“é¡¹ {item_id} çš„è¯¦æƒ…å¤±è´¥: {e}")
            raise e

    def _get_tmdb_id(self, item_id: str) -> Optional[str]:
        """ä» Emby è·å–åª’ä½“é¡¹çš„ TMDB ID"""
        try:
            url = f"{self.config.server_config.server}/Users/{self.config.server_config.user_id}/Items/{item_id}"
            params = {"api_key": self.config.server_config.api_key, "Fields": "ProviderIds"}
            proxies = self.proxy_manager.get_proxies(url)
            response = self.session.get(url, params=params, timeout=15, proxies=proxies)
            response.raise_for_status()
            provider_ids = response.json().get("ProviderIds", {})
            return provider_ids.get("Tmdb")
        except Exception as e:
            logging.error(f"ã€æµ·æŠ¥å¤‡ä»½ã€‘è·å–åª’ä½“é¡¹ {item_id} çš„ TMDB ID å¤±è´¥: {e}")
            return None

    def _scan_local_cache(self, media_ids: List[str], content_types: List[str], task_cat: str) -> List[Dict]:
        """æ‰«ææœ¬åœ°ç¼“å­˜ç›®å½•ï¼Œç”Ÿæˆåˆå§‹å¾…åŠåˆ—è¡¨"""
        ui_logger.info(f"â¡ï¸ [é˜¶æ®µ1.2] å¼€å§‹æ‰«ææœ¬åœ°ç¼“å­˜ç›®å½•: {self.pm_config.local_cache_path}", task_category=task_cat)
        
        if not self.pm_config.local_cache_path or not os.path.isdir(self.pm_config.local_cache_path):
            raise ValueError(f"æœ¬åœ°ç¼“å­˜è·¯å¾„ '{self.pm_config.local_cache_path}' æ— æ•ˆæˆ–æœªé…ç½®ã€‚")

        type_map = {
            "poster": "poster.jpg",
            "logo": "clearlogo.png",
            "fanart": "fanart.jpg"
        }
        
        initial_pending_list = []
        tmdb_id_map = {}

        # æ‰¹é‡è·å–TMDB IDä»¥å‡å°‘APIè°ƒç”¨
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_id = {executor.submit(self._get_tmdb_id, item_id): item_id for item_id in media_ids}
            for future in future_to_id:
                item_id = future_to_id[future]
                try:
                    tmdb_id = future.result()
                    if tmdb_id:
                        tmdb_id_map[item_id] = tmdb_id
                except Exception as e:
                    logging.error(f"ã€æµ·æŠ¥å¤‡ä»½ã€‘è·å– TMDB ID æ—¶å‡ºé”™ (Emby ID: {item_id}): {e}")

        for item_id in media_ids:
            tmdb_id = tmdb_id_map.get(item_id)
            if not tmdb_id:
                ui_logger.warning(f"âš ï¸ è·³è¿‡ Emby ID: {item_id}ï¼Œå› ä¸ºå®ƒæ²¡æœ‰å…³è”çš„ TMDB IDã€‚", task_category=task_cat)
                continue

            item_cache_dir = os.path.join(self.pm_config.local_cache_path, tmdb_id)
            if not os.path.isdir(item_cache_dir):
                continue

            for content_type in content_types:
                filename = type_map.get(content_type)
                if not filename: continue
                
                filepath = os.path.join(item_cache_dir, filename)
                if os.path.isfile(filepath):
                    try:
                        file_size = os.path.getsize(filepath)
                        initial_pending_list.append({
                            "local_path": filepath,
                            "tmdb_id": tmdb_id,
                            "image_type": content_type,
                            "size": file_size
                        })
                    except OSError as e:
                        ui_logger.error(f"âŒ æ— æ³•è·å–æ–‡ä»¶å¤§å°: {filepath}ã€‚é”™è¯¯: {e}", task_category=task_cat)
        
        ui_logger.info(f"âœ… [é˜¶æ®µ1.2] æœ¬åœ°æ‰«æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(initial_pending_list)} ä¸ªå¾…å¤„ç†æ–‡ä»¶ã€‚", task_category=task_cat)
        return initial_pending_list

    def _get_repo_index(self, repo_config: Dict) -> Optional[Dict]:
        """è·å–å•ä¸ªä»“åº“çš„ database.json å†…å®¹"""
        repo_url = repo_config['repo_url']
        branch = repo_config.get('branch', 'main')
        pat = repo_config.get('personal_access_token') or self.pm_config.global_personal_access_token
        
        match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo_url)
        if not match:
            logging.error(f"ã€æµ·æŠ¥å¤‡ä»½ã€‘æ— æ•ˆçš„ GitHub ä»“åº“ URL: {repo_url}")
            return None
        owner, repo = match.groups()
        
        api_url = f"https://api.github.com/repos/{owner}/{repo}/contents/database.json?ref={branch}"
        headers = {"Accept": "application/vnd.github.v3+json"}
        if pat:
            headers["Authorization"] = f"token {pat}"
        
        try:
            proxies = self.proxy_manager.get_proxies(api_url)
            response = self.session.get(api_url, headers=headers, timeout=30, proxies=proxies)
            if response.status_code == 404:
                return {"version": 1, "last_updated": "", "images": {}} # ä»“åº“æ˜¯æ–°çš„ï¼Œè¿”å›ç©ºç´¢å¼•
            response.raise_for_status()
            
            content = base64.b64decode(response.json()['content']).decode('utf-8')
            return json.loads(content)
        except Exception as e:
            logging.error(f"ã€æµ·æŠ¥å¤‡ä»½ã€‘è·å–ä»“åº“ {repo_url} çš„ç´¢å¼•å¤±è´¥: {e}")
            return None

    def _get_aggregated_remote_index(self, task_cat: str, force_refresh: bool = False) -> Dict:
        """é€šè¿‡ç¼“å­˜æˆ–å¹¶å‘è·å–ï¼Œå¾—åˆ°èšåˆçš„æ‰€æœ‰è¿œç¨‹æ–‡ä»¶ç´¢å¼•"""
        ui_logger.info("â¡ï¸ [é˜¶æ®µ1.3] å¼€å§‹è·å–å¹¶èšåˆæ‰€æœ‰è¿œç¨‹ä»“åº“çš„ç´¢å¼•...", task_category=task_cat)
        
        lock_path = AGGREGATED_INDEX_CACHE_FILE + ".lock"
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šå¢åŠ ä¸€ä¸ªæ ‡å¿—ä½æ¥åˆ¤æ–­æ˜¯å¦éœ€è¦å†™ç¼“å­˜ ---
        should_write_cache = False

        if not force_refresh:
            try:
                with FileLock(lock_path, timeout=5):
                    if os.path.exists(AGGREGATED_INDEX_CACHE_FILE):
                        with open(AGGREGATED_INDEX_CACHE_FILE, 'r', encoding='utf-8') as f:
                            cache_data = json.load(f)
                        
                        cached_at = datetime.fromisoformat(cache_data.get("cached_at"))
                        now = datetime.now()
                        
                        if now - cached_at < timedelta(seconds=AGGREGATED_INDEX_CACHE_DURATION):
                            aggregated_index = cache_data.get("aggregated_index", {})
                            if aggregated_index:
                                age = now - cached_at
                                if age.total_seconds() < 60:
                                    age_str = f"{int(age.total_seconds())}ç§’å‰"
                                elif age.total_seconds() < 3600:
                                    age_str = f"{int(age.total_seconds() / 60)}åˆ†é’Ÿå‰"
                                else:
                                    age_str = f"{age.total_seconds() / 3600:.1f}å°æ—¶å‰"
                                
                                ui_logger.info(f"âœ… [é˜¶æ®µ1.3] å‘½ä¸­æœ¬åœ°èšåˆç´¢å¼•ç¼“å­˜ (æ›´æ–°äº {age_str})ã€‚", task_category=task_cat)
                                return aggregated_index
                            else:
                                ui_logger.warning("âš ï¸ æ£€æµ‹åˆ°æœ‰æ•ˆçš„ç©ºç¼“å­˜æ–‡ä»¶ï¼Œå¯èƒ½ç”±ä¹‹å‰çš„ç½‘ç»œé—®é¢˜å¯¼è‡´ï¼Œå°†å¼ºåˆ¶åˆ·æ–°ã€‚", task_category=task_cat)
                                should_write_cache = True # å‘ç°ç©ºç¼“å­˜ï¼Œæ ‡è®°éœ€è¦é‡å†™
                        else:
                            should_write_cache = True # ç¼“å­˜è¿‡æœŸï¼Œæ ‡è®°éœ€è¦é‡å†™
                    else:
                        should_write_cache = True # ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ ‡è®°éœ€è¦å†™å…¥
            except (Timeout, IOError, json.JSONDecodeError) as e:
                ui_logger.warning(f"âš ï¸ è¯»å–èšåˆç¼“å­˜å¤±è´¥ï¼Œå°†å¼ºåˆ¶ä»ç½‘ç»œè·å–ã€‚åŸå› : {e}", task_category=task_cat)
                should_write_cache = True # è¯»å–å¤±è´¥ï¼Œæ ‡è®°éœ€è¦é‡å†™
        else:
            ui_logger.info("   - [æ¨¡å¼] å·²å¯ç”¨å¼ºåˆ¶åˆ·æ–°ï¼Œå°†å¿½ç•¥æœ¬åœ°ç¼“å­˜ã€‚", task_category=task_cat)
            should_write_cache = True # å¼ºåˆ¶åˆ·æ–°ï¼Œæ ‡è®°éœ€è¦é‡å†™

        remote_file_map = {}
        repos = self.pm_config.github_repos
        total_repos = len(repos)
        successful_fetches = 0
        repos_with_data_count = 0
        
        ui_logger.info(f"   - [æ¨¡å¼] ç¼“å­˜æœªå‘½ä¸­æˆ–è¢«å¼ºåˆ¶åˆ·æ–°ï¼Œæ­£åœ¨ä»ç½‘ç»œå®æ—¶è·å–æ‰€æœ‰è¿œç¨‹ä»“åº“çš„ç´¢å¼•...", task_category=task_cat)
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_repo = {executor.submit(self._get_repo_index, repo.model_dump()): repo for repo in repos}
            for future in future_to_repo:
                repo_config = future_to_repo[future]
                try:
                    time.sleep(0.1)
                    index_content = future.result()
                    if index_content is not None:
                        successful_fetches += 1
                        if "images" in index_content and index_content["images"]:
                            repos_with_data_count += 1
                            for tmdb_id, images in index_content["images"].items():
                                for image_type, image_info in images.items():
                                    key = f"{tmdb_id}-{image_type}"
                                    remote_file_map[key] = image_info
                except Exception as e:
                    ui_logger.error(f"âŒ å¤„ç†ä»“åº“ {repo_config.repo_url} ç´¢å¼•æ—¶å‡ºé”™: {e}", task_category=task_cat)

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨æ ‡å¿—ä½æ¥å†³å®šæ˜¯å¦æ‰§è¡Œå†™å…¥å’Œæ—¥å¿—æ‰“å° ---
        if should_write_cache:
            total_records_aggregated = len(remote_file_map)
            log_message_prefix = "âœ… [é˜¶æ®µ5]" if force_refresh else "âœ… [é˜¶æ®µ1.3]"
            
            if successful_fetches == total_repos:
                try:
                    with FileLock(lock_path, timeout=10):
                        cache_content = {
                            "cached_at": datetime.now().isoformat(),
                            "aggregated_index": remote_file_map
                        }
                        with open(AGGREGATED_INDEX_CACHE_FILE, 'w', encoding='utf-8') as f:
                            json.dump(cache_content, f)
                    
                    if total_records_aggregated == 0:
                        ui_logger.info(f"{log_message_prefix} æˆåŠŸæ£€æŸ¥æ‰€æœ‰({total_repos}/{total_repos})ä»“åº“ï¼Œæ‰€æœ‰ç´¢å¼•å‡ä¸ºç©ºã€‚å·²å†™å…¥ä¸€ä¸ªç©ºçš„èšåˆç¼“å­˜æ–‡ä»¶ã€‚", task_category=task_cat)
                    else:
                        ui_logger.info(f"{log_message_prefix} æˆåŠŸèšåˆæ‰€æœ‰({total_repos}/{total_repos})ä»“åº“çš„ç´¢å¼•ã€‚å…±èšåˆæ¥è‡ª {repos_with_data_count} ä¸ªä»“åº“çš„ {total_records_aggregated} æ¡è®°å½•ï¼Œå¹¶å·²å†™å…¥æœ¬åœ°ç¼“å­˜ã€‚", task_category=task_cat)

                except Exception as e:
                    ui_logger.error(f"âŒ {log_message_prefix} å†™å…¥èšåˆç´¢å¼•ç¼“å­˜å¤±è´¥: {e}", task_category=task_cat)
            else:
                ui_logger.warning(f"âš ï¸ {log_message_prefix} æœªèƒ½æˆåŠŸè·å–æ‰€æœ‰ä»“åº“çš„ç´¢å¼•({successful_fetches}/{total_repos})ï¼Œèšåˆç¼“å­˜æ›´æ–°å¤±è´¥ã€‚", task_category=task_cat)
        
        return remote_file_map

    def _classify_pending_files(self, initial_list: List[Dict], remote_map: Dict, overwrite: bool, task_cat: str) -> Tuple[List, List]:
        """å°†å¾…åŠåˆ—è¡¨åˆ†ä¸ºæ–°å¢å’Œè¦†ç›–ä¸¤ç±»"""
        ui_logger.info("â¡ï¸ [é˜¶æ®µ2] å¼€å§‹å¯¹å¾…åŠæ–‡ä»¶è¿›è¡Œåˆ†ç±» (æ–°å¢/è¦†ç›–)...", task_category=task_cat)
        new_files = []
        overwrite_files = []
        # --- æ–°å¢ï¼šè·³è¿‡è®¡æ•°å™¨ ---
        skipped_count = 0
        
        for item in initial_list:
            key = f"{item['tmdb_id']}-{item['image_type']}"
            remote_info = remote_map.get(key)
            
            if not remote_info:
                new_files.append(item)
            elif overwrite:
                item['remote_info'] = remote_info
                overwrite_files.append(item)
            else:
                # --- æ–°å¢ï¼šå¢åŠ è·³è¿‡è®¡æ•° ---
                skipped_count += 1
        
        # --- ä¿®æ”¹ï¼šåœ¨æ—¥å¿—ä¸­åŠ å…¥è·³è¿‡æ•°é‡ ---
        ui_logger.info(f"âœ… [é˜¶æ®µ2] åˆ†ç±»å®Œæˆã€‚æ–°å¢: {len(new_files)} é¡¹, è¦†ç›–: {len(overwrite_files)} é¡¹, è·³è¿‡: {skipped_count} é¡¹ã€‚", task_category=task_cat)
        return new_files, overwrite_files

    # backend/poster_manager_logic.py (å‡½æ•°æ›¿æ¢)

    def _calculate_dispatch_plan(self, new_files: List, overwrite_files: List, task_cat: str) -> Dict:
        """æ ¸å¿ƒç®—æ³•ï¼šè®¡ç®—æ–‡ä»¶åˆ†å‘è®¡åˆ’ (æ‰“åŒ…åˆ†é… + é™çº§ç­–ç•¥)"""
        ui_logger.info("â¡ï¸ [é˜¶æ®µ3] æ­£åœ¨è¿›è¡Œç²¾ç¡®é¢„è®¡ç®—ï¼Œç”Ÿæˆæ–‡ä»¶åˆ†å‘è®¡åˆ’...", task_category=task_cat)
        
        dispatch_plan = {repo.repo_url: {"new": [], "overwrite": []} for repo in self.pm_config.github_repos}
        
        threshold_bytes = self.pm_config.repository_size_threshold_mb * 1024 * 1024
        temp_repo_states = {
            repo.repo_url: threshold_bytes - repo.state.size_bytes
            for repo in self.pm_config.github_repos
        }

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šå¯¹è¦†ç›–æ–‡ä»¶è¿›è¡Œåˆ†ç»„å¤„ç†å’Œæ—¥å¿—ä¼˜åŒ– ---
        # 1. åˆ†ç»„è¦†ç›–æ–‡ä»¶
        grouped_overwrite_files = {}
        for item in overwrite_files:
            key = (item['tmdb_id'], item['remote_info']['repo_url'])
            if key not in grouped_overwrite_files:
                grouped_overwrite_files[key] = {"files": [], "total_delta": 0}
            
            new_size = item['size']
            old_size = item['remote_info']['size']
            delta = new_size - old_size
            
            grouped_overwrite_files[key]["files"].append(item)
            grouped_overwrite_files[key]["total_delta"] += delta

        # 2. å¤„ç†åˆ†ç»„åçš„è¦†ç›–æ–‡ä»¶
        for (tmdb_id, repo_url), group in grouped_overwrite_files.items():
            total_delta = group['total_delta']
            
            if temp_repo_states.get(repo_url, -1) < total_delta:
                file_names = ', '.join([os.path.basename(f['local_path']) for f in group['files']])
                raise ValueError(f"æ–‡ä»¶è¦†ç›–å¤±è´¥ï¼šè¦†ç›– TMDB ID {tmdb_id} çš„æ–‡ä»¶ ({file_names}) å°†å¯¼è‡´ä»“åº“ {repo_url} è¶…å‡ºå®¹é‡é™åˆ¶ã€‚")
            
            for item in group['files']:
                dispatch_plan[repo_url]["overwrite"].append(item)
            
            temp_repo_states[repo_url] -= total_delta
            
            file_names_str = ' '.join([os.path.basename(f['local_path']) for f in group['files']])
            ui_logger.info(f"  - [è®¡åˆ’-è¦†ç›–] [{tmdb_id}] -({file_names_str})-> åˆ†é…è‡³åŸä»“åº“ {repo_url} (ç©ºé—´å˜åŒ–: {total_delta/1024/1024:+.2f} MB)", task_category=task_cat)
        # --- ä¿®æ”¹ç»“æŸ ---

        # 3. æ–‡ä»¶åˆ†ç»„ (æ–°å¢æ–‡ä»¶)
        grouped_new_files = {}
        for item in new_files:
            tmdb_id = item['tmdb_id']
            if tmdb_id not in grouped_new_files:
                grouped_new_files[tmdb_id] = {"files": [], "total_size": 0}
            grouped_new_files[tmdb_id]["files"].append(item)
            grouped_new_files[tmdb_id]["total_size"] += item['size']

        # 4. æ‰“åŒ…åˆ†é… (æ–°å¢æ–‡ä»¶)
        for tmdb_id, group in grouped_new_files.items():
            allocated_as_group = False
            # ä¸»ç­–ç•¥ï¼šå°è¯•æ•´ä½“æ”¾å…¥
            for repo in self.pm_config.github_repos:
                if temp_repo_states.get(repo.repo_url, -1) >= group['total_size']:
                    for item in group['files']:
                        dispatch_plan[repo.repo_url]["new"].append(item)
                    temp_repo_states[repo.repo_url] -= group['total_size']
                    ui_logger.info(f"  - [è®¡åˆ’-æ‰“åŒ…] [{tmdb_id}] å›¾ç‰‡ç»„ (å…± {len(group['files'])} é¡¹, {group['total_size']/1024/1024:.2f} MB) -> åˆ†é…è‡³ {repo.repo_url}", task_category=task_cat)
                    allocated_as_group = True
                    break
            
            # é™çº§ç­–ç•¥ï¼šå¦‚æœæ— æ³•æ•´ä½“æ”¾å…¥ï¼Œåˆ™é€ä¸ªåˆ†é…
            if not allocated_as_group:
                ui_logger.warning(f"  - âš ï¸ [è®¡åˆ’-é™çº§] [{tmdb_id}] å›¾ç‰‡ç»„ (æ€»å¤§å° {group['total_size']/1024/1024:.2f} MB) æ— æ³•æ•´ä½“æ”¾å…¥ä»»ä½•ä»“åº“ï¼Œå°†å°è¯•å•ç‹¬åˆ†é…...", task_category=task_cat)
                for item in group['files']:
                    allocated_individually = False
                    for repo in self.pm_config.github_repos:
                        if temp_repo_states.get(repo.repo_url, -1) >= item['size']:
                            dispatch_plan[repo.repo_url]["new"].append(item)
                            temp_repo_states[repo.repo_url] -= item['size']
                            ui_logger.info(f"    - [è®¡åˆ’-é™çº§åˆ†é…] {os.path.basename(item['local_path'])} ({item['size']/1024/1024:.2f} MB) -> åˆ†é…è‡³ {repo.repo_url}", task_category=task_cat)
                            allocated_individually = True
                            break
                    if not allocated_individually:
                         raise ValueError(f"æ–‡ä»¶åˆ†é…å¤±è´¥ï¼šæ–‡ä»¶ {item['local_path']} ({item['size']/1024/1024:.2f} MB) è¿‡å¤§ï¼Œæ‰€æœ‰ä»“åº“å‡æ— è¶³å¤Ÿç©ºé—´å®¹çº³ã€‚")

        ui_logger.info("âœ… [é˜¶æ®µ3] æ–‡ä»¶åˆ†å‘è®¡åˆ’åˆ¶å®šæˆåŠŸã€‚", task_category=task_cat)
        return dispatch_plan

    def _execute_github_write_request(self, method: str, url: str, pat: str, payload: Optional[Dict] = None) -> Dict:
        """é€šè¿‡ curl æ‰§è¡Œ GitHub å†™å…¥æ“ä½œï¼ˆæ— é‡è¯•ï¼‰"""
        command = [
            'curl', '-L', '-X', method,
            '-H', 'Accept: application/vnd.github.v3+json',
            '-H', f'Authorization: token {pat}',
            '-H', 'Content-Type: application/json'
        ]
        
        json_payload_str = ""
        if payload:
            command.extend(['--data-binary', '@-'])
            json_payload_str = json.dumps(payload)

        proxies = self.proxy_manager.get_proxies(url)
        if proxies.get('https'):
            command.extend(['--proxy', proxies['https']])

        command.append(url)

        result = subprocess.run(command, input=json_payload_str, capture_output=True, text=True, check=False)
        
        response_data = {}
        try:
            if result.stdout:
                response_data = json.loads(result.stdout)
        except json.JSONDecodeError:
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šå°† curl çš„ stderr åŠ å…¥å¼‚å¸¸ä¿¡æ¯ ---
            raise Exception(f"cURL è¿”å›äº†éJSONå“åº”: {result.stdout or 'æ— è¾“å‡º'} | é”™è¯¯: {result.stderr or 'æ— é”™è¯¯ä¿¡æ¯'}")

        if result.returncode != 0 or (response_data.get("message") and response_data.get("documentation_url")):
            error_message = response_data.get('message', f"cURL é”™è¯¯: {result.stderr}")
            if response_data.get('status') == '422' and "sha" in error_message:
                error_message = f"æ— æ•ˆè¯·æ±‚ (422)ã€‚æœåŠ¡å™¨æç¤º 'sha' å‚æ•°æœ‰é—®é¢˜ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºåœ¨æ‚¨æ“ä½œæœŸé—´ï¼Œæ–‡ä»¶è¢«å…¶ä»–è¿›ç¨‹ä¿®æ”¹ã€‚è¯·é‡è¯•ã€‚({error_message})"
            elif "409 Conflict" in result.stderr:
                error_message = "GitHub API è¿”å› 409 Conflict é”™è¯¯ï¼Œè¿™é€šå¸¸æ˜¯å¹¶å‘å†™å…¥å†²çªå¯¼è‡´çš„ã€‚è¯·ç¨åé‡è¯•ã€‚"
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šå°† curl çš„ stderr åŠ å…¥å¼‚å¸¸ä¿¡æ¯ ---
            elif "schannel: failed to receive handshake" in result.stderr or "curl: (35)" in result.stderr:
                error_message = f"SSL/TLS æ¡æ‰‹å¤±è´¥ã€‚è¿™é€šå¸¸æ˜¯ä¸´æ—¶çš„ç½‘ç»œæˆ–ä»£ç†é—®é¢˜ã€‚é”™è¯¯: {result.stderr}"

            raise Exception(f"GitHub API é”™è¯¯: {error_message}")

        return response_data
    
    def _execute_github_write_request_with_retry(self, method: str, url: str, pat: str, payload: Optional[Dict] = None, task_cat: str = "GitHubå†™å…¥") -> Dict:
        """
        æ‰§è¡Œ GitHub å†™å…¥æ“ä½œï¼Œå¹¶å¢åŠ äº†é’ˆå¯¹ç½‘ç»œé”™è¯¯çš„é‡è¯•é€»è¾‘ã€‚
        """
        max_retries = 3
        retry_delay = 5  # seconds
        for attempt in range(max_retries):
            try:
                return self._execute_github_write_request(method, url, pat, payload)
            except Exception as e:
                # åªå¯¹ç‰¹å®šçš„ã€å¯èƒ½æ˜¯ç¬æ€çš„ç½‘ç»œé”™è¯¯è¿›è¡Œé‡è¯•
                error_str = str(e).lower()
                if "ssl/tls" in error_str or "handshake" in error_str or "curl: (35)" in error_str:
                    if attempt < max_retries - 1:
                        ui_logger.warning(f"  - âš ï¸ ç½‘ç»œæ“ä½œå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries})ï¼Œå°†åœ¨ {retry_delay} ç§’åé‡è¯•... åŸå› : {e}", task_category=task_cat)
                        time.sleep(retry_delay)
                        continue
                # å¯¹äºå…¶ä»–é”™è¯¯æˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåˆ™ç›´æ¥æŠ›å‡ºå¼‚å¸¸
                raise e
        # è¿™è¡Œä»£ç ç†è®ºä¸Šä¸ä¼šè¢«æ‰§è¡Œï¼Œä½†ä¸ºäº†ä»£ç å®Œæ•´æ€§ä¿ç•™
        raise Exception("é‡è¯•é€»è¾‘æ‰§è¡Œå®Œæ¯•ä½†æœªèƒ½æˆåŠŸã€‚")


    def _get_latest_repo_size(self, repo_url: str, pat: str) -> int:
        """è·å–ä»“åº“çš„æœ€æ–°å¤§å° (KB)"""
        match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo_url)
        owner, repo = match.groups()
        api_url = f"https://api.github.com/repos/{owner}/{repo}"
        headers = {"Accept": "application/vnd.github.v3+json"}
        if pat:
            headers["Authorization"] = f"token {pat}"
        
        proxies = self.proxy_manager.get_proxies(api_url)
        response = self.session.get(api_url, headers=headers, timeout=30, proxies=proxies)
        response.raise_for_status()
        return response.json().get('size', 0)

    # backend/poster_manager_logic.py (å‡½æ•°æ›¿æ¢)

    def _execute_dispatch_plan(self, dispatch_plan: Dict, task_cat: str, cancellation_event: threading.Event):
        """æ‰§è¡Œæ–‡ä»¶ä¸Šä¼ å’Œç´¢å¼•æ›´æ–°"""
        ui_logger.info("â¡ï¸ [é˜¶æ®µ4] å¼€å§‹æ‰§è¡Œæ–‡ä»¶ä¸Šä¼ å’Œç´¢å¼•æ›´æ–°...", task_category=task_cat)
        
        # æŒ‰ä¼˜å…ˆçº§å¤„ç†ä»“åº“
        for repo_config in self.pm_config.github_repos:
            repo_url = repo_config.repo_url
            plan = dispatch_plan.get(repo_url)
            if not plan or (not plan['new'] and not plan['overwrite']):
                continue

            if cancellation_event.is_set():
                ui_logger.warning("âš ï¸ ä»»åŠ¡åœ¨æ‰§è¡Œé˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                return

            ui_logger.info(f"  - æ­£åœ¨å¤„ç†ä»“åº“: {repo_url}", task_category=task_cat)
            pat = repo_config.personal_access_token or self.pm_config.global_personal_access_token
            branch = repo_config.branch
            match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo_url)
            owner, repo_name = match.groups()

            # --- æ ¸å¿ƒä¿®æ”¹ï¼šä¸ºåŠ é”æ“ä½œå¢åŠ ä¸“é—¨çš„ try...except å— ---
            try:
                # 1. å°è¯•åŠ é”
                lock_path = ".lock"
                lock_api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/{lock_path}"
                lock_payload = {
                    "message": f"feat: Acquire lock for task",
                    "content": base64.b64encode(f"locked_at: {datetime.now().isoformat()}".encode()).decode(),
                    "branch": branch
                }
                self._execute_github_write_request_with_retry("PUT", lock_api_url, pat, lock_payload, task_cat=task_cat)
                ui_logger.info(f"    - ğŸ”’ å·²æˆåŠŸåœ¨ä»“åº“ {repo_url} ä¸­åˆ›å»º .lock æ–‡ä»¶ã€‚", task_category=task_cat)
            
            except Exception as e:
                if "422" in str(e) or "Unprocessable Entity" in str(e):
                    error_message = (
                        f"âŒ æ— æ³•é”å®šä»“åº“ {repo_url}ï¼Œä»»åŠ¡ä¸­æ­¢ï¼\n"
                        f"    - **å¯èƒ½åŸå› **: ä¸Šä¸€æ¬¡å¤‡ä»½ä»»åŠ¡å¼‚å¸¸ä¸­æ–­ï¼Œå¯¼è‡´ .lock æ–‡ä»¶æœªèƒ½è¢«è‡ªåŠ¨åˆ é™¤ã€‚\n"
                        f"    - **ä¿®å¤å»ºè®®**: è¯·æ‰‹åŠ¨å‰å¾€è¯¥ GitHub ä»“åº“ï¼Œæ£€æŸ¥å¹¶åˆ é™¤æ ¹ç›®å½•ä¸‹çš„ `.lock` æ–‡ä»¶åï¼Œå†é‡æ–°è¿è¡Œå¤‡ä»½ä»»åŠ¡ã€‚\n"
                        f"    - **è¡¥å……è¯´æ˜**: å¦‚æœæ‚¨ç¡®è®¤æ²¡æœ‰å…¶ä»–ä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œåˆ é™¤ .lock æ–‡ä»¶æ˜¯å®‰å…¨çš„æ“ä½œã€‚é‡æ–°è¿è¡Œä¸€æ¬¡å®Œæ•´çš„å¤‡ä»½ä»»åŠ¡å¯ä»¥ä¿®å¤ä»»ä½•æ½œåœ¨çš„ç´¢å¼•ä¸ä¸€è‡´é—®é¢˜ã€‚"
                    )
                    ui_logger.error(error_message, task_category=task_cat)
                    # æŠ›å‡ºä¸€ä¸ªæ›´æ˜ç¡®çš„å¼‚å¸¸ï¼Œè®©å¤–å±‚çŸ¥é“æ˜¯é”çš„é—®é¢˜
                    raise Exception(f"è·å–ä»“åº“ {repo_url} çš„é”å¤±è´¥ã€‚")
                else:
                    # å¦‚æœæ˜¯å…¶ä»–é”™è¯¯ï¼Œåˆ™åŸæ ·æŠ›å‡º
                    raise e
            # --- ä¿®æ”¹ç»“æŸ ---

            try:
                # 2. è·å–æœ€æ–°ç´¢å¼•
                current_index = self._get_repo_index(repo_config.model_dump())
                if current_index is None:
                    raise Exception("è·å–æœ€æ–°ç´¢å¼•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ã€‚")

                # 3. ä¸Šä¼ æ–‡ä»¶
                files_to_process = plan['overwrite'] + plan['new']
                for item in files_to_process:
                    if cancellation_event.is_set(): raise InterruptedError("ä»»åŠ¡è¢«å–æ¶ˆ")
                    
                    action_type = 'æ–°å¢'
                    
                    cooldown = self.pm_config.file_upload_cooldown_seconds
                    if cooldown > 0:
                        ui_logger.info(f"    - â±ï¸ æ–‡ä»¶ä¸Šä¼ å†·å´ {cooldown} ç§’...", task_category=task_cat)
                        time.sleep(cooldown)
                    
                    with open(item['local_path'], 'rb') as f:
                        content_b64 = base64.b64encode(f.read()).decode()
                    
                    github_path = f"images/{item['tmdb_id']}/{os.path.basename(item['local_path'])}"
                    api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/{github_path}"
                    
                    payload = {
                        "message": f"feat: Add/Update {item['image_type']} for {item['tmdb_id']}",
                        "content": content_b64,
                        "branch": branch
                    }

                    is_overwrite = 'remote_info' in item
                    if is_overwrite:
                        payload['sha'] = item['remote_info']['sha']
                        action_type = 'è¦†ç›–'

                    try:
                        response_data = self._execute_github_write_request_with_retry("PUT", api_url, pat, payload, task_cat=task_cat)
                    except Exception as e:
                        if "422" in str(e) and not is_overwrite:
                            ui_logger.info(f"    - ğŸ”„ æ–‡ä»¶ {github_path} å·²å­˜åœ¨ï¼Œè§¦å‘â€œæ–°å¢è½¬è¦†ç›–â€å®¹é”™æœºåˆ¶...", task_category=task_cat)
                            action_type = 'è¦†ç›–'
                            get_resp = self.session.get(api_url, headers={"Authorization": f"token {pat}"}, proxies=self.proxy_manager.get_proxies(api_url)).json()
                            payload['sha'] = get_resp['sha']
                            response_data = self._execute_github_write_request_with_retry("PUT", api_url, pat, payload, task_cat=task_cat)
                        else:
                            raise e
                    
                    if action_type == 'è¦†ç›–':
                        ui_logger.info(f"    - âœ… è¦†ç›–ä¸Šä¼ æˆåŠŸ: {github_path}", task_category=task_cat)
                    else:
                        ui_logger.info(f"    - â¬†ï¸ æ–°å¢ä¸Šä¼ æˆåŠŸ: {github_path}", task_category=task_cat)

                    # 4. æ›´æ–°å†…å­˜ä¸­çš„ç´¢å¼•
                    tmdb_id_str = str(item['tmdb_id'])
                    if tmdb_id_str not in current_index['images']:
                        current_index['images'][tmdb_id_str] = {}
                    
                    current_index['images'][tmdb_id_str][item['image_type']] = {
                        "repo_url": repo_url,
                        "sha": response_data['content']['sha'],
                        "size": response_data['content']['size'],
                        "url": response_data['content']['download_url']
                    }

                # 5. æäº¤æ›´æ–°åçš„ç´¢å¼•
                current_index['last_updated'] = datetime.now().isoformat()
                index_api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/database.json"
                
                get_index_resp = self.session.get(index_api_url, headers={"Authorization": f"token {pat}"}, proxies=self.proxy_manager.get_proxies(index_api_url)).json()
                index_sha = get_index_resp.get('sha')

                index_payload = {
                    "message": f"chore: Update database index",
                    "content": base64.b64encode(json.dumps(current_index, indent=2).encode()).decode(),
                    "branch": branch
                }
                if index_sha:
                    index_payload['sha'] = index_sha
                
                self._execute_github_write_request_with_retry("PUT", index_api_url, pat, index_payload, task_cat=task_cat)
                ui_logger.info(f"    - ç´¢å¼•æ–‡ä»¶ database.json æ›´æ–°æˆåŠŸã€‚", task_category=task_cat)

            finally:
                # 7. è§£é”
                lock_get_resp = self.session.get(lock_api_url, headers={"Authorization": f"token {pat}"}, proxies=self.proxy_manager.get_proxies(lock_api_url)).json()
                lock_sha = lock_get_resp.get('sha')
                if lock_sha:
                    delete_payload = {
                        "message": "feat: Release lock",
                        "sha": lock_sha,
                        "branch": branch
                    }
                    self._execute_github_write_request_with_retry("DELETE", lock_api_url, pat, delete_payload, task_cat=task_cat)
                    ui_logger.info(f"    - ğŸ”“ å·²æˆåŠŸä»ä»“åº“ {repo_url} ä¸­ç§»é™¤ .lock æ–‡ä»¶ã€‚", task_category=task_cat)

        ui_logger.info("âœ… [é˜¶æ®µ4] æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å’Œç´¢å¼•æ›´æ–°å®Œæˆã€‚", task_category=task_cat)

    def _update_all_repo_sizes(self, task_cat: str):
        """
        åœ¨ä»»åŠ¡ç»“æŸåï¼Œæ ¹æ®æœ€æ–°çš„èšåˆç´¢å¼•ï¼Œç»Ÿä¸€è®¡ç®—å¹¶å›å†™æ‰€æœ‰ä»“åº“çš„å®¹é‡çŠ¶æ€ã€‚
        """
        ui_logger.info(f"â¡ï¸ [é˜¶æ®µ5] æ­£åœ¨æ ¹æ®æœ€æ–°ç´¢å¼•ç»Ÿä¸€æ›´æ–°æ‰€æœ‰ä»“åº“çš„å®¹é‡çŠ¶æ€...", task_category=task_cat)
        
        if not os.path.exists(AGGREGATED_INDEX_CACHE_FILE):
            ui_logger.warning("âš ï¸ æœªæ‰¾åˆ°èšåˆç´¢å¼•ç¼“å­˜æ–‡ä»¶ï¼Œè·³è¿‡å®¹é‡æ›´æ–°ã€‚", task_category=task_cat)
            return
        
        with open(AGGREGATED_INDEX_CACHE_FILE, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        aggregated_index = cache_data.get("aggregated_index", {})

        repo_sizes = {}
        for key, image_info in aggregated_index.items():
            repo_url = image_info.get("repo_url")
            size = image_info.get("size", 0)
            if repo_url:
                repo_sizes[repo_url] = repo_sizes.get(repo_url, 0) + size
        
        current_app_config = app_config_module.load_app_config()
        updated_repos = []
        for repo in current_app_config.poster_manager_config.github_repos:
            new_size_bytes = repo_sizes.get(repo.repo_url, 0)
            if repo.state.size_bytes != new_size_bytes:
                repo.state.size_bytes = new_size_bytes
                repo.state.last_checked = datetime.now().isoformat()
                updated_repos.append(repo)
        
        if updated_repos:
            app_config_module.save_app_config(current_app_config)
            self.pm_config = current_app_config.poster_manager_config
            for repo in updated_repos:
                match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo.repo_url)
                name = f"{match.group(1)}/{match.group(2)}" if match else repo.repo_url
                
                # --- æ ¸å¿ƒä¿®æ”¹ï¼šåº”ç”¨æ–°çš„æ—¥å¿—æ ¼å¼åŒ–è§„åˆ™ ---
                size_in_gb = repo.state.size_bytes / (1024 * 1024 * 1024)
                if size_in_gb >= 1:
                    ui_logger.info(f"   - [å®¹é‡æ›´æ–°] ä»“åº“ {name} æœ€æ–°å®¹é‡ä¸º {size_in_gb:.2f} GBã€‚", task_category=task_cat)
                else:
                    size_in_mb = repo.state.size_bytes / (1024 * 1024)
                    ui_logger.info(f"   - [å®¹é‡æ›´æ–°] ä»“åº“ {name} æœ€æ–°å®¹é‡ä¸º {size_in_mb:.2f} MBã€‚", task_category=task_cat)
                # --- ä¿®æ”¹ç»“æŸ ---
        else:
            ui_logger.info("   - [å®¹é‡æ›´æ–°] æ‰€æœ‰ä»“åº“å®¹é‡å‡æ— å˜åŒ–ã€‚", task_category=task_cat)

    def _restore_single_item(self, item_id: str, tmdb_id: str, content_types: List[str], remote_map: Dict, task_cat: str):
        """æ¢å¤å•ä¸ªåª’ä½“é¡¹çš„å›¾ç‰‡"""
        import requests
        
        item_name = ""
        try:
            # å°è¯•è·å– Emby åª’ä½“é¡¹åç§°ç”¨äºæ—¥å¿—è®°å½•
            details = self._get_emby_item_details(item_id, "Name")
            item_name = details.get("Name", f"ID {item_id}")
        except Exception:
            item_name = f"ID {item_id}"

        ui_logger.info(f"  -> æ­£åœ¨ä¸ºã€{item_name}ã€‘æ‰§è¡Œæ¢å¤...", task_category=task_cat)
        
        for image_type in content_types:
            key = f"{tmdb_id}-{image_type}"
            image_info = remote_map.get(key)

            if not image_info:
                ui_logger.debug(f"     - è·³è¿‡: åœ¨è¿œç¨‹å¤‡ä»½ä¸­æœªæ‰¾åˆ°ã€{item_name}ã€‘çš„ {image_type} å›¾ç‰‡ã€‚", task_category=task_cat)
                continue

            image_url = image_info.get("url")
            if not image_url:
                ui_logger.warning(f"     - âš ï¸ è­¦å‘Š: è¿œç¨‹å¤‡ä»½ä¸­ã€{item_name}ã€‘çš„ {image_type} å›¾ç‰‡è®°å½•ç¼ºå°‘ä¸‹è½½URLã€‚", task_category=task_cat)
                continue

            try:
                # åº”ç”¨å›¾ç‰‡ä¸‹è½½å†·å´
                time.sleep(self.pm_config.image_download_cooldown_seconds)
                
                ui_logger.debug(f"     - æ­£åœ¨ä¸‹è½½: {image_url}", task_category=task_cat)
                proxies = self.proxy_manager.get_proxies(image_url)
                image_response = self.session.get(image_url, timeout=60, proxies=proxies)
                image_response.raise_for_status()
                image_data = image_response.content
                
                # å‡†å¤‡ä¸Šä¼ åˆ° Emby
                emby_image_type_map = {
                    "poster": "Primary",
                    "logo": "Logo",
                    "fanart": "Backdrop"
                }
                emby_image_type = emby_image_type_map.get(image_type)
                if not emby_image_type: continue

                upload_url = f"{self.config.server_config.server}/Items/{item_id}/Images/{emby_image_type}"
                
                # åˆ é™¤æ—§å›¾
                try:
                    delete_proxies = self.proxy_manager.get_proxies(upload_url)
                    self.session.delete(upload_url, params={"api_key": self.config.server_config.api_key}, timeout=20, proxies=delete_proxies)
                except requests.RequestException:
                    # åˆ é™¤å¤±è´¥æ˜¯æ­£å¸¸æƒ…å†µï¼ˆå¯èƒ½åŸæœ¬å°±æ²¡æœ‰å›¾ï¼‰
                    pass

                # ä¸Šä¼ æ–°å›¾
                headers = {'Content-Type': image_response.headers.get('Content-Type', 'image/jpeg')}
                upload_proxies = self.proxy_manager.get_proxies(upload_url)
                upload_response = self.session.post(
                    upload_url, 
                    params={"api_key": self.config.server_config.api_key}, 
                    data=image_data, 
                    headers=headers, 
                    timeout=60,
                    proxies=upload_proxies
                )
                upload_response.raise_for_status()
                ui_logger.info(f"     - âœ… æˆåŠŸæ¢å¤ã€{item_name}ã€‘çš„ {image_type} å›¾ç‰‡ã€‚", task_category=task_cat)

            except Exception as e:
                ui_logger.error(f"     - âŒ æ¢å¤ã€{item_name}ã€‘çš„ {image_type} å›¾ç‰‡å¤±è´¥: {e}", task_category=task_cat)


    def start_restore_task(
        self,
        scope: ScheduledTasksTargetScope,
        content_types: List[str],
        cancellation_event: threading.Event,
        task_id: str,
        task_manager: TaskManager
    ):
        """ä» GitHub æ¢å¤å›¾ç‰‡åˆ° Emby çš„ä¸»ä»»åŠ¡æµç¨‹"""
        task_cat = f"æµ·æŠ¥æ¢å¤({scope.mode})"
        ui_logger.info(f"ğŸ‰ ä»»åŠ¡å¯åŠ¨ï¼Œæ¨¡å¼: {scope.mode}, å†…å®¹: {content_types}", task_category=task_cat)

        try:
            # é˜¶æ®µä¸€ï¼šé¢„å¤„ç†
            ui_logger.info("â¡ï¸ [é˜¶æ®µ1/2] æ­£åœ¨è·å–è¿œç¨‹ç´¢å¼•å’Œåª’ä½“åˆ—è¡¨...", task_category=task_cat)
            remote_map = self._get_aggregated_remote_index(task_cat)
            if not remote_map:
                raise ValueError("æ— æ³•è·å–è¿œç¨‹èšåˆç´¢å¼•ï¼Œä»»åŠ¡ä¸­æ­¢ã€‚")

            selector = MediaSelector(self.config)
            media_ids = selector.get_item_ids(scope)
            
            total_items = len(media_ids)
            task_manager.update_task_progress(task_id, 0, total_items)
            ui_logger.info(f"âœ… [é˜¶æ®µ1/2] é¢„å¤„ç†å®Œæˆï¼Œå…±éœ€æ£€æŸ¥ {total_items} ä¸ªåª’ä½“é¡¹ã€‚", task_category=task_cat)
            
            # é˜¶æ®µäºŒï¼šæ‰§è¡Œæ¢å¤
            ui_logger.info("â¡ï¸ [é˜¶æ®µ2/2] å¼€å§‹é€ä¸€æ¢å¤åª’ä½“é¡¹...", task_category=task_cat)
            
            # æ‰¹é‡è·å–TMDB ID
            tmdb_id_map = {}
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_id = {executor.submit(self._get_tmdb_id, item_id): item_id for item_id in media_ids}
                for future in future_to_id:
                    item_id = future_to_id[future]
                    try:
                        tmdb_id = future.result()
                        if tmdb_id:
                            tmdb_id_map[item_id] = tmdb_id
                    except Exception as e:
                        logging.error(f"ã€æµ·æŠ¥æ¢å¤ã€‘è·å– TMDB ID æ—¶å‡ºé”™ (Emby ID: {item_id}): {e}")

            for i, item_id in enumerate(media_ids):
                if cancellation_event.is_set():
                    ui_logger.warning("âš ï¸ ä»»åŠ¡åœ¨æ‰§è¡Œé˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                    return

                tmdb_id = tmdb_id_map.get(item_id)
                if not tmdb_id:
                    ui_logger.debug(f"  -> è·³è¿‡ Emby ID: {item_id}ï¼Œå› ä¸ºå®ƒæ²¡æœ‰å…³è”çš„ TMDB IDã€‚", task_category=task_cat)
                else:
                    self._restore_single_item(item_id, tmdb_id, content_types, remote_map, task_cat)
                
                task_manager.update_task_progress(task_id, i + 1, total_items)

            ui_logger.info("ğŸ‰ æ¢å¤ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚", task_category=task_cat)

        except Exception as e:
            ui_logger.error(f"âŒ æ¢å¤ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", task_category=task_cat, exc_info=True)
            raise e
        

    def get_stats(self, force_refresh: bool = False) -> Dict:
        """è·å–çŠ¶æ€ä»ªè¡¨ç›˜æ‰€éœ€çš„æ•°æ®"""
        repo_count = len(self.pm_config.github_repos)
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šå°† force_refresh å‚æ•°ä¼ é€’ä¸‹å» ---
        remote_map = self._get_aggregated_remote_index("çŠ¶æ€è·å–", force_refresh=force_refresh)
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šå¦‚æœæ‰§è¡Œäº†å¼ºåˆ¶åˆ·æ–°ï¼Œåˆ™åŒæ­¥æ›´æ–° config.json ä¸­çš„å®¹é‡ ---
        if force_refresh:
            self._update_all_repo_sizes("çŠ¶æ€è·å–")

        total_images = len(remote_map)
        total_size_bytes = 0
        type_counts = {"poster": 0, "logo": 0, "fanart": 0}
        for key, value in remote_map.items():
            total_size_bytes += value.get("size", 0)
            if "poster" in key: type_counts["poster"] += 1
            elif "logo" in key: type_counts["logo"] += 1
            elif "fanart" in key: type_counts["fanart"] += 1

        repo_details = []
        total_capacity_bytes = 0
        threshold_bytes = self.pm_config.repository_size_threshold_mb * 1024 * 1024
        
        # é‡æ–°åŠ è½½ä¸€æ¬¡é…ç½®ï¼Œä»¥è·å–å¯èƒ½è¢« _update_all_repo_sizes æ›´æ–°è¿‡çš„æœ€æ–°çŠ¶æ€
        current_config = app_config_module.load_app_config()
        
        for repo in current_config.poster_manager_config.github_repos:
            match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo.repo_url)
            name = f"{match.group(1)}/{match.group(2)}" if match else repo.repo_url
            used_bytes = repo.state.size_bytes
            repo_details.append({
                "name": name,
                "used_bytes": used_bytes,
                "threshold_bytes": threshold_bytes,
                "last_checked": repo.state.last_checked
            })
            total_capacity_bytes += threshold_bytes

        return {
            "total_images": total_images,
            "total_size_bytes": total_size_bytes,
            "repo_count": repo_count,
            "total_capacity_bytes": total_capacity_bytes,
            "type_counts": type_counts,
            "repo_details": repo_details
        }

    # backend/poster_manager_logic.py (å‡½æ•°æ›¿æ¢ - ç»ˆæä¿®å¤)

    def _get_emby_image_details(self, item_id: str) -> Dict[str, Dict]:
        """
        ä¸€æ¬¡æ€§è·å– Emby åª’ä½“é¡¹æ‰€æœ‰ç±»å‹å›¾ç‰‡ï¼ˆæµ·æŠ¥ã€Logoã€èƒŒæ™¯å›¾ï¼‰çš„è¯¦ç»†ä¿¡æ¯ã€‚
        è¿”å›ä¸€ä¸ªä»¥å›¾ç‰‡ç±»å‹ä¸ºé”®çš„å­—å…¸ã€‚
        """
        import requests
        from urllib.parse import quote, urlencode
        
        task_cat = f"æµ·æŠ¥ç®¡ç†-è°ƒè¯•({item_id})"
        emby_images = {}
        api_key = self.config.server_config.api_key

        try:
            url = f"{self.config.server_config.server}/Items/{item_id}/Images"
            params = {"api_key": api_key}
            proxies = self.proxy_manager.get_proxies(url)
            
            ui_logger.debug(f"â¡ï¸ [è°ƒè¯•] æ­£åœ¨è¯·æ±‚å›¾ç‰‡å…ƒæ•°æ®: {url}", task_category=task_cat)
            response = self.session.get(url, params=params, timeout=15, proxies=proxies)
            response.raise_for_status()
            
            all_image_metadata = response.json()
            if not all_image_metadata:
                ui_logger.debug(f"   - [è°ƒè¯•] æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡å…ƒæ•°æ®ã€‚", task_category=task_cat)
                return {}

            ui_logger.debug(f"   - [è°ƒè¯•] æˆåŠŸè·å–åˆ° {len(all_image_metadata)} æ¡å›¾ç‰‡å…ƒæ•°æ®ã€‚", task_category=task_cat)

            for image_info in all_image_metadata:
                image_type_from_api = image_info.get("ImageType")
                
                type_map = {
                    "Primary": "poster",
                    "Logo": "logo",
                    "Backdrop": "fanart"
                }
                standard_type = type_map.get(image_type_from_api)
                
                if not standard_type:
                    continue

                image_tag = image_info.get("ImageTag")
                base_path = f"Items/{item_id}/Images/{image_type_from_api}"
                
                # --- æ ¸å¿ƒä¿®å¤ï¼šæ·»åŠ å¿…è¦çš„å›¾ç‰‡å¤„ç†å‚æ•°ä»¥è·å–äºŒè¿›åˆ¶æµ ---
                query_params = {
                    'api_key': api_key,
                    'quality': 100  # è¯·æ±‚æœ€é«˜è´¨é‡çš„å›¾ç‰‡
                }
                if image_tag:
                    query_params['tag'] = image_tag
                # --- ä¿®å¤ç»“æŸ ---
                
                image_path_to_emby = f"{base_path}?{urlencode(query_params)}"
                
                proxy_url = f"/api/emby-image-proxy?path={quote(image_path_to_emby)}"
                
                ui_logger.debug(f"   - [è°ƒè¯•] ä¸ºç±»å‹ '{standard_type}' ç”Ÿæˆçš„ä»£ç† URL: {proxy_url}", task_category=task_cat)

                width = image_info.get('Width', 0)
                height = image_info.get('Height', 0)
                size_bytes = image_info.get('Size', 0)
                
                resolution = f"{width}x{height}" if width and height else "æœªçŸ¥åˆ†è¾¨ç‡"
                size_str = f"{size_bytes / 1024 / 1024:.2f} MB" if size_bytes > 1024 * 1024 else f"{size_bytes / 1024:.1f} KB"

                emby_images[standard_type] = {
                    "url": proxy_url,
                    "resolution": resolution,
                    "size": size_str
                }
            
            return emby_images

        except requests.RequestException as e:
            ui_logger.error(f"âŒ [è°ƒè¯•] è·å– Emby å›¾ç‰‡å…ƒæ•°æ®åˆ—è¡¨å¤±è´¥: {e}", task_category=task_cat)
            return {}
        except Exception as e:
            ui_logger.error(f"âŒ [è°ƒè¯•] å¤„ç† Emby å›¾ç‰‡å…ƒæ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", task_category=task_cat, exc_info=True)
            return {}
    def get_single_item_details(self, item_id: str) -> Dict:
        """è·å–å•ä¸ªåª’ä½“é¡¹åœ¨ Emby å’Œ GitHub ä¸¤ä¾§çš„å›¾ç‰‡è¯¦æƒ…"""
        # 1. è·å– Emby ä¾§ä¿¡æ¯
        emby_details = self._get_emby_item_details(item_id, "Name,ProductionYear,ProviderIds")
        emby_images = self._get_emby_image_details(item_id)
        
        # 2. è·å– GitHub ä¾§ä¿¡æ¯
        github_images = {}
        tmdb_id = emby_details.get("ProviderIds", {}).get("Tmdb")
        if tmdb_id:
            remote_map = self._get_aggregated_remote_index(f"å•ä½“æŸ¥è¯¢({emby_details.get('Name')})")
            for img_type in ["poster", "logo", "fanart"]:
                key = f"{tmdb_id}-{img_type}"
                if key in remote_map:
                    gh_info = remote_map[key]
                    size_bytes = gh_info.get("size", 0)
                    github_images[img_type] = {
                        "url": gh_info.get("url"),
                        "resolution": "æœªçŸ¥åˆ†è¾¨ç‡",
                        "size": f"{size_bytes / 1024 / 1024:.2f} MB" if size_bytes > 1024 * 1024 else f"{size_bytes / 1024:.1f} KB"
                    }

        return {
            "emby": emby_images,
            "github": github_images
        }

    # backend/poster_manager_logic.py (å‡½æ•°æ›¿æ¢)

    def backup_single_image(self, item_id: str, image_type: str):
        """å•ä½“å¤‡ä»½ï¼šä»Embyä¸‹è½½å›¾ç‰‡ï¼Œå­˜å…¥æœ¬åœ°ç¼“å­˜ï¼Œå†ä¸Šä¼ åˆ°GitHub"""
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šæå‰è·å–è¯¦æƒ…ï¼Œæ„å»ºæ›´å‹å¥½çš„æ—¥å¿— ---
        item_details = self._get_emby_item_details(item_id, "Name,ProviderIds")
        item_name = item_details.get("Name", f"ID {item_id}")
        image_type_map = {"poster": "æµ·æŠ¥", "logo": "Logo", "fanart": "èƒŒæ™¯å›¾"}
        image_type_cn = image_type_map.get(image_type, image_type)
        
        task_cat = f"å•ä½“å¤‡ä»½-{item_name}"
        ui_logger.info(f"â¡ï¸ å¼€å§‹ä¸ºã€{item_name}ã€‘æ‰§è¡Œå•ä½“å¤‡ä»½ ({image_type_cn})...", task_category=task_cat)
        
        tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
        if not tmdb_id: raise ValueError("åª’ä½“é¡¹ç¼ºå°‘ TMDB IDã€‚")
        
        # 2. ä»Embyä¸‹è½½å›¾ç‰‡
        emby_key = {"poster": "Primary", "logo": "Logo", "fanart": "Backdrop"}.get(image_type)
        emby_image_url = f"{self.config.server_config.server}/Items/{item_id}/Images/{emby_key}"
        proxies = self.proxy_manager.get_proxies(emby_image_url)
        response = self.session.get(emby_image_url, params={"api_key": self.config.server_config.api_key}, timeout=60, proxies=proxies)
        response.raise_for_status()
        image_data = response.content

        # 3. å†™å…¥æœ¬åœ°ç¼“å­˜
        filename = {"poster": "poster.jpg", "logo": "clearlogo.png", "fanart": "fanart.jpg"}.get(image_type)
        local_dir = os.path.join(self.pm_config.local_cache_path, tmdb_id)
        os.makedirs(local_dir, exist_ok=True)
        local_path = os.path.join(local_dir, filename)
        with open(local_path, 'wb') as f:
            f.write(image_data)
        ui_logger.info(f"  - âœ… å›¾ç‰‡å·²æˆåŠŸä¸‹è½½å¹¶è¦†ç›–æœ¬åœ°ç¼“å­˜: {local_path}", task_category=task_cat)

        # 4. æ‰§è¡Œä¸Šä¼ ï¼ˆå¤ç”¨å¤‡ä»½æµç¨‹ï¼‰
        item_info = {
            "local_path": local_path,
            "tmdb_id": tmdb_id,
            "image_type": image_type,
            "size": len(image_data)
        }
        remote_map = self._get_aggregated_remote_index(task_cat)
        
        # --- æ ¸å¿ƒä¿®å¤ï¼šæ­£ç¡®ä¼ é€’æ‰€æœ‰å‚æ•°ï¼Œå¹¶å°† overwrite ç¡¬ç¼–ç ä¸º True ---
        overwrite_remote = True 
        new_files, overwrite_files = self._classify_pending_files([item_info], remote_map, overwrite_remote, task_cat)
        
        dispatch_plan = self._calculate_dispatch_plan(new_files, overwrite_files, task_cat)
        self._execute_dispatch_plan(dispatch_plan, task_cat, threading.Event())
        
        ui_logger.info(f"ğŸ‰ å•ä½“å¤‡ä»½ä»»åŠ¡å®Œæˆã€‚", task_category=task_cat)

    def delete_single_image(self, item_id: str, image_type: str):
        """å•ä½“åˆ é™¤ï¼šä»GitHubåˆ é™¤å›¾ç‰‡å’Œç´¢å¼•æ¡ç›®"""
        task_cat = f"å•ä½“åˆ é™¤({item_id}-{image_type})"
        ui_logger.info(f"â¡ï¸ å¼€å§‹æ‰§è¡Œå•ä½“åˆ é™¤...", task_category=task_cat)

        tmdb_id = self._get_tmdb_id(item_id)
        if not tmdb_id: raise ValueError("åª’ä½“é¡¹ç¼ºå°‘ TMDB IDã€‚")

        remote_map = self._get_aggregated_remote_index(task_cat)
        key = f"{tmdb_id}-{image_type}"
        image_info = remote_map.get(key)
        if not image_info: raise ValueError("åœ¨è¿œç¨‹å¤‡ä»½ä¸­æœªæ‰¾åˆ°è¯¥å›¾ç‰‡ã€‚")

        repo_url = image_info['repo_url']
        sha = image_info['sha']
        
        repo_config = next((r for r in self.pm_config.github_repos if r.repo_url == repo_url), None)
        if not repo_config: raise ValueError(f"é…ç½®ä¸­æ‰¾ä¸åˆ°ä»“åº“ {repo_url}ã€‚")

        pat = repo_config.personal_access_token or self.pm_config.global_personal_access_token
        branch = repo_config.branch
        match = re.match(r"httpshttps?://github\.com/([^/]+)/([^/]+)", repo_url)
        owner, repo_name = match.groups()
        
        # 1. åˆ é™¤æ–‡ä»¶
        filename = {"poster": "poster.jpg", "logo": "clearlogo.png", "fanart": "fanart.jpg"}.get(image_type)
        github_path = f"images/{tmdb_id}/{filename}"
        api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/{github_path}"
        delete_payload = {"message": f"refactor: Delete {github_path}", "sha": sha, "branch": branch}
        self._execute_github_write_request("DELETE", api_url, pat, delete_payload)
        ui_logger.info(f"  - âœ… å·²æˆåŠŸä» GitHub åˆ é™¤æ–‡ä»¶: {github_path}", task_category=task_cat)

        # 2. æ›´æ–°ç´¢å¼•
        index = self._get_repo_index(repo_config.model_dump())
        if str(tmdb_id) in index['images'] and image_type in index['images'][str(tmdb_id)]:
            del index['images'][str(tmdb_id)][image_type]
            if not index['images'][str(tmdb_id)]:
                del index['images'][str(tmdb_id)]
        
        index['last_updated'] = datetime.now().isoformat()
        index_api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/database.json"
        get_index_resp = self.session.get(index_api_url, headers={"Authorization": f"token {pat}"}, proxies=self.proxy_manager.get_proxies(index_api_url)).json()
        index_sha = get_index_resp.get('sha')
        index_payload = {
            "message": "chore: Update index after deletion",
            "content": base64.b64encode(json.dumps(index, indent=2).encode()).decode(),
            "branch": branch,
            "sha": index_sha
        }
        self._execute_github_write_request("PUT", index_api_url, pat, index_payload)
        ui_logger.info(f"  - âœ… ç´¢å¼•æ–‡ä»¶ database.json æ›´æ–°æˆåŠŸã€‚", task_category=task_cat)

        # 3. æ›´æ–°ä»“åº“å¤§å°
        latest_size_kb = self._get_latest_repo_size(repo_url, pat)
        current_config = app_config_module.load_app_config()
        for r in current_config.poster_manager_config.github_repos:
            if r.repo_url == repo_url:
                r.state.size_kb = latest_size_kb
                r.state.last_checked = datetime.now().isoformat()
                break
        app_config_module.save_app_config(current_config)
        ui_logger.info(f"  - âœ… ä»“åº“ {repo_url} æœ€æ–°å®¹é‡ {latest_size_kb} KB å·²å›å†™é…ç½®ã€‚", task_category=task_cat)
        
        # 4. æ¸…ç†èšåˆç¼“å­˜
        if os.path.exists(AGGREGATED_INDEX_CACHE_FILE):
            os.remove(AGGREGATED_INDEX_CACHE_FILE)
            ui_logger.info(f"  - âœ… æœ¬åœ°èšåˆç¼“å­˜å·²æ¸…ç†ã€‚", task_category=task_cat)

        ui_logger.info(f"ğŸ‰ å•ä½“åˆ é™¤ä»»åŠ¡å®Œæˆã€‚", task_category=task_cat)



    def start_backup_task(
        self,
        scope: ScheduledTasksTargetScope,
        content_types: List[str],
        overwrite: bool,
        cancellation_event: threading.Event,
        task_id: str,
        task_manager: TaskManager
    ):
        """ä»æœ¬åœ°ç¼“å­˜å¤‡ä»½åˆ° GitHub çš„ä¸»ä»»åŠ¡æµç¨‹"""
        task_cat = f"æµ·æŠ¥å¤‡ä»½({scope.mode})"
        ui_logger.info(f"ğŸ‰ ä»»åŠ¡å¯åŠ¨ï¼Œæ¨¡å¼: {scope.mode}, å†…å®¹: {content_types}, è¦†ç›–: {overwrite}", task_category=task_cat)

        try:
            # é˜¶æ®µä¸€ï¼šé¢„å¤„ç†
            selector = MediaSelector(self.config)
            media_ids = selector.get_item_ids(scope)
            task_manager.update_task_progress(task_id, 10, 100)
            if cancellation_event.is_set(): return

            initial_list = self._scan_local_cache(media_ids, content_types, task_cat)
            task_manager.update_task_progress(task_id, 25, 100)
            if cancellation_event.is_set(): return

            remote_map = self._get_aggregated_remote_index(task_cat, force_refresh=False)
            task_manager.update_task_progress(task_id, 40, 100)
            if cancellation_event.is_set(): return

            # é˜¶æ®µäºŒï¼šåˆ†ç±»
            new_files, overwrite_files = self._classify_pending_files(initial_list, remote_map, overwrite, task_cat)
            task_manager.update_task_progress(task_id, 50, 100)
            if not new_files and not overwrite_files:
                ui_logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å‡å·²æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œæ— éœ€å¤‡ä»½ã€‚", task_category=task_cat)
                task_manager.update_task_progress(task_id, 100, 100)
                return

            # é˜¶æ®µä¸‰ï¼šåˆ†å‘è®¡åˆ’
            dispatch_plan = self._calculate_dispatch_plan(new_files, overwrite_files, task_cat)
            task_manager.update_task_progress(task_id, 60, 100)
            if cancellation_event.is_set(): return

            # é˜¶æ®µå››ï¼šæ‰§è¡Œ
            self._execute_dispatch_plan(dispatch_plan, task_cat, cancellation_event)
            
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šä»»åŠ¡æ”¶å°¾é˜¶æ®µ ---
            if not cancellation_event.is_set():
                # é˜¶æ®µ 5.1: æ›´æ–°èšåˆç¼“å­˜
                self._get_aggregated_remote_index(task_cat, force_refresh=True)
                # é˜¶æ®µ 5.2: æ›´æ–°æ‰€æœ‰ä»“åº“å®¹é‡
                self._update_all_repo_sizes(task_cat)

            task_manager.update_task_progress(task_id, 100, 100)
            
            ui_logger.info("ğŸ‰ å¤‡ä»½ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚", task_category=task_cat)

        except Exception as e:
            ui_logger.error(f"âŒ å¤‡ä»½ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", task_category=task_cat, exc_info=True)
            raise e