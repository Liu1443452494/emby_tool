

import logging
import os
import json
import threading
import time
import re
import base64
import subprocess
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
from filelock import FileLock, Timeout
from datetime import datetime, timedelta

from log_manager import ui_logger
from models import AppConfig, PosterManagerConfig, ScheduledTasksTargetScope
from task_manager import TaskManager
from media_selector import MediaSelector
from proxy_manager import ProxyManager
import config as app_config_module


AGGREGATED_INDEX_CACHE_FILE = os.path.join('/app/data', 'poster_manager_aggregated_index.json')
AGGREGATED_INDEX_CACHE_DURATION = 3600  # ç¼“å­˜1å°æ—¶ (3600ç§’)

class PosterManagerLogic:
    def __init__(self, config: AppConfig):
        self.config = config
        self.pm_config = config.poster_manager_config
        self.proxy_manager = ProxyManager(config)
        self.session = self._create_session()

    def _create_session(self):
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        return session
    
    def _get_emby_item_details(self, item_id: str, fields: str) -> Dict:
        """ä» Emby è·å–åª’ä½“é¡¹çš„è¯¦ç»†ä¿¡æ¯"""
        import requests
        try:
            url = f"{self.config.server_config.server}/Users/{self.config.server_config.user_id}/Items/{item_id}"
            params = {"api_key": self.config.server_config.api_key, "Fields": fields}
            proxies = self.proxy_manager.get_proxies(url)
            response = self.session.get(url, params=params, timeout=15, proxies=proxies)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logging.error(f"ã€æµ·æŠ¥ç®¡ç†ã€‘è·å–åª’ä½“é¡¹ {item_id} çš„è¯¦æƒ…å¤±è´¥: {e}")
            raise e

    def _get_tmdb_id(self, item_id: str) -> Optional[str]:
        """ä» Emby è·å–åª’ä½“é¡¹çš„ TMDB ID"""
        try:
            url = f"{self.config.server_config.server}/Users/{self.config.server_config.user_id}/Items/{item_id}"
            params = {"api_key": self.config.server_config.api_key, "Fields": "ProviderIds"}
            proxies = self.proxy_manager.get_proxies(url)
            response = self.session.get(url, params=params, timeout=15, proxies=proxies)
            response.raise_for_status()
            provider_ids = response.json().get("ProviderIds", {})
            return provider_ids.get("Tmdb")
        except Exception as e:
            logging.error(f"ã€æµ·æŠ¥å¤‡ä»½ã€‘è·å–åª’ä½“é¡¹ {item_id} çš„ TMDB ID å¤±è´¥: {e}")
            return None

    def _scan_local_cache(self, media_ids: List[str], content_types: List[str], task_cat: str) -> List[Dict]:
        """æ‰«ææœ¬åœ°ç¼“å­˜ç›®å½•ï¼Œç”Ÿæˆåˆå§‹å¾…åŠåˆ—è¡¨"""
        ui_logger.info(f"â¡ï¸ [é˜¶æ®µ1.2] å¼€å§‹æ‰«ææœ¬åœ°ç¼“å­˜ç›®å½•: {self.pm_config.local_cache_path}", task_category=task_cat)
        
        if not self.pm_config.local_cache_path or not os.path.isdir(self.pm_config.local_cache_path):
            raise ValueError(f"æœ¬åœ°ç¼“å­˜è·¯å¾„ '{self.pm_config.local_cache_path}' æ— æ•ˆæˆ–æœªé…ç½®ã€‚")

        type_map = {
            "poster": "poster.jpg",
            "logo": "clearlogo.png",
            "fanart": "fanart.jpg"
        }
        
        initial_pending_list = []
        tmdb_id_map = {}


        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_id = {executor.submit(self._get_tmdb_id, item_id): item_id for item_id in media_ids}
            for future in future_to_id:
                item_id = future_to_id[future]
                try:
                    tmdb_id = future.result()
                    if tmdb_id:
                        tmdb_id_map[item_id] = tmdb_id
                except Exception as e:
                    logging.error(f"ã€æµ·æŠ¥å¤‡ä»½ã€‘è·å– TMDB ID æ—¶å‡ºé”™ (Emby ID: {item_id}): {e}")

        for item_id in media_ids:
            tmdb_id = tmdb_id_map.get(item_id)
            if not tmdb_id:
                ui_logger.warning(f"âš ï¸ è·³è¿‡ Emby ID: {item_id}ï¼Œå› ä¸ºå®ƒæ²¡æœ‰å…³è”çš„ TMDB IDã€‚", task_category=task_cat)
                continue

            item_cache_dir = os.path.join(self.pm_config.local_cache_path, tmdb_id)
            if not os.path.isdir(item_cache_dir):
                continue

            for content_type in content_types:
                filename = type_map.get(content_type)
                if not filename: continue
                
                filepath = os.path.join(item_cache_dir, filename)
                if os.path.isfile(filepath):
                    try:
                        file_size = os.path.getsize(filepath)
                        initial_pending_list.append({
                            "local_path": filepath,
                            "tmdb_id": tmdb_id,
                            "image_type": content_type,
                            "size": file_size
                        })
                    except OSError as e:
                        ui_logger.error(f"âŒ æ— æ³•è·å–æ–‡ä»¶å¤§å°: {filepath}ã€‚é”™è¯¯: {e}", task_category=task_cat)
        
        ui_logger.info(f"âœ… [é˜¶æ®µ1.2] æœ¬åœ°æ‰«æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(initial_pending_list)} ä¸ªå¾…å¤„ç†æ–‡ä»¶ã€‚", task_category=task_cat)
        return initial_pending_list

    def _get_repo_index(self, repo_config: Dict) -> Optional[Dict]:
        """è·å–å•ä¸ªä»“åº“çš„ database.json å†…å®¹"""
        repo_url = repo_config['repo_url']
        branch = repo_config.get('branch', 'main')
        pat = repo_config.get('personal_access_token') or self.pm_config.global_personal_access_token
        
        match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo_url)
        if not match:
            logging.error(f"ã€æµ·æŠ¥å¤‡ä»½ã€‘æ— æ•ˆçš„ GitHub ä»“åº“ URL: {repo_url}")
            return None
        owner, repo = match.groups()
        
        api_url = f"https://api.github.com/repos/{owner}/{repo}/contents/database.json?ref={branch}"
        headers = {"Accept": "application/vnd.github.v3+json"}
        if pat:
            headers["Authorization"] = f"token {pat}"
        
        try:
            proxies = self.proxy_manager.get_proxies(api_url)
            response = self.session.get(api_url, headers=headers, timeout=30, proxies=proxies)
            if response.status_code == 404:
                return {"version": 1, "last_updated": "", "images": {}} # ä»“åº“æ˜¯æ–°çš„ï¼Œè¿”å›ç©ºç´¢å¼•
            response.raise_for_status()
            
            content = base64.b64decode(response.json()['content']).decode('utf-8')
            return json.loads(content)
        except Exception as e:
            logging.error(f"ã€æµ·æŠ¥å¤‡ä»½ã€‘è·å–ä»“åº“ {repo_url} çš„ç´¢å¼•å¤±è´¥: {e}")
            return None

    def _get_aggregated_remote_index(self, task_cat: str, force_refresh: bool = False) -> Dict:
        """é€šè¿‡ç¼“å­˜æˆ–å¹¶å‘è·å–ï¼Œå¾—åˆ°èšåˆçš„æ‰€æœ‰è¿œç¨‹æ–‡ä»¶ç´¢å¼•"""
        ui_logger.info("â¡ï¸ [é˜¶æ®µ1.3] å¼€å§‹è·å–å¹¶èšåˆæ‰€æœ‰è¿œç¨‹ä»“åº“çš„ç´¢å¼•...", task_category=task_cat)
        
        lock_path = AGGREGATED_INDEX_CACHE_FILE + ".lock"
        

        should_write_cache = False

        if not force_refresh:
            try:
                with FileLock(lock_path, timeout=5):
                    if os.path.exists(AGGREGATED_INDEX_CACHE_FILE):
                        with open(AGGREGATED_INDEX_CACHE_FILE, 'r', encoding='utf-8') as f:
                            cache_data = json.load(f)
                        
                        cached_at = datetime.fromisoformat(cache_data.get("cached_at"))
                        now = datetime.now()
                        
                        if now - cached_at < timedelta(seconds=AGGREGATED_INDEX_CACHE_DURATION):
                            aggregated_index = cache_data.get("aggregated_index", {})
                            if aggregated_index:
                                age = now - cached_at
                                if age.total_seconds() < 60:
                                    age_str = f"{int(age.total_seconds())}ç§’å‰"
                                elif age.total_seconds() < 3600:
                                    age_str = f"{int(age.total_seconds() / 60)}åˆ†é’Ÿå‰"
                                else:
                                    age_str = f"{age.total_seconds() / 3600:.1f}å°æ—¶å‰"
                                
                                ui_logger.info(f"âœ… [é˜¶æ®µ1.3] å‘½ä¸­æœ¬åœ°èšåˆç´¢å¼•ç¼“å­˜ (æ›´æ–°äº {age_str})ã€‚", task_category=task_cat)
                                return aggregated_index
                            else:
                                ui_logger.warning("âš ï¸ æ£€æµ‹åˆ°æœ‰æ•ˆçš„ç©ºç¼“å­˜æ–‡ä»¶ï¼Œå¯èƒ½ç”±ä¹‹å‰çš„ç½‘ç»œé—®é¢˜å¯¼è‡´ï¼Œå°†å¼ºåˆ¶åˆ·æ–°ã€‚", task_category=task_cat)
                                should_write_cache = True # å‘ç°ç©ºç¼“å­˜ï¼Œæ ‡è®°éœ€è¦é‡å†™
                        else:
                            should_write_cache = True # ç¼“å­˜è¿‡æœŸï¼Œæ ‡è®°éœ€è¦é‡å†™
                    else:
                        should_write_cache = True # ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ ‡è®°éœ€è¦å†™å…¥
            except (Timeout, IOError, json.JSONDecodeError) as e:
                ui_logger.warning(f"âš ï¸ è¯»å–èšåˆç¼“å­˜å¤±è´¥ï¼Œå°†å¼ºåˆ¶ä»ç½‘ç»œè·å–ã€‚åŸå› : {e}", task_category=task_cat)
                should_write_cache = True # è¯»å–å¤±è´¥ï¼Œæ ‡è®°éœ€è¦é‡å†™
        else:
            ui_logger.info("   - [æ¨¡å¼] å·²å¯ç”¨å¼ºåˆ¶åˆ·æ–°ï¼Œå°†å¿½ç•¥æœ¬åœ°ç¼“å­˜ã€‚", task_category=task_cat)
            should_write_cache = True # å¼ºåˆ¶åˆ·æ–°ï¼Œæ ‡è®°éœ€è¦é‡å†™

        remote_file_map = {}
        repos = self.pm_config.github_repos
        total_repos = len(repos)
        successful_fetches = 0
        repos_with_data_count = 0
        
        ui_logger.info(f"   - [æ¨¡å¼] ç¼“å­˜æœªå‘½ä¸­æˆ–è¢«å¼ºåˆ¶åˆ·æ–°ï¼Œæ­£åœ¨ä»ç½‘ç»œå®æ—¶è·å–æ‰€æœ‰è¿œç¨‹ä»“åº“çš„ç´¢å¼•...", task_category=task_cat)
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_repo = {executor.submit(self._get_repo_index, repo.model_dump()): repo for repo in repos}
            for future in future_to_repo:
                repo_config = future_to_repo[future]
                try:
                    time.sleep(0.1)
                    index_content = future.result()
                    if index_content is not None:
                        successful_fetches += 1
                        if "images" in index_content and index_content["images"]:
                            repos_with_data_count += 1
                            for tmdb_id, images in index_content["images"].items():
                                for image_type, image_info in images.items():
                                    key = f"{tmdb_id}-{image_type}"
                                    remote_file_map[key] = image_info
                except Exception as e:
                    ui_logger.error(f"âŒ å¤„ç†ä»“åº“ {repo_config.repo_url} ç´¢å¼•æ—¶å‡ºé”™: {e}", task_category=task_cat)


        if should_write_cache:
            total_records_aggregated = len(remote_file_map)
            log_message_prefix = "âœ… [é˜¶æ®µ5]" if force_refresh else "âœ… [é˜¶æ®µ1.3]"
            
            if successful_fetches == total_repos:
                try:
                    with FileLock(lock_path, timeout=10):
                        cache_content = {
                            "cached_at": datetime.now().isoformat(),
                            "aggregated_index": remote_file_map
                        }
                        with open(AGGREGATED_INDEX_CACHE_FILE, 'w', encoding='utf-8') as f:
                            json.dump(cache_content, f)
                    
                    if total_records_aggregated == 0:
                        ui_logger.info(f"{log_message_prefix} æˆåŠŸæ£€æŸ¥æ‰€æœ‰({total_repos}/{total_repos})ä»“åº“ï¼Œæ‰€æœ‰ç´¢å¼•å‡ä¸ºç©ºã€‚å·²å†™å…¥ä¸€ä¸ªç©ºçš„èšåˆç¼“å­˜æ–‡ä»¶ã€‚", task_category=task_cat)
                    else:
                        ui_logger.info(f"{log_message_prefix} æˆåŠŸèšåˆæ‰€æœ‰({total_repos}/{total_repos})ä»“åº“çš„ç´¢å¼•ã€‚å…±èšåˆæ¥è‡ª {repos_with_data_count} ä¸ªä»“åº“çš„ {total_records_aggregated} æ¡è®°å½•ï¼Œå¹¶å·²å†™å…¥æœ¬åœ°ç¼“å­˜ã€‚", task_category=task_cat)

                except Exception as e:
                    ui_logger.error(f"âŒ {log_message_prefix} å†™å…¥èšåˆç´¢å¼•ç¼“å­˜å¤±è´¥: {e}", task_category=task_cat)
            else:
                ui_logger.warning(f"âš ï¸ {log_message_prefix} æœªèƒ½æˆåŠŸè·å–æ‰€æœ‰ä»“åº“çš„ç´¢å¼•({successful_fetches}/{total_repos})ï¼Œèšåˆç¼“å­˜æ›´æ–°å¤±è´¥ã€‚", task_category=task_cat)
        
        return remote_file_map

    def _classify_pending_files(self, initial_list: List[Dict], remote_map: Dict, overwrite: bool, task_cat: str) -> Tuple[List, List]:
        """å°†å¾…åŠåˆ—è¡¨åˆ†ä¸ºæ–°å¢å’Œè¦†ç›–ä¸¤ç±»"""
        ui_logger.info("â¡ï¸ [é˜¶æ®µ2] å¼€å§‹å¯¹å¾…åŠæ–‡ä»¶è¿›è¡Œåˆ†ç±» (æ–°å¢/è¦†ç›–)...", task_category=task_cat)
        new_files = []
        overwrite_files = []

        skipped_count = 0
        
        for item in initial_list:
            key = f"{item['tmdb_id']}-{item['image_type']}"
            remote_info = remote_map.get(key)
            
            if not remote_info:
                new_files.append(item)
            elif overwrite:
                item['remote_info'] = remote_info
                overwrite_files.append(item)
            else:

                skipped_count += 1
        

        ui_logger.info(f"âœ… [é˜¶æ®µ2] åˆ†ç±»å®Œæˆã€‚æ–°å¢: {len(new_files)} é¡¹, è¦†ç›–: {len(overwrite_files)} é¡¹, è·³è¿‡: {skipped_count} é¡¹ã€‚", task_category=task_cat)
        return new_files, overwrite_files



    def _calculate_dispatch_plan(self, new_files: List, overwrite_files: List, task_cat: str) -> Dict:
        """æ ¸å¿ƒç®—æ³•ï¼šè®¡ç®—æ–‡ä»¶åˆ†å‘è®¡åˆ’ (æ‰“åŒ…åˆ†é… + é™çº§ç­–ç•¥)"""
        ui_logger.info("â¡ï¸ [é˜¶æ®µ3] æ­£åœ¨è¿›è¡Œç²¾ç¡®é¢„è®¡ç®—ï¼Œç”Ÿæˆæ–‡ä»¶åˆ†å‘è®¡åˆ’...", task_category=task_cat)
        
        dispatch_plan = {repo.repo_url: {"new": [], "overwrite": []} for repo in self.pm_config.github_repos}
        
        threshold_bytes = self.pm_config.repository_size_threshold_mb * 1024 * 1024
        temp_repo_states = {
            repo.repo_url: threshold_bytes - repo.state.size_bytes
            for repo in self.pm_config.github_repos
        }



        grouped_overwrite_files = {}
        for item in overwrite_files:
            key = (item['tmdb_id'], item['remote_info']['repo_url'])
            if key not in grouped_overwrite_files:
                grouped_overwrite_files[key] = {"files": [], "total_delta": 0}
            
            new_size = item['size']
            old_size = item['remote_info']['size']
            delta = new_size - old_size
            
            grouped_overwrite_files[key]["files"].append(item)
            grouped_overwrite_files[key]["total_delta"] += delta


        for (tmdb_id, repo_url), group in grouped_overwrite_files.items():
            total_delta = group['total_delta']
            
            if temp_repo_states.get(repo_url, -1) < total_delta:
                file_names = ', '.join([os.path.basename(f['local_path']) for f in group['files']])
                raise ValueError(f"æ–‡ä»¶è¦†ç›–å¤±è´¥ï¼šè¦†ç›– TMDB ID {tmdb_id} çš„æ–‡ä»¶ ({file_names}) å°†å¯¼è‡´ä»“åº“ {repo_url} è¶…å‡ºå®¹é‡é™åˆ¶ã€‚")
            
            for item in group['files']:
                dispatch_plan[repo_url]["overwrite"].append(item)
            
            temp_repo_states[repo_url] -= total_delta
            
            file_names_str = ' '.join([os.path.basename(f['local_path']) for f in group['files']])
            ui_logger.info(f"  - [è®¡åˆ’-è¦†ç›–] [{tmdb_id}] -({file_names_str})-> åˆ†é…è‡³åŸä»“åº“ {repo_url} (ç©ºé—´å˜åŒ–: {total_delta/1024/1024:+.2f} MB)", task_category=task_cat)



        grouped_new_files = {}
        for item in new_files:
            tmdb_id = item['tmdb_id']
            if tmdb_id not in grouped_new_files:
                grouped_new_files[tmdb_id] = {"files": [], "total_size": 0}
            grouped_new_files[tmdb_id]["files"].append(item)
            grouped_new_files[tmdb_id]["total_size"] += item['size']


        for tmdb_id, group in grouped_new_files.items():
            allocated_as_group = False

            for repo in self.pm_config.github_repos:
                if temp_repo_states.get(repo.repo_url, -1) >= group['total_size']:
                    for item in group['files']:
                        dispatch_plan[repo.repo_url]["new"].append(item)
                    temp_repo_states[repo.repo_url] -= group['total_size']
                    ui_logger.info(f"  - [è®¡åˆ’-æ‰“åŒ…] [{tmdb_id}] å›¾ç‰‡ç»„ (å…± {len(group['files'])} é¡¹, {group['total_size']/1024/1024:.2f} MB) -> åˆ†é…è‡³ {repo.repo_url}", task_category=task_cat)
                    allocated_as_group = True
                    break
            

            if not allocated_as_group:
                ui_logger.warning(f"  - âš ï¸ [è®¡åˆ’-é™çº§] [{tmdb_id}] å›¾ç‰‡ç»„ (æ€»å¤§å° {group['total_size']/1024/1024:.2f} MB) æ— æ³•æ•´ä½“æ”¾å…¥ä»»ä½•ä»“åº“ï¼Œå°†å°è¯•å•ç‹¬åˆ†é…...", task_category=task_cat)
                for item in group['files']:
                    allocated_individually = False
                    for repo in self.pm_config.github_repos:
                        if temp_repo_states.get(repo.repo_url, -1) >= item['size']:
                            dispatch_plan[repo.repo_url]["new"].append(item)
                            temp_repo_states[repo.repo_url] -= item['size']
                            ui_logger.info(f"    - [è®¡åˆ’-é™çº§åˆ†é…] {os.path.basename(item['local_path'])} ({item['size']/1024/1024:.2f} MB) -> åˆ†é…è‡³ {repo.repo_url}", task_category=task_cat)
                            allocated_individually = True
                            break
                    if not allocated_individually:
                         raise ValueError(f"æ–‡ä»¶åˆ†é…å¤±è´¥ï¼šæ–‡ä»¶ {item['local_path']} ({item['size']/1024/1024:.2f} MB) è¿‡å¤§ï¼Œæ‰€æœ‰ä»“åº“å‡æ— è¶³å¤Ÿç©ºé—´å®¹çº³ã€‚")

        ui_logger.info("âœ… [é˜¶æ®µ3] æ–‡ä»¶åˆ†å‘è®¡åˆ’åˆ¶å®šæˆåŠŸã€‚", task_category=task_cat)
        return dispatch_plan

    def _execute_github_write_request(self, method: str, url: str, pat: str, payload: Optional[Dict] = None) -> Dict:
        """é€šè¿‡ curl æ‰§è¡Œ GitHub å†™å…¥æ“ä½œï¼ˆæ— é‡è¯•ï¼‰"""
        command = [
            'curl', '-L', '-X', method,
            '-H', 'Accept: application/vnd.github.v3+json',
            '-H', f'Authorization: token {pat}',
            '-H', 'Content-Type: application/json'
        ]
        
        json_payload_str = ""
        if payload:
            command.extend(['--data-binary', '@-'])
            json_payload_str = json.dumps(payload)

        proxies = self.proxy_manager.get_proxies(url)
        if proxies.get('https'):
            command.extend(['--proxy', proxies['https']])

        command.append(url)

        result = subprocess.run(command, input=json_payload_str, capture_output=True, text=True, check=False)
        
        response_data = {}
        try:
            if result.stdout:
                response_data = json.loads(result.stdout)
        except json.JSONDecodeError:

            raise Exception(f"cURL è¿”å›äº†éJSONå“åº”: {result.stdout or 'æ— è¾“å‡º'} | é”™è¯¯: {result.stderr or 'æ— é”™è¯¯ä¿¡æ¯'}")

        if result.returncode != 0 or (response_data.get("message") and response_data.get("documentation_url")):
            error_message = response_data.get('message', f"cURL é”™è¯¯: {result.stderr}")
            if response_data.get('status') == '422' and "sha" in error_message:
                error_message = f"æ— æ•ˆè¯·æ±‚ (422)ã€‚æœåŠ¡å™¨æç¤º 'sha' å‚æ•°æœ‰é—®é¢˜ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºåœ¨æ‚¨æ“ä½œæœŸé—´ï¼Œæ–‡ä»¶è¢«å…¶ä»–è¿›ç¨‹ä¿®æ”¹ã€‚è¯·é‡è¯•ã€‚({error_message})"
            elif "409 Conflict" in result.stderr:
                error_message = "GitHub API è¿”å› 409 Conflict é”™è¯¯ï¼Œè¿™é€šå¸¸æ˜¯å¹¶å‘å†™å…¥å†²çªå¯¼è‡´çš„ã€‚è¯·ç¨åé‡è¯•ã€‚"

            elif "schannel: failed to receive handshake" in result.stderr or "curl: (35)" in result.stderr:
                error_message = f"SSL/TLS æ¡æ‰‹å¤±è´¥ã€‚è¿™é€šå¸¸æ˜¯ä¸´æ—¶çš„ç½‘ç»œæˆ–ä»£ç†é—®é¢˜ã€‚é”™è¯¯: {result.stderr}"

            raise Exception(f"GitHub API é”™è¯¯: {error_message}")

        return response_data
    
    def _execute_github_write_request_with_retry(self, method: str, url: str, pat: str, payload: Optional[Dict] = None, task_cat: str = "GitHubå†™å…¥") -> Dict:
        """
        æ‰§è¡Œ GitHub å†™å…¥æ“ä½œï¼Œå¹¶å¢åŠ äº†é’ˆå¯¹ç½‘ç»œé”™è¯¯çš„é‡è¯•é€»è¾‘ã€‚
        """
        max_retries = 3
        retry_delay = 5  # seconds
        for attempt in range(max_retries):
            try:
                return self._execute_github_write_request(method, url, pat, payload)
            except Exception as e:

                error_str = str(e).lower()
                if "ssl/tls" in error_str or "handshake" in error_str or "curl: (35)" in error_str:
                    if attempt < max_retries - 1:
                        ui_logger.warning(f"  - âš ï¸ ç½‘ç»œæ“ä½œå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries})ï¼Œå°†åœ¨ {retry_delay} ç§’åé‡è¯•... åŸå› : {e}", task_category=task_cat)
                        time.sleep(retry_delay)
                        continue

                raise e

        raise Exception("é‡è¯•é€»è¾‘æ‰§è¡Œå®Œæ¯•ä½†æœªèƒ½æˆåŠŸã€‚")


    def _get_latest_repo_size(self, repo_url: str, pat: str) -> int:
        """è·å–ä»“åº“çš„æœ€æ–°å¤§å° (KB)"""
        match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo_url)
        owner, repo = match.groups()
        api_url = f"https://api.github.com/repos/{owner}/{repo}"
        headers = {"Accept": "application/vnd.github.v3+json"}
        if pat:
            headers["Authorization"] = f"token {pat}"
        
        proxies = self.proxy_manager.get_proxies(api_url)
        response = self.session.get(api_url, headers=headers, timeout=30, proxies=proxies)
        response.raise_for_status()
        return response.json().get('size', 0)



    def _execute_dispatch_plan(self, dispatch_plan: Dict, task_cat: str, cancellation_event: threading.Event):
        """æ‰§è¡Œæ–‡ä»¶ä¸Šä¼ å’Œç´¢å¼•æ›´æ–°"""
        ui_logger.info("â¡ï¸ [é˜¶æ®µ4] å¼€å§‹æ‰§è¡Œæ–‡ä»¶ä¸Šä¼ å’Œç´¢å¼•æ›´æ–°...", task_category=task_cat)
        

        for repo_config in self.pm_config.github_repos:
            repo_url = repo_config.repo_url
            plan = dispatch_plan.get(repo_url)
            if not plan or (not plan['new'] and not plan['overwrite']):
                continue

            if cancellation_event.is_set():
                ui_logger.warning("âš ï¸ ä»»åŠ¡åœ¨æ‰§è¡Œé˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                return

            ui_logger.info(f"  - æ­£åœ¨å¤„ç†ä»“åº“: {repo_url}", task_category=task_cat)
            pat = repo_config.personal_access_token or self.pm_config.global_personal_access_token
            branch = repo_config.branch
            match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo_url)
            owner, repo_name = match.groups()


            try:

                lock_path = ".lock"
                lock_api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/{lock_path}"
                lock_payload = {
                    "message": f"feat: Acquire lock for task",
                    "content": base64.b64encode(f"locked_at: {datetime.now().isoformat()}".encode()).decode(),
                    "branch": branch
                }
                self._execute_github_write_request_with_retry("PUT", lock_api_url, pat, lock_payload, task_cat=task_cat)
                ui_logger.info(f"    - ğŸ”’ å·²æˆåŠŸåœ¨ä»“åº“ {repo_url} ä¸­åˆ›å»º .lock æ–‡ä»¶ã€‚", task_category=task_cat)
            
            except Exception as e:
                if "422" in str(e) or "Unprocessable Entity" in str(e):
                    error_message = (
                        f"âŒ æ— æ³•é”å®šä»“åº“ {repo_url}ï¼Œä»»åŠ¡ä¸­æ­¢ï¼\n"
                        f"    - **å¯èƒ½åŸå› **: ä¸Šä¸€æ¬¡å¤‡ä»½ä»»åŠ¡å¼‚å¸¸ä¸­æ–­ï¼Œå¯¼è‡´ .lock æ–‡ä»¶æœªèƒ½è¢«è‡ªåŠ¨åˆ é™¤ã€‚\n"
                        f"    - **ä¿®å¤å»ºè®®**: è¯·æ‰‹åŠ¨å‰å¾€è¯¥ GitHub ä»“åº“ï¼Œæ£€æŸ¥å¹¶åˆ é™¤æ ¹ç›®å½•ä¸‹çš„ `.lock` æ–‡ä»¶åï¼Œå†é‡æ–°è¿è¡Œå¤‡ä»½ä»»åŠ¡ã€‚\n"
                        f"    - **è¡¥å……è¯´æ˜**: å¦‚æœæ‚¨ç¡®è®¤æ²¡æœ‰å…¶ä»–ä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œåˆ é™¤ .lock æ–‡ä»¶æ˜¯å®‰å…¨çš„æ“ä½œã€‚é‡æ–°è¿è¡Œä¸€æ¬¡å®Œæ•´çš„å¤‡ä»½ä»»åŠ¡å¯ä»¥ä¿®å¤ä»»ä½•æ½œåœ¨çš„ç´¢å¼•ä¸ä¸€è‡´é—®é¢˜ã€‚"
                    )
                    ui_logger.error(error_message, task_category=task_cat)

                    raise Exception(f"è·å–ä»“åº“ {repo_url} çš„é”å¤±è´¥ã€‚")
                else:

                    raise e


            try:

                current_index = self._get_repo_index(repo_config.model_dump())
                if current_index is None:
                    raise Exception("è·å–æœ€æ–°ç´¢å¼•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ã€‚")


                files_to_process = plan['overwrite'] + plan['new']
                for item in files_to_process:
                    if cancellation_event.is_set(): raise InterruptedError("ä»»åŠ¡è¢«å–æ¶ˆ")
                    
                    action_type = 'æ–°å¢'
                    
                    cooldown = self.pm_config.file_upload_cooldown_seconds
                    if cooldown > 0:
                        ui_logger.info(f"    - â±ï¸ æ–‡ä»¶ä¸Šä¼ å†·å´ {cooldown} ç§’...", task_category=task_cat)
                        time.sleep(cooldown)
                    
                    with open(item['local_path'], 'rb') as f:
                        content_b64 = base64.b64encode(f.read()).decode()
                    
                    github_path = f"images/{item['tmdb_id']}/{os.path.basename(item['local_path'])}"
                    api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/{github_path}"
                    
                    payload = {
                        "message": f"feat: Add/Update {item['image_type']} for {item['tmdb_id']}",
                        "content": content_b64,
                        "branch": branch
                    }

                    is_overwrite = 'remote_info' in item
                    if is_overwrite:
                        payload['sha'] = item['remote_info']['sha']
                        action_type = 'è¦†ç›–'

                    try:
                        response_data = self._execute_github_write_request_with_retry("PUT", api_url, pat, payload, task_cat=task_cat)
                    except Exception as e:
                        if "422" in str(e) and not is_overwrite:
                            ui_logger.info(f"    - ğŸ”„ æ–‡ä»¶ {github_path} å·²å­˜åœ¨ï¼Œè§¦å‘â€œæ–°å¢è½¬è¦†ç›–â€å®¹é”™æœºåˆ¶...", task_category=task_cat)
                            action_type = 'è¦†ç›–'
                            get_resp = self.session.get(api_url, headers={"Authorization": f"token {pat}"}, proxies=self.proxy_manager.get_proxies(api_url)).json()
                            payload['sha'] = get_resp['sha']
                            response_data = self._execute_github_write_request_with_retry("PUT", api_url, pat, payload, task_cat=task_cat)
                        else:
                            raise e
                    
                    if action_type == 'è¦†ç›–':
                        ui_logger.info(f"    - âœ… è¦†ç›–ä¸Šä¼ æˆåŠŸ: {github_path}", task_category=task_cat)
                    else:
                        ui_logger.info(f"    - â¬†ï¸ æ–°å¢ä¸Šä¼ æˆåŠŸ: {github_path}", task_category=task_cat)


                    tmdb_id_str = str(item['tmdb_id'])
                    if tmdb_id_str not in current_index['images']:
                        current_index['images'][tmdb_id_str] = {}
                    
                    current_index['images'][tmdb_id_str][item['image_type']] = {
                        "repo_url": repo_url,
                        "sha": response_data['content']['sha'],
                        "size": response_data['content']['size'],
                        "url": response_data['content']['download_url']
                    }


                current_index['last_updated'] = datetime.now().isoformat()
                index_api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/database.json"
                
                get_index_resp = self.session.get(index_api_url, headers={"Authorization": f"token {pat}"}, proxies=self.proxy_manager.get_proxies(index_api_url)).json()
                index_sha = get_index_resp.get('sha')

                index_payload = {
                    "message": f"chore: Update database index",
                    "content": base64.b64encode(json.dumps(current_index, indent=2).encode()).decode(),
                    "branch": branch
                }
                if index_sha:
                    index_payload['sha'] = index_sha
                
                self._execute_github_write_request_with_retry("PUT", index_api_url, pat, index_payload, task_cat=task_cat)
                ui_logger.info(f"    - ç´¢å¼•æ–‡ä»¶ database.json æ›´æ–°æˆåŠŸã€‚", task_category=task_cat)

            finally:

                lock_get_resp = self.session.get(lock_api_url, headers={"Authorization": f"token {pat}"}, proxies=self.proxy_manager.get_proxies(lock_api_url)).json()
                lock_sha = lock_get_resp.get('sha')
                if lock_sha:
                    delete_payload = {
                        "message": "feat: Release lock",
                        "sha": lock_sha,
                        "branch": branch
                    }
                    self._execute_github_write_request_with_retry("DELETE", lock_api_url, pat, delete_payload, task_cat=task_cat)
                    ui_logger.info(f"    - ğŸ”“ å·²æˆåŠŸä»ä»“åº“ {repo_url} ä¸­ç§»é™¤ .lock æ–‡ä»¶ã€‚", task_category=task_cat)

        ui_logger.info("âœ… [é˜¶æ®µ4] æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å’Œç´¢å¼•æ›´æ–°å®Œæˆã€‚", task_category=task_cat)

    def _update_all_repo_sizes(self, task_cat: str):
        """
        åœ¨ä»»åŠ¡ç»“æŸåï¼Œæ ¹æ®æœ€æ–°çš„èšåˆç´¢å¼•ï¼Œç»Ÿä¸€è®¡ç®—å¹¶å›å†™æ‰€æœ‰ä»“åº“çš„å®¹é‡çŠ¶æ€ã€‚
        """
        ui_logger.info(f"â¡ï¸ [é˜¶æ®µ5] æ­£åœ¨æ ¹æ®æœ€æ–°ç´¢å¼•ç»Ÿä¸€æ›´æ–°æ‰€æœ‰ä»“åº“çš„å®¹é‡çŠ¶æ€...", task_category=task_cat)
        
        if not os.path.exists(AGGREGATED_INDEX_CACHE_FILE):
            ui_logger.warning("âš ï¸ æœªæ‰¾åˆ°èšåˆç´¢å¼•ç¼“å­˜æ–‡ä»¶ï¼Œè·³è¿‡å®¹é‡æ›´æ–°ã€‚", task_category=task_cat)
            return
        
        with open(AGGREGATED_INDEX_CACHE_FILE, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        aggregated_index = cache_data.get("aggregated_index", {})

        repo_sizes = {}
        for key, image_info in aggregated_index.items():
            repo_url = image_info.get("repo_url")
            size = image_info.get("size", 0)
            if repo_url:
                repo_sizes[repo_url] = repo_sizes.get(repo_url, 0) + size
        
        current_app_config = app_config_module.load_app_config()
        updated_repos = []
        for repo in current_app_config.poster_manager_config.github_repos:
            new_size_bytes = repo_sizes.get(repo.repo_url, 0)
            if repo.state.size_bytes != new_size_bytes:
                repo.state.size_bytes = new_size_bytes
                repo.state.last_checked = datetime.now().isoformat()
                updated_repos.append(repo)
        
        if updated_repos:
            app_config_module.save_app_config(current_app_config)
            self.pm_config = current_app_config.poster_manager_config
            for repo in updated_repos:
                match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo.repo_url)
                name = f"{match.group(1)}/{match.group(2)}" if match else repo.repo_url
                

                size_in_gb = repo.state.size_bytes / (1024 * 1024 * 1024)
                if size_in_gb >= 1:
                    ui_logger.info(f"   - [å®¹é‡æ›´æ–°] ä»“åº“ {name} æœ€æ–°å®¹é‡ä¸º {size_in_gb:.2f} GBã€‚", task_category=task_cat)
                else:
                    size_in_mb = repo.state.size_bytes / (1024 * 1024)
                    ui_logger.info(f"   - [å®¹é‡æ›´æ–°] ä»“åº“ {name} æœ€æ–°å®¹é‡ä¸º {size_in_mb:.2f} MBã€‚", task_category=task_cat)

        else:
            ui_logger.info("   - [å®¹é‡æ›´æ–°] æ‰€æœ‰ä»“åº“å®¹é‡å‡æ— å˜åŒ–ã€‚", task_category=task_cat)

   
    # backend/poster_manager_logic.py (å‡½æ•°æ›¿æ¢)

    # backend/poster_manager_logic.py (å‡½æ•°æ›¿æ¢)

    def start_restore_from_remote_task(
        self,
        scope: ScheduledTasksTargetScope,
        content_types: List[str],
        cancellation_event: threading.Event,
        task_id: str,
        task_manager: TaskManager
    ):
        """ä» GitHub å¤‡ä»½åå‘æ¢å¤å›¾ç‰‡åˆ° Emby çš„ä¸»ä»»åŠ¡æµç¨‹ (å·²ä¿®å¤èŒƒå›´è¿‡æ»¤)"""
        from concurrent.futures import as_completed
        
        overwrite = self.pm_config.overwrite_on_restore
        task_cat = "æµ·æŠ¥æ¢å¤(åå‘)"
        overwrite_text = "å¼ºåˆ¶è¦†ç›–" if overwrite else "æ™ºèƒ½è·³è¿‡"
        ui_logger.info(f"ğŸ‰ ä»»åŠ¡å¯åŠ¨ï¼Œæ¨¡å¼: ä»è¿œç¨‹å¤‡ä»½æ¢å¤, èŒƒå›´: {scope.mode}, å†…å®¹: {content_types}, ç­–ç•¥: {overwrite_text}", task_category=task_cat)

        try:
            # é˜¶æ®µä¸€ï¼šè·å–è¿œç¨‹æ•°æ®å’ŒIDæ˜ å°„
            ui_logger.info("â¡ï¸ [é˜¶æ®µ1/5] æ­£åœ¨è·å–è¿œç¨‹ç´¢å¼•å’ŒIDæ˜ å°„...", task_category=task_cat)
            remote_map = self._get_aggregated_remote_index(task_cat)
            if not remote_map:
                raise ValueError("æ— æ³•è·å–è¿œç¨‹èšåˆç´¢å¼•ï¼Œä»»åŠ¡ä¸­æ­¢ã€‚")

            id_map_file = os.path.join('/app/data', 'id_map.json')
            if not os.path.exists(id_map_file):
                raise ValueError("IDæ˜ å°„è¡¨ (id_map.json) ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œåå‘æ¢å¤ã€‚è¯·å…ˆåœ¨â€œå®šæ—¶ä»»åŠ¡â€é¡µé¢ç”Ÿæˆæ˜ å°„è¡¨ã€‚")
            with open(id_map_file, 'r', encoding='utf-8') as f:
                id_map = json.load(f)

            # é˜¶æ®µäºŒï¼šæ ¹æ®è¿œç¨‹å¤‡ä»½ï¼Œæ„å»ºåˆå§‹çš„ Emby åª’ä½“æ£€æŸ¥åˆ—è¡¨
            ui_logger.info("â¡ï¸ [é˜¶æ®µ2/5] æ­£åœ¨æ ¹æ®è¿œç¨‹å¤‡ä»½æ„å»ºåˆå§‹æ£€æŸ¥åˆ—è¡¨...", task_category=task_cat)
            target_tmdb_ids = {
                key.split('-')[0] for key, value in remote_map.items()
                if any(f'-{ct}' in key for ct in content_types)
            }
            
            initial_emby_ids_to_check = set()
            for tmdb_id in target_tmdb_ids:
                if tmdb_id in id_map:
                    initial_emby_ids_to_check.update(id_map[tmdb_id])
            
            if not initial_emby_ids_to_check:
                ui_logger.info("âœ… è¿œç¨‹å¤‡ä»½ä¸­çš„æ‰€æœ‰åª’ä½“åœ¨æ‚¨çš„ Emby åº“ä¸­å‡æœªæ‰¾åˆ°ï¼Œä»»åŠ¡å®Œæˆã€‚", task_category=task_cat)
                return

            ui_logger.info(f"   - è¿œç¨‹æ•°æ®åº“åŒ…å« {len(target_tmdb_ids)} ä¸ªæœ‰å¤‡ä»½çš„TMDB IDï¼Œå¯¹åº”åˆ°æœ¬åœ° Emby åº“ä¸­çš„ {len(initial_emby_ids_to_check)} ä¸ªåª’ä½“å®ä¾‹ã€‚", task_category=task_cat)

            # é˜¶æ®µä¸‰ï¼šæ ¹æ®ç”¨æˆ·æŒ‡å®šçš„èŒƒå›´ï¼Œè¿‡æ»¤å‡ºæœ€ç»ˆè¦å¤„ç†çš„åª’ä½“
            ui_logger.info("â¡ï¸ [é˜¶æ®µ3/5] æ­£åœ¨æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„èŒƒå›´è¿›è¡Œè¿‡æ»¤...", task_category=task_cat)
            selector = MediaSelector(self.config)
            scoped_emby_ids = set(selector.get_item_ids(scope))
            
            final_item_ids_to_process = initial_emby_ids_to_check.intersection(scoped_emby_ids)

            if not final_item_ids_to_process:
                ui_logger.info("âœ… åœ¨æŒ‡å®šèŒƒå›´å†…ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸è¿œç¨‹å¤‡ä»½åŒ¹é…çš„åª’ä½“ï¼Œä»»åŠ¡å®Œæˆã€‚", task_category=task_cat)
                return
            
            ui_logger.info(f"   - è¿‡æ»¤åï¼Œæœ€ç»ˆç¡®å®šéœ€è¦å¤„ç† {len(final_item_ids_to_process)} ä¸ªåª’ä½“å®ä¾‹ã€‚", task_category=task_cat)

            # é˜¶æ®µå››ï¼šæ„å»ºæ¢å¤è®¡åˆ’
            ui_logger.info(f"â¡ï¸ [é˜¶æ®µ4/5] æ­£åœ¨æ£€æŸ¥ç›®æ ‡åª’ä½“çŠ¶æ€å¹¶æ„å»ºæ¢å¤è®¡åˆ’...", task_category=task_cat)
            restore_plan = []
            total_items_to_check = len(final_item_ids_to_process)
            task_manager.update_task_progress(task_id, 0, total_items_to_check)

            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_id = {executor.submit(self._get_emby_item_details, item_id, "Name,ImageTags,ProviderIds"): item_id for item_id in final_item_ids_to_process}
                for i, future in enumerate(as_completed(future_to_id)):
                    if cancellation_event.is_set():
                        ui_logger.warning("âš ï¸ ä»»åŠ¡åœ¨æ„å»ºè®¡åˆ’é˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                        return
                    
                    item_id = future_to_id[future]
                    try:
                        details = future.result()
                        item_name = details.get("Name", f"ID {item_id}")
                        image_tags = details.get("ImageTags", {})
                        tmdb_id = details.get("ProviderIds", {}).get("Tmdb")

                        if not tmdb_id:
                            ui_logger.debug(f"   - [è·³è¿‡] åª’ä½“ã€{item_name}ã€‘(Emby Item ID: {item_id}) ç¼ºå°‘ TMDB IDã€‚", task_category=task_cat)
                            continue

                        for image_type in content_types:
                            remote_key = f"{tmdb_id}-{image_type}"
                            if remote_key not in remote_map:
                                continue

                            needs_restore = False
                            if overwrite:
                                needs_restore = True
                            else:
                                type_map = {"poster": "Primary", "logo": "Logo", "fanart": "Backdrop"}
                                if not image_tags.get(type_map.get(image_type)):
                                    needs_restore = True
                                else:
                                    ui_logger.info(f"   - [è·³è¿‡] åª’ä½“ã€{item_name}ã€‘(Emby Item ID: {item_id}) å·²å­˜åœ¨ {image_type} å›¾ç‰‡ã€‚", task_category=task_cat)
                            
                            if needs_restore:
                                restore_plan.append({"item_id": item_id, "item_name": item_name, "image_type": image_type, "tmdb_id": tmdb_id})
                    except Exception as e:
                        ui_logger.error(f"   - âŒ è·å–åª’ä½“ (Emby Item ID: {item_id}) è¯¦æƒ…å¤±è´¥: {e}", task_category=task_cat)
                    finally:
                        task_manager.update_task_progress(task_id, i + 1, total_items_to_check)

            ui_logger.info(f"âœ… æ¢å¤è®¡åˆ’æ„å»ºå®Œæˆï¼Œå…±éœ€æ¢å¤ {len(restore_plan)} å¼ å›¾ç‰‡ã€‚", task_category=task_cat)

            # é˜¶æ®µäº”ï¼šæ‰§è¡Œæ¢å¤
            ui_logger.info("â¡ï¸ [é˜¶æ®µ5/5] å¼€å§‹é€ä¸€æ‰§è¡Œæ¢å¤...", task_category=task_cat)
            total_to_restore = len(restore_plan)
            task_manager.update_task_progress(task_id, 0, total_to_restore)

            for i, plan_item in enumerate(restore_plan):
                if cancellation_event.is_set():
                    ui_logger.warning("âš ï¸ ä»»åŠ¡åœ¨æ‰§è¡Œé˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                    return
                
                item_id = plan_item["item_id"]
                item_name = plan_item["item_name"]
                image_type = plan_item["image_type"]
                tmdb_id = plan_item["tmdb_id"]
                
                ui_logger.info(f"  -> æ­£åœ¨ä¸ºã€{item_name}ã€‘(Emby Item ID: {item_id}) æ¢å¤ {image_type}...", task_category=task_cat)
                try:
                    self._restore_single_image_from_plan(item_id, image_type, tmdb_id, remote_map, task_cat)
                    ui_logger.info(f"     - âœ… æˆåŠŸæ¢å¤ã€{item_name}ã€‘(Emby Item ID: {item_id}) çš„ {image_type} å›¾ç‰‡ã€‚", task_category=task_cat)
                except Exception as e:
                    ui_logger.error(f"     - âŒ æ¢å¤ã€{item_name}ã€‘(Emby Item ID: {item_id}) çš„ {image_type} å›¾ç‰‡å¤±è´¥: {e}", task_category=task_cat)

                task_manager.update_task_progress(task_id, i + 1, total_to_restore)

            ui_logger.info("ğŸ‰ æ¢å¤ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚", task_category=task_cat)

        except Exception as e:
            ui_logger.error(f"âŒ æ¢å¤ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", task_category=task_cat, exc_info=True)
            raise e


    # backend/poster_manager_logic.py (å‡½æ•°æ›¿æ¢)

    def _restore_single_image_from_plan(self, item_id: str, image_type: str, tmdb_id: str, remote_map: Dict, task_cat: str):
        """æ ¹æ®è®¡åˆ’ï¼Œæ¢å¤å•å¼ æŒ‡å®šç±»å‹çš„å›¾ç‰‡"""
        import requests
        
        key = f"{tmdb_id}-{image_type}"
        image_info = remote_map.get(key)

        if not image_info:
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šä¿®æ­£æ—¥å¿—æ‰“å°ä¸­çš„å˜é‡ä½ç½® ---
            ui_logger.debug(f"     - è·³è¿‡: åœ¨è¿œç¨‹å¤‡ä»½ä¸­æœªæ‰¾åˆ° TMDB ID {tmdb_id} çš„ {image_type} å›¾ç‰‡ã€‚", task_category=task_cat)
            return

        image_url = image_info.get("url")
        if not image_url:
            ui_logger.warning(f"     - âš ï¸ è­¦å‘Š: è¿œç¨‹å¤‡ä»½ä¸­ TMDB ID {tmdb_id} çš„ {image_type} å›¾ç‰‡è®°å½•ç¼ºå°‘ä¸‹è½½URLã€‚", task_category=task_cat)
            return

        try:
            cooldown = self.pm_config.image_download_cooldown_seconds
            if cooldown > 0:
                ui_logger.info(f"     - â±ï¸ ä¸‹è½½å†·å´ {cooldown} ç§’...", task_category=task_cat)
                time.sleep(cooldown)
            
            ui_logger.debug(f"     - æ­£åœ¨ä¸‹è½½: {image_url}", task_category=task_cat)
            proxies = self.proxy_manager.get_proxies(image_url)
            image_response = self.session.get(image_url, timeout=60, proxies=proxies)
            image_response.raise_for_status()
            image_data = image_response.content
            
            emby_image_type_map = {
                "poster": "Primary",
                "logo": "Logo",
                "fanart": "Backdrop"
            }
            emby_image_type = emby_image_type_map.get(image_type)
            if not emby_image_type: return

            upload_url = f"{self.config.server_config.server}/Items/{item_id}/Images/{emby_image_type}"
            
            try:
                delete_proxies = self.proxy_manager.get_proxies(upload_url)
                self.session.delete(upload_url, params={"api_key": self.config.server_config.api_key}, timeout=20, proxies=delete_proxies)
            except requests.RequestException:
                pass

            base64_encoded_data = base64.b64encode(image_data).decode('utf-8')
            headers = {'Content-Type': image_response.headers.get('Content-Type', 'image/jpeg')}
            upload_proxies = self.proxy_manager.get_proxies(upload_url)
            
            upload_response = self.session.post(
                upload_url, 
                params={"api_key": self.config.server_config.api_key}, 
                data=base64_encoded_data,
                headers=headers, 
                timeout=60,
                proxies=upload_proxies
            )
            upload_response.raise_for_status()
            

        except Exception as e:

            raise e


    def start_restore_task(
        self,
        scope: ScheduledTasksTargetScope,
        content_types: List[str],
        cancellation_event: threading.Event,
        task_id: str,
        task_manager: TaskManager
    ):
        """
        æ¢å¤ä»»åŠ¡çš„å…¥å£å‡½æ•°ï¼Œæ ¹æ®é…ç½®åˆ†å‘åˆ°ä¸åŒçš„æ‰§è¡Œæµç¨‹ã€‚
        """
        if self.pm_config.restore_mode == 'from_remote':
            from concurrent.futures import as_completed
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šè¡¥ä¸Šç¼ºå¤±çš„ task_manager å‚æ•° ---
            self.start_restore_from_remote_task(scope, content_types, cancellation_event, task_id, task_manager)
            return
        # --- ä¿®æ”¹ç»“æŸ ---

        """ä» GitHub æ¢å¤å›¾ç‰‡åˆ° Emby çš„ä¸»ä»»åŠ¡æµç¨‹ (æ ‡å‡†æ¨¡å¼)"""
        overwrite = self.pm_config.overwrite_on_restore
        task_cat = f"æµ·æŠ¥æ¢å¤({scope.mode})"
        overwrite_text = "å¼ºåˆ¶è¦†ç›–" if overwrite else "æ™ºèƒ½è·³è¿‡"
        ui_logger.info(f"ğŸ‰ ä»»åŠ¡å¯åŠ¨ï¼Œæ¨¡å¼: æ ‡å‡†, èŒƒå›´: {scope.mode}, å†…å®¹: {content_types}, ç­–ç•¥: {overwrite_text}", task_category=task_cat)

        try:
            # é˜¶æ®µä¸€ï¼šé¢„å¤„ç†
            ui_logger.info("â¡ï¸ [é˜¶æ®µ1/3] æ­£åœ¨è·å–è¿œç¨‹ç´¢å¼•å’Œåª’ä½“åˆ—è¡¨...", task_category=task_cat)
            remote_map = self._get_aggregated_remote_index(task_cat)
            if not remote_map:
                raise ValueError("æ— æ³•è·å–è¿œç¨‹èšåˆç´¢å¼•ï¼Œä»»åŠ¡ä¸­æ­¢ã€‚")

            selector = MediaSelector(self.config)
            media_ids = selector.get_item_ids(scope)
            
            # é˜¶æ®µäºŒï¼šæ„å»ºæ¢å¤è®¡åˆ’
            ui_logger.info(f"â¡ï¸ [é˜¶æ®µ2/3] æ­£åœ¨æ„å»ºæ¢å¤è®¡åˆ’...", task_category=task_cat)
            restore_plan = []
            skipped_for_no_backup = 0
            total_items_to_check = len(media_ids)
            task_manager.update_task_progress(task_id, 0, total_items_to_check)

            item_details_map = {}
            from concurrent.futures import as_completed
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_id = {executor.submit(self._get_emby_item_details, item_id, "Name,ImageTags,ProviderIds"): item_id for item_id in media_ids}
                for future in as_completed(future_to_id):
                    item_id = future_to_id[future]
                    try:
                        item_details_map[item_id] = future.result()
                    except Exception as e:
                        ui_logger.error(f"   - âŒ è·å–åª’ä½“ {item_id} è¯¦æƒ…å¤±è´¥: {e}", task_category=task_cat)

            for i, item_id in enumerate(media_ids):
                if cancellation_event.is_set():
                    ui_logger.warning("âš ï¸ ä»»åŠ¡åœ¨æ„å»ºè®¡åˆ’é˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                    return
                
                details = item_details_map.get(item_id)
                if not details:
                    task_manager.update_task_progress(task_id, i + 1, total_items_to_check)
                    continue

                item_name = details.get("Name", f"ID {item_id}")
                image_tags = details.get("ImageTags", {})
                tmdb_id = details.get("ProviderIds", {}).get("Tmdb")

                if not tmdb_id:
                    ui_logger.debug(f"   - [è·³è¿‡] åª’ä½“ã€{item_name}ã€‘ç¼ºå°‘ TMDB IDã€‚", task_category=task_cat)
                    task_manager.update_task_progress(task_id, i + 1, total_items_to_check)
                    continue

                for image_type in content_types:
                    remote_key = f"{tmdb_id}-{image_type}"
                    if remote_key not in remote_map:
                        skipped_for_no_backup += 1
                        continue
                    
                    needs_restore = False
                    if overwrite:
                        needs_restore = True
                    else:
                        type_map = {"poster": "Primary", "logo": "Logo", "fanart": "Backdrop"}
                        if not image_tags.get(type_map.get(image_type)):
                            needs_restore = True
                        else:
                            ui_logger.info(f"   - [è·³è¿‡] åª’ä½“ã€{item_name}ã€‘å·²å­˜åœ¨ {image_type} å›¾ç‰‡ã€‚", task_category=task_cat)
                    
                    if needs_restore:
                        restore_plan.append({"item_id": item_id, "item_name": item_name, "image_type": image_type, "tmdb_id": tmdb_id})
                
                task_manager.update_task_progress(task_id, i + 1, total_items_to_check)

            log_message = f"âœ… æ¢å¤è®¡åˆ’æ„å»ºå®Œæˆï¼Œå…±éœ€æ¢å¤ {len(restore_plan)} å¼ å›¾ç‰‡ã€‚"
            if skipped_for_no_backup > 0:
                log_message += f" (å› è¿œç¨‹æ— å¤‡ä»½è€Œè·³è¿‡ {skipped_for_no_backup} å¼ )"
            ui_logger.info(log_message, task_category=task_cat)

            # é˜¶æ®µä¸‰ï¼šæ‰§è¡Œæ¢å¤è®¡åˆ’
            ui_logger.info("â¡ï¸ [é˜¶æ®µ3/3] å¼€å§‹é€ä¸€æ‰§è¡Œæ¢å¤...", task_category=task_cat)
            total_to_restore = len(restore_plan)
            task_manager.update_task_progress(task_id, 0, total_to_restore)

            for i, plan_item in enumerate(restore_plan):
                if cancellation_event.is_set():
                    ui_logger.warning("âš ï¸ ä»»åŠ¡åœ¨æ‰§è¡Œé˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                    return
                
                item_id = plan_item["item_id"]
                item_name = plan_item["item_name"]
                image_type = plan_item["image_type"]
                tmdb_id = plan_item["tmdb_id"]
                
                ui_logger.info(f"  -> æ­£åœ¨ä¸ºã€{item_name}ã€‘æ¢å¤ {image_type}...", task_category=task_cat)
                try:
                    self._restore_single_image_from_plan(item_id, image_type, tmdb_id, remote_map, task_cat)
                    ui_logger.info(f"     - âœ… æˆåŠŸæ¢å¤ã€{item_name}ã€‘çš„ {image_type} å›¾ç‰‡ã€‚", task_category=task_cat)
                except Exception as e:
                    ui_logger.error(f"     - âŒ æ¢å¤ã€{item_name}ã€‘çš„ {image_type} å›¾ç‰‡å¤±è´¥: {e}", task_category=task_cat)

                task_manager.update_task_progress(task_id, i + 1, total_to_restore)

            ui_logger.info("ğŸ‰ æ¢å¤ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚", task_category=task_cat)

        except Exception as e:
            ui_logger.error(f"âŒ æ¢å¤ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", task_category=task_cat, exc_info=True)
            raise e
        

    # backend/poster_manager_logic.py (æ–°å¢ä»£ç å—)

    def start_restore_from_local_task(
        self,
        scope: ScheduledTasksTargetScope,
        content_types: List[str],
        overwrite: bool,
        cancellation_event: threading.Event,
        task_id: str,
        task_manager: TaskManager
    ):
        """ä»æœ¬åœ°ç¼“å­˜æ¢å¤å›¾ç‰‡åˆ° Emby çš„ä¸»ä»»åŠ¡æµç¨‹"""
        from concurrent.futures import as_completed
        
        task_cat = f"æµ·æŠ¥æ¢å¤(æœ¬åœ°)"
        overwrite_text = "å¼ºåˆ¶è¦†ç›–" if overwrite else "æ™ºèƒ½è·³è¿‡"
        ui_logger.info(f"ğŸ‰ ä»»åŠ¡å¯åŠ¨ï¼Œæ¨¡å¼: ä»æœ¬åœ°ç¼“å­˜æ¢å¤, èŒƒå›´: {scope.mode}, å†…å®¹: {content_types}, ç­–ç•¥: {overwrite_text}", task_category=task_cat)

        try:
            # é˜¶æ®µä¸€ï¼šè·å–ç›®æ ‡åª’ä½“
            ui_logger.info("â¡ï¸ [é˜¶æ®µ1/4] æ­£åœ¨æ ¹æ®èŒƒå›´è·å–ç›®æ ‡åª’ä½“...", task_category=task_cat)
            selector = MediaSelector(self.config)
            media_ids = selector.get_item_ids(scope)
            if not media_ids:
                ui_logger.info("âœ… åœ¨æŒ‡å®šèŒƒå›´å†…æœªæ‰¾åˆ°ä»»ä½•åª’ä½“é¡¹ï¼Œä»»åŠ¡å®Œæˆã€‚", task_category=task_cat)
                return

            # é˜¶æ®µäºŒï¼šæ‰«ææœ¬åœ°ç¼“å­˜
            ui_logger.info("â¡ï¸ [é˜¶æ®µ2/4] æ­£åœ¨æ‰«ææœ¬åœ°ç¼“å­˜ç›®å½•...", task_category=task_cat)
            initial_pending_list = self._scan_local_cache(media_ids, content_types, task_cat)
            if not initial_pending_list:
                ui_logger.info("âœ… åœ¨æœ¬åœ°ç¼“å­˜ä¸­æœªæ‰¾åˆ°ä¸ç›®æ ‡åª’ä½“åŒ¹é…çš„ä»»ä½•å›¾ç‰‡ï¼Œä»»åŠ¡å®Œæˆã€‚", task_category=task_cat)
                return

            # é˜¶æ®µä¸‰ï¼šæ„å»ºæ¢å¤è®¡åˆ’
            ui_logger.info(f"â¡ï¸ [é˜¶æ®µ3/4] æ­£åœ¨æ£€æŸ¥ç›®æ ‡åª’ä½“çŠ¶æ€å¹¶æ„å»ºæ¢å¤è®¡åˆ’...", task_category=task_cat)
            
            # å°†å¾…åŠåˆ—è¡¨æŒ‰ emby_item_id åˆ†ç»„ï¼Œä»¥ä¾¿æ‰¹é‡è·å–è¯¦æƒ…
            emby_ids_to_check = {item['local_path'].split(os.sep)[-2] for item in initial_pending_list}
            
            tmdb_to_emby_map = {}
            for item_id in media_ids:
                tmdb_id = self._get_tmdb_id(item_id)
                if tmdb_id:
                    if tmdb_id not in tmdb_to_emby_map:
                        tmdb_to_emby_map[tmdb_id] = []
                    tmdb_to_emby_map[tmdb_id].append(item_id)

            emby_item_details_map = {}
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_id = {executor.submit(self._get_emby_item_details, item_id, "Name,ImageTags"): item_id for item_id in media_ids}
                for future in as_completed(future_to_id):
                    item_id = future_to_id[future]
                    try:
                        emby_item_details_map[item_id] = future.result()
                    except Exception as e:
                        ui_logger.error(f"   - âŒ è·å–åª’ä½“ (Emby Item ID: {item_id}) è¯¦æƒ…å¤±è´¥: {e}", task_category=task_cat)

            restore_plan = []
            for item in initial_pending_list:
                tmdb_id = item['tmdb_id']
                emby_ids = tmdb_to_emby_map.get(tmdb_id, [])
                for emby_id in emby_ids:
                    details = emby_item_details_map.get(emby_id)
                    if not details:
                        continue

                    image_tags = details.get("ImageTags", {})
                    item_name = details.get("Name", f"ID {emby_id}")
                    image_type = item['image_type']
                    
                    needs_restore = False
                    if overwrite:
                        needs_restore = True
                    else:
                        type_map = {"poster": "Primary", "logo": "Logo", "fanart": "Backdrop"}
                        if not image_tags.get(type_map.get(image_type)):
                            needs_restore = True
                        else:
                            ui_logger.info(f"   - [è·³è¿‡] åª’ä½“ã€{item_name}ã€‘(Emby Item ID: {emby_id}) å·²å­˜åœ¨ {image_type} å›¾ç‰‡ã€‚", task_category=task_cat)
                    
                    if needs_restore:
                        restore_plan.append({
                            "item_id": emby_id,
                            "item_name": item_name,
                            "image_type": image_type,
                            "local_path": item['local_path']
                        })

            ui_logger.info(f"âœ… æ¢å¤è®¡åˆ’æ„å»ºå®Œæˆï¼Œå…±éœ€æ¢å¤ {len(restore_plan)} å¼ å›¾ç‰‡ã€‚", task_category=task_cat)

            # é˜¶æ®µå››ï¼šæ‰§è¡Œæ¢å¤
            ui_logger.info("â¡ï¸ [é˜¶æ®µ4/4] å¼€å§‹é€ä¸€æ‰§è¡Œæ¢å¤...", task_category=task_cat)
            total_to_restore = len(restore_plan)
            task_manager.update_task_progress(task_id, 0, total_to_restore)

            for i, plan_item in enumerate(restore_plan):
                if cancellation_event.is_set():
                    ui_logger.warning("âš ï¸ ä»»åŠ¡åœ¨æ‰§è¡Œé˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                    return
                
                item_id = plan_item["item_id"]
                item_name = plan_item["item_name"]
                image_type = plan_item["image_type"]
                local_path = plan_item["local_path"]
                
                ui_logger.info(f"  -> æ­£åœ¨ä¸ºã€{item_name}ã€‘(Emby Item ID: {item_id}) ä»æœ¬åœ°æ¢å¤ {image_type}...", task_category=task_cat)
                try:
                    self._restore_single_image_from_local(item_id, image_type, local_path)
                    ui_logger.info(f"     - âœ… æˆåŠŸæ¢å¤ã€{item_name}ã€‘(Emby Item ID: {item_id}) çš„ {image_type} å›¾ç‰‡ã€‚", task_category=task_cat)
                except Exception as e:
                    ui_logger.error(f"     - âŒ æ¢å¤ã€{item_name}ã€‘(Emby Item ID: {item_id}) çš„ {image_type} å›¾ç‰‡å¤±è´¥: {e}", task_category=task_cat)

                task_manager.update_task_progress(task_id, i + 1, total_to_restore)

            ui_logger.info("ğŸ‰ ä»æœ¬åœ°ç¼“å­˜æ¢å¤ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚", task_category=task_cat)

        except Exception as e:
            ui_logger.error(f"âŒ ä»æœ¬åœ°ç¼“å­˜æ¢å¤ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", task_category=task_cat, exc_info=True)
            raise e

    def _restore_single_image_from_local(self, item_id: str, image_type: str, local_path: str):
        """ä»æœ¬åœ°æ–‡ä»¶è·¯å¾„æ¢å¤å•å¼ å›¾ç‰‡åˆ°Emby"""
        import requests
        import mimetypes
        import base64

        if not os.path.exists(local_path):
            raise FileNotFoundError(f"æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {local_path}")

        with open(local_path, 'rb') as f:
            image_data = f.read()

        content_type, _ = mimetypes.guess_type(local_path)
        if not content_type:
            content_type = 'image/jpeg' if image_type in ['poster', 'fanart'] else 'image/png'

        emby_image_type_map = {
            "poster": "Primary",
            "logo": "Logo",
            "fanart": "Backdrop"
        }
        emby_image_type = emby_image_type_map.get(image_type)
        if not emby_image_type:
            return

        upload_url = f"{self.config.server_config.server}/Items/{item_id}/Images/{emby_image_type}"
        
        try:
            delete_proxies = self.proxy_manager.get_proxies(upload_url)
            self.session.delete(upload_url, params={"api_key": self.config.server_config.api_key}, timeout=20, proxies=delete_proxies)
        except requests.RequestException:
            pass

        base64_encoded_data = base64.b64encode(image_data).decode('utf-8')
        headers = {'Content-Type': content_type}
        upload_proxies = self.proxy_manager.get_proxies(upload_url)
        
        upload_response = self.session.post(
            upload_url, 
            params={"api_key": self.config.server_config.api_key}, 
            data=base64_encoded_data,
            headers=headers, 
            timeout=60,
            proxies=upload_proxies
        )
        upload_response.raise_for_status()
        

    def get_stats(self, force_refresh: bool = False) -> Dict:
        """è·å–çŠ¶æ€ä»ªè¡¨ç›˜æ‰€éœ€çš„æ•°æ®"""
        repo_count = len(self.pm_config.github_repos)
        

        remote_map = self._get_aggregated_remote_index("çŠ¶æ€è·å–", force_refresh=force_refresh)
        

        if force_refresh:
            self._update_all_repo_sizes("çŠ¶æ€è·å–")

        total_images = len(remote_map)
        total_size_bytes = 0
        type_counts = {"poster": 0, "logo": 0, "fanart": 0}
        for key, value in remote_map.items():
            total_size_bytes += value.get("size", 0)
            if "poster" in key: type_counts["poster"] += 1
            elif "logo" in key: type_counts["logo"] += 1
            elif "fanart" in key: type_counts["fanart"] += 1

        repo_details = []
        total_capacity_bytes = 0
        threshold_bytes = self.pm_config.repository_size_threshold_mb * 1024 * 1024
        

        current_config = app_config_module.load_app_config()
        
        for repo in current_config.poster_manager_config.github_repos:
            match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo.repo_url)
            name = f"{match.group(1)}/{match.group(2)}" if match else repo.repo_url
            used_bytes = repo.state.size_bytes
            repo_details.append({
                "name": name,
                "used_bytes": used_bytes,
                "threshold_bytes": threshold_bytes,
                "last_checked": repo.state.last_checked
            })
            total_capacity_bytes += threshold_bytes

        return {
            "total_images": total_images,
            "total_size_bytes": total_size_bytes,
            "repo_count": repo_count,
            "total_capacity_bytes": total_capacity_bytes,
            "type_counts": type_counts,
            "repo_details": repo_details
        }

    def _get_emby_image_details(self, item_id: str) -> Dict[str, Dict]:
        """
        ä¸€æ¬¡æ€§è·å– Emby åª’ä½“é¡¹æ‰€æœ‰ç±»å‹å›¾ç‰‡ï¼ˆæµ·æŠ¥ã€Logoã€èƒŒæ™¯å›¾ï¼‰çš„è¯¦ç»†ä¿¡æ¯ã€‚
        è¿”å›ä¸€ä¸ªä»¥å›¾ç‰‡ç±»å‹ä¸ºé”®çš„å­—å…¸ã€‚
        """
        import requests
        from urllib.parse import quote, urlencode
        
        task_cat = f"æµ·æŠ¥ç®¡ç†-è°ƒè¯•({item_id})"
        emby_images = {}
        api_key = self.config.server_config.api_key

        try:
            url = f"{self.config.server_config.server}/Items/{item_id}/Images"
            params = {"api_key": api_key}
            proxies = self.proxy_manager.get_proxies(url)
            
            ui_logger.debug(f"â¡ï¸ [è°ƒè¯•] æ­£åœ¨è¯·æ±‚å›¾ç‰‡å…ƒæ•°æ®: {url}", task_category=task_cat)
            response = self.session.get(url, params=params, timeout=15, proxies=proxies)
            response.raise_for_status()
            
            all_image_metadata = response.json()
            if not all_image_metadata:
                ui_logger.debug(f"   - [è°ƒè¯•] æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡å…ƒæ•°æ®ã€‚", task_category=task_cat)
                return {}

            ui_logger.debug(f"   - [è°ƒè¯•] æˆåŠŸè·å–åˆ° {len(all_image_metadata)} æ¡å›¾ç‰‡å…ƒæ•°æ®ã€‚", task_category=task_cat)

            for image_info in all_image_metadata:
                image_type_from_api = image_info.get("ImageType")
                
                type_map = {
                    "Primary": "poster",
                    "Logo": "logo",
                    "Backdrop": "fanart"
                }
                standard_type = type_map.get(image_type_from_api)
                
                if not standard_type:
                    continue

                image_tag = image_info.get("ImageTag")
                base_path = f"Items/{item_id}/Images/{image_type_from_api}"
                

                query_params = {
                    'api_key': api_key,
                    'quality': 100  # è¯·æ±‚æœ€é«˜è´¨é‡çš„å›¾ç‰‡
                }
                if image_tag:
                    query_params['tag'] = image_tag

                
                image_path_to_emby = f"{base_path}?{urlencode(query_params)}"
                
                proxy_url = f"/api/emby-image-proxy?path={quote(image_path_to_emby)}"
                
                ui_logger.debug(f"   - [è°ƒè¯•] ä¸ºç±»å‹ '{standard_type}' ç”Ÿæˆçš„ä»£ç† URL: {proxy_url}", task_category=task_cat)

                width = image_info.get('Width', 0)
                height = image_info.get('Height', 0)
                size_bytes = image_info.get('Size', 0)
                
                resolution = f"{width}x{height}" if width and height else "æœªçŸ¥åˆ†è¾¨ç‡"
                size_str = f"{size_bytes / 1024 / 1024:.2f} MB" if size_bytes > 1024 * 1024 else f"{size_bytes / 1024:.1f} KB"

                emby_images[standard_type] = {
                    "url": proxy_url,
                    "resolution": resolution,
                    "size": size_str
                }
            
            return emby_images

        except requests.RequestException as e:
            ui_logger.error(f"âŒ [è°ƒè¯•] è·å– Emby å›¾ç‰‡å…ƒæ•°æ®åˆ—è¡¨å¤±è´¥: {e}", task_category=task_cat)
            return {}
        except Exception as e:
            ui_logger.error(f"âŒ [è°ƒè¯•] å¤„ç† Emby å›¾ç‰‡å…ƒæ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", task_category=task_cat, exc_info=True)
            return {}
    def get_single_item_details(self, item_id: str) -> Dict:
        """è·å–å•ä¸ªåª’ä½“é¡¹åœ¨ Emby å’Œ GitHub ä¸¤ä¾§çš„å›¾ç‰‡è¯¦æƒ…"""

        emby_details = self._get_emby_item_details(item_id, "Name,ProductionYear,ProviderIds")
        emby_images = self._get_emby_image_details(item_id)
        

        github_images = {}
        tmdb_id = emby_details.get("ProviderIds", {}).get("Tmdb")
        if tmdb_id:
            remote_map = self._get_aggregated_remote_index(f"å•ä½“æŸ¥è¯¢({emby_details.get('Name')})")
            for img_type in ["poster", "logo", "fanart"]:
                key = f"{tmdb_id}-{img_type}"
                if key in remote_map:
                    gh_info = remote_map[key]
                    size_bytes = gh_info.get("size", 0)
                    github_images[img_type] = {
                        "url": gh_info.get("url"),
                        "resolution": "æœªçŸ¥åˆ†è¾¨ç‡",
                        "size": f"{size_bytes / 1024 / 1024:.2f} MB" if size_bytes > 1024 * 1024 else f"{size_bytes / 1024:.1f} KB"
                    }

        return {
            "emby": emby_images,
            "github": github_images
        }



    def backup_single_image(self, item_id: str, image_type: str):
        """å•ä½“å¤‡ä»½ï¼šä»Embyä¸‹è½½å›¾ç‰‡ï¼Œå­˜å…¥æœ¬åœ°ç¼“å­˜ï¼Œå†ä¸Šä¼ åˆ°GitHub"""
        item_details = self._get_emby_item_details(item_id, "Name,ProviderIds")
        item_name = item_details.get("Name", f"ID {item_id}")
        image_type_map = {"poster": "æµ·æŠ¥", "logo": "Logo", "fanart": "èƒŒæ™¯å›¾"}
        image_type_cn = image_type_map.get(image_type, image_type)
        
        task_cat = f"å•ä½“å¤‡ä»½-{item_name}"
        ui_logger.info(f"â¡ï¸ å¼€å§‹ä¸ºã€{item_name}ã€‘æ‰§è¡Œå•ä½“å¤‡ä»½ ({image_type_cn})...", task_category=task_cat)
        
        tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
        if not tmdb_id: raise ValueError("åª’ä½“é¡¹ç¼ºå°‘ TMDB IDã€‚")
        
        emby_key = {"poster": "Primary", "logo": "Logo", "fanart": "Backdrop"}.get(image_type)
        emby_image_url = f"{self.config.server_config.server}/Items/{item_id}/Images/{emby_key}"
        proxies = self.proxy_manager.get_proxies(emby_image_url)
        response = self.session.get(emby_image_url, params={"api_key": self.config.server_config.api_key}, timeout=60, proxies=proxies)
        response.raise_for_status()
        image_data = response.content

        filename = {"poster": "poster.jpg", "logo": "clearlogo.png", "fanart": "fanart.jpg"}.get(image_type)
        local_dir = os.path.join(self.pm_config.local_cache_path, tmdb_id)
        os.makedirs(local_dir, exist_ok=True)
        local_path = os.path.join(local_dir, filename)
        with open(local_path, 'wb') as f:
            f.write(image_data)
        ui_logger.info(f"  - âœ… å›¾ç‰‡å·²æˆåŠŸä¸‹è½½å¹¶è¦†ç›–æœ¬åœ°ç¼“å­˜: {local_path}", task_category=task_cat)

        item_info = {
            "local_path": local_path,
            "tmdb_id": tmdb_id,
            "image_type": image_type,
            "size": len(image_data)
        }
        remote_map = self._get_aggregated_remote_index(task_cat)
        
        overwrite_remote = True 
        new_files, overwrite_files = self._classify_pending_files([item_info], remote_map, overwrite_remote, task_cat)
        
        dispatch_plan = self._calculate_dispatch_plan(new_files, overwrite_files, task_cat)
        self._execute_dispatch_plan(dispatch_plan, task_cat, threading.Event())
        

        self._get_aggregated_remote_index(task_cat, force_refresh=True)
        
        ui_logger.info(f"ğŸ‰ å•ä½“å¤‡ä»½ä»»åŠ¡å®Œæˆã€‚", task_category=task_cat)



    def delete_single_image(self, item_id: str, image_type: str):
        """å•ä½“åˆ é™¤ï¼šä»GitHubåˆ é™¤å›¾ç‰‡å’Œç´¢å¼•æ¡ç›®"""
        task_cat = f"å•ä½“åˆ é™¤({item_id}-{image_type})"
        ui_logger.info(f"â¡ï¸ å¼€å§‹æ‰§è¡Œå•ä½“åˆ é™¤...", task_category=task_cat)

        tmdb_id = self._get_tmdb_id(item_id)
        if not tmdb_id: raise ValueError("åª’ä½“é¡¹ç¼ºå°‘ TMDB IDã€‚")

        remote_map = self._get_aggregated_remote_index(task_cat)
        key = f"{tmdb_id}-{image_type}"
        image_info = remote_map.get(key)
        if not image_info: raise ValueError("åœ¨è¿œç¨‹å¤‡ä»½ä¸­æœªæ‰¾åˆ°è¯¥å›¾ç‰‡ã€‚")

        repo_url = image_info['repo_url']
        sha = image_info['sha']
        
        repo_config = next((r for r in self.pm_config.github_repos if r.repo_url == repo_url), None)
        if not repo_config: raise ValueError(f"é…ç½®ä¸­æ‰¾ä¸åˆ°ä»“åº“ {repo_url}ã€‚")

        pat = repo_config.personal_access_token or self.pm_config.global_personal_access_token
        branch = repo_config.branch
        match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo_url)
        if not match:
            raise ValueError(f"æ— æ³•ä»ä»“åº“URLä¸­è§£æå‡º owner å’Œ repo: {repo_url}")
        owner, repo_name = match.groups()
        
        filename = {"poster": "poster.jpg", "logo": "clearlogo.png", "fanart": "fanart.jpg"}.get(image_type)
        github_path = f"images/{tmdb_id}/{filename}"
        api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/{github_path}"
        delete_payload = {"message": f"refactor: Delete {github_path}", "sha": sha, "branch": branch}
        self._execute_github_write_request("DELETE", api_url, pat, delete_payload)
        ui_logger.info(f"  - âœ… å·²æˆåŠŸä» GitHub åˆ é™¤æ–‡ä»¶: {github_path}", task_category=task_cat)

        index = self._get_repo_index(repo_config.model_dump())
        if str(tmdb_id) in index['images'] and image_type in index['images'][str(tmdb_id)]:
            del index['images'][str(tmdb_id)][image_type]
            if not index['images'][str(tmdb_id)]:
                del index['images'][str(tmdb_id)]
        
        index['last_updated'] = datetime.now().isoformat()
        index_api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/database.json"
        get_index_resp = self.session.get(index_api_url, headers={"Authorization": f"token {pat}"}, proxies=self.proxy_manager.get_proxies(index_api_url)).json()
        index_sha = get_index_resp.get('sha')
        index_payload = {
            "message": "chore: Update index after deletion",
            "content": base64.b64encode(json.dumps(index, indent=2).encode()).decode(),
            "branch": branch,
            "sha": index_sha
        }
        self._execute_github_write_request("PUT", index_api_url, pat, index_payload)
        ui_logger.info(f"  - âœ… ç´¢å¼•æ–‡ä»¶ database.json æ›´æ–°æˆåŠŸã€‚", task_category=task_cat)


        self._update_all_repo_sizes(task_cat)
        self._get_aggregated_remote_index(task_cat, force_refresh=True)

        ui_logger.info(f"ğŸ‰ å•ä½“åˆ é™¤ä»»åŠ¡å®Œæˆã€‚", task_category=task_cat)



    def restore_single_image(self, item_id: str, image_type: str):
        """å•ä½“æ¢å¤ï¼šä»GitHubä¸‹è½½å›¾ç‰‡ï¼Œå¹¶æ¢å¤åˆ°Emby"""
        item_details = self._get_emby_item_details(item_id, "Name,ProviderIds")
        item_name = item_details.get("Name", f"ID {item_id}")
        image_type_map = {"poster": "æµ·æŠ¥", "logo": "Logo", "fanart": "èƒŒæ™¯å›¾"}
        image_type_cn = image_type_map.get(image_type, image_type)

        task_cat = f"å•ä½“æ¢å¤-{item_name}"
        ui_logger.info(f"â¡ï¸ å¼€å§‹ä¸ºã€{item_name}ã€‘æ¢å¤ã€{image_type_cn}ã€‘...", task_category=task_cat)

        try:
            tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
            if not tmdb_id:
                raise ValueError("åª’ä½“é¡¹ç¼ºå°‘ TMDB IDï¼Œæ— æ³•è¿›è¡Œæ¢å¤ã€‚")

            remote_map = self._get_aggregated_remote_index(task_cat)
            
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šç¡®ä¿ä¼ é€’ç»™ image_type çš„æ˜¯å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯åˆ—è¡¨ ---
            self._restore_single_image_from_plan(item_id, image_type, tmdb_id, remote_map, task_cat)
            
            ui_logger.info(f"ğŸ‰ ä¸ºã€{item_name}ã€‘æ¢å¤ã€{image_type_cn}ã€‘çš„ä»»åŠ¡å·²å®Œæˆã€‚", task_category=task_cat)
        
        except Exception as e:

            ui_logger.error(f"âŒ ä¸ºã€{item_name}ã€‘æ¢å¤ã€{image_type_cn}ã€‘çš„ä»»åŠ¡å¤±è´¥ã€‚", task_category=task_cat)
            raise e



    def start_backup_task(
        self,
        scope: ScheduledTasksTargetScope,
        content_types: List[str],
        overwrite: bool,
        cancellation_event: threading.Event,
        task_id: str,
        task_manager: TaskManager
    ):
        """ä»æœ¬åœ°ç¼“å­˜å¤‡ä»½åˆ° GitHub çš„ä¸»ä»»åŠ¡æµç¨‹"""
        task_cat = f"æµ·æŠ¥å¤‡ä»½({scope.mode})"
        ui_logger.info(f"ğŸ‰ ä»»åŠ¡å¯åŠ¨ï¼Œæ¨¡å¼: {scope.mode}, å†…å®¹: {content_types}, è¦†ç›–: {overwrite}", task_category=task_cat)

        try:

            selector = MediaSelector(self.config)
            media_ids = selector.get_item_ids(scope)
            task_manager.update_task_progress(task_id, 10, 100)
            if cancellation_event.is_set(): return

            initial_list = self._scan_local_cache(media_ids, content_types, task_cat)
            task_manager.update_task_progress(task_id, 25, 100)
            if cancellation_event.is_set(): return

            remote_map = self._get_aggregated_remote_index(task_cat, force_refresh=False)
            task_manager.update_task_progress(task_id, 40, 100)
            if cancellation_event.is_set(): return


            new_files, overwrite_files = self._classify_pending_files(initial_list, remote_map, overwrite, task_cat)
            task_manager.update_task_progress(task_id, 50, 100)
            if not new_files and not overwrite_files:
                ui_logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å‡å·²æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œæ— éœ€å¤‡ä»½ã€‚", task_category=task_cat)
                task_manager.update_task_progress(task_id, 100, 100)
                return


            dispatch_plan = self._calculate_dispatch_plan(new_files, overwrite_files, task_cat)
            task_manager.update_task_progress(task_id, 60, 100)
            if cancellation_event.is_set(): return


            self._execute_dispatch_plan(dispatch_plan, task_cat, cancellation_event)
            

            if not cancellation_event.is_set():

                self._get_aggregated_remote_index(task_cat, force_refresh=True)

                self._update_all_repo_sizes(task_cat)

            task_manager.update_task_progress(task_id, 100, 100)
            
            ui_logger.info("ğŸ‰ å¤‡ä»½ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚", task_category=task_cat)

        except Exception as e:
            ui_logger.error(f"âŒ å¤‡ä»½ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", task_category=task_cat, exc_info=True)
            raise e