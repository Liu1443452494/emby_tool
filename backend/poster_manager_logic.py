# backend/poster_manager_logic.py

import logging
import os
import json
import threading
import time
import re
import base64
import subprocess
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
from filelock import FileLock, Timeout
from datetime import datetime, timedelta

from log_manager import ui_logger
from models import AppConfig, PosterManagerConfig, ScheduledTasksTargetScope
from task_manager import TaskManager
from media_selector import MediaSelector
from proxy_manager import ProxyManager
import config as app_config_module

# --- å¸¸é‡å®šä¹‰ ---
AGGREGATED_INDEX_CACHE_FILE = os.path.join('/app/data', 'poster_manager_aggregated_index.json')
AGGREGATED_INDEX_CACHE_DURATION = 3600  # ç¼“å­˜1å°æ—¶ (3600ç§’)

class PosterManagerLogic:
    def __init__(self, config: AppConfig):
        self.config = config
        self.pm_config = config.poster_manager_config
        self.proxy_manager = ProxyManager(config)
        self.session = self._create_session()

    def _create_session(self):
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        return session

    def _get_tmdb_id(self, item_id: str) -> Optional[str]:
        """ä» Emby è·å–åª’ä½“é¡¹çš„ TMDB ID"""
        try:
            url = f"{self.config.server_config.server}/Users/{self.config.server_config.user_id}/Items/{item_id}"
            params = {"api_key": self.config.server_config.api_key, "Fields": "ProviderIds"}
            proxies = self.proxy_manager.get_proxies(url)
            response = self.session.get(url, params=params, timeout=15, proxies=proxies)
            response.raise_for_status()
            provider_ids = response.json().get("ProviderIds", {})
            return provider_ids.get("Tmdb")
        except Exception as e:
            logging.error(f"ã€æµ·æŠ¥å¤‡ä»½ã€‘è·å–åª’ä½“é¡¹ {item_id} çš„ TMDB ID å¤±è´¥: {e}")
            return None

    def _scan_local_cache(self, media_ids: List[str], content_types: List[str], task_cat: str) -> List[Dict]:
        """æ‰«ææœ¬åœ°ç¼“å­˜ç›®å½•ï¼Œç”Ÿæˆåˆå§‹å¾…åŠåˆ—è¡¨"""
        ui_logger.info(f"â¡ï¸ [é˜¶æ®µ1.2] å¼€å§‹æ‰«ææœ¬åœ°ç¼“å­˜ç›®å½•: {self.pm_config.local_cache_path}", task_category=task_cat)
        
        if not self.pm_config.local_cache_path or not os.path.isdir(self.pm_config.local_cache_path):
            raise ValueError(f"æœ¬åœ°ç¼“å­˜è·¯å¾„ '{self.pm_config.local_cache_path}' æ— æ•ˆæˆ–æœªé…ç½®ã€‚")

        type_map = {
            "poster": "poster.jpg",
            "logo": "clearlogo.png",
            "fanart": "fanart.jpg"
        }
        
        initial_pending_list = []
        tmdb_id_map = {}

        # æ‰¹é‡è·å–TMDB IDä»¥å‡å°‘APIè°ƒç”¨
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_id = {executor.submit(self._get_tmdb_id, item_id): item_id for item_id in media_ids}
            for future in future_to_id:
                item_id = future_to_id[future]
                try:
                    tmdb_id = future.result()
                    if tmdb_id:
                        tmdb_id_map[item_id] = tmdb_id
                except Exception as e:
                    logging.error(f"ã€æµ·æŠ¥å¤‡ä»½ã€‘è·å– TMDB ID æ—¶å‡ºé”™ (Emby ID: {item_id}): {e}")

        for item_id in media_ids:
            tmdb_id = tmdb_id_map.get(item_id)
            if not tmdb_id:
                ui_logger.warning(f"âš ï¸ è·³è¿‡ Emby ID: {item_id}ï¼Œå› ä¸ºå®ƒæ²¡æœ‰å…³è”çš„ TMDB IDã€‚", task_category=task_cat)
                continue

            item_cache_dir = os.path.join(self.pm_config.local_cache_path, tmdb_id)
            if not os.path.isdir(item_cache_dir):
                continue

            for content_type in content_types:
                filename = type_map.get(content_type)
                if not filename: continue
                
                filepath = os.path.join(item_cache_dir, filename)
                if os.path.isfile(filepath):
                    try:
                        file_size = os.path.getsize(filepath)
                        initial_pending_list.append({
                            "local_path": filepath,
                            "tmdb_id": tmdb_id,
                            "image_type": content_type,
                            "size": file_size
                        })
                    except OSError as e:
                        ui_logger.error(f"âŒ æ— æ³•è·å–æ–‡ä»¶å¤§å°: {filepath}ã€‚é”™è¯¯: {e}", task_category=task_cat)
        
        ui_logger.info(f"âœ… [é˜¶æ®µ1.2] æœ¬åœ°æ‰«æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(initial_pending_list)} ä¸ªå¾…å¤„ç†æ–‡ä»¶ã€‚", task_category=task_cat)
        return initial_pending_list

    def _get_repo_index(self, repo_config: Dict) -> Optional[Dict]:
        """è·å–å•ä¸ªä»“åº“çš„ database.json å†…å®¹"""
        repo_url = repo_config['repo_url']
        branch = repo_config.get('branch', 'main')
        pat = repo_config.get('personal_access_token') or self.pm_config.global_personal_access_token
        
        match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo_url)
        if not match:
            logging.error(f"ã€æµ·æŠ¥å¤‡ä»½ã€‘æ— æ•ˆçš„ GitHub ä»“åº“ URL: {repo_url}")
            return None
        owner, repo = match.groups()
        
        api_url = f"https://api.github.com/repos/{owner}/{repo}/contents/database.json?ref={branch}"
        headers = {"Accept": "application/vnd.github.v3+json"}
        if pat:
            headers["Authorization"] = f"token {pat}"
        
        try:
            proxies = self.proxy_manager.get_proxies(api_url)
            response = self.session.get(api_url, headers=headers, timeout=30, proxies=proxies)
            if response.status_code == 404:
                return {"version": 1, "last_updated": "", "images": {}} # ä»“åº“æ˜¯æ–°çš„ï¼Œè¿”å›ç©ºç´¢å¼•
            response.raise_for_status()
            
            content = base64.b64decode(response.json()['content']).decode('utf-8')
            return json.loads(content)
        except Exception as e:
            logging.error(f"ã€æµ·æŠ¥å¤‡ä»½ã€‘è·å–ä»“åº“ {repo_url} çš„ç´¢å¼•å¤±è´¥: {e}")
            return None

    def _get_aggregated_remote_index(self, task_cat: str) -> Dict:
        """é€šè¿‡ç¼“å­˜æˆ–å¹¶å‘è·å–ï¼Œå¾—åˆ°èšåˆçš„æ‰€æœ‰è¿œç¨‹æ–‡ä»¶ç´¢å¼•"""
        ui_logger.info("â¡ï¸ [é˜¶æ®µ1.3] å¼€å§‹è·å–å¹¶èšåˆæ‰€æœ‰è¿œç¨‹ä»“åº“çš„ç´¢å¼•...", task_category=task_cat)
        
        lock_path = AGGREGATED_INDEX_CACHE_FILE + ".lock"
        try:
            with FileLock(lock_path, timeout=5):
                if os.path.exists(AGGREGATED_INDEX_CACHE_FILE):
                    with open(AGGREGATED_INDEX_CACHE_FILE, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)
                    cached_at = datetime.fromisoformat(cache_data.get("cached_at"))
                    if datetime.now() - cached_at < timedelta(seconds=AGGREGATED_INDEX_CACHE_DURATION):
                        ui_logger.info("âœ… [é˜¶æ®µ1.3] å‘½ä¸­æœ¬åœ°èšåˆç´¢å¼•ç¼“å­˜ã€‚", task_category=task_cat)
                        return cache_data.get("aggregated_index", {})
        except (Timeout, IOError, json.JSONDecodeError) as e:
            ui_logger.warning(f"âš ï¸ è¯»å–èšåˆç¼“å­˜å¤±è´¥ï¼Œå°†å¼ºåˆ¶ä»ç½‘ç»œè·å–ã€‚åŸå› : {e}", task_category=task_cat)

        remote_file_map = {}
        repos = self.pm_config.github_repos
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_repo = {executor.submit(self._get_repo_index, repo.model_dump()): repo for repo in repos}
            for future in future_to_repo:
                repo_config = future_to_repo[future]
                try:
                    time.sleep(0.1) # å†…éƒ¨çŸ­å†·å´
                    index_content = future.result()
                    if index_content and "images" in index_content:
                        for tmdb_id, images in index_content["images"].items():
                            for image_type, image_info in images.items():
                                key = f"{tmdb_id}-{image_type}"
                                remote_file_map[key] = image_info
                except Exception as e:
                    ui_logger.error(f"âŒ å¤„ç†ä»“åº“ {repo_config.repo_url} ç´¢å¼•æ—¶å‡ºé”™: {e}", task_category=task_cat)

        # å›å†™ç¼“å­˜
        try:
            with FileLock(lock_path, timeout=10):
                cache_content = {
                    "cached_at": datetime.now().isoformat(),
                    "aggregated_index": remote_file_map
                }
                with open(AGGREGATED_INDEX_CACHE_FILE, 'w', encoding='utf-8') as f:
                    json.dump(cache_content, f)
                ui_logger.info("âœ… [é˜¶æ®µ1.3] æˆåŠŸèšåˆè¿œç¨‹ç´¢å¼•å¹¶å·²å†™å…¥æœ¬åœ°ç¼“å­˜ã€‚", task_category=task_cat)
        except Exception as e:
            ui_logger.error(f"âŒ å†™å…¥èšåˆç´¢å¼•ç¼“å­˜å¤±è´¥: {e}", task_category=task_cat)
            
        return remote_file_map

    def _classify_pending_files(self, initial_list: List[Dict], remote_map: Dict, task_cat: str) -> Tuple[List, List]:
        """å°†å¾…åŠåˆ—è¡¨åˆ†ä¸ºæ–°å¢å’Œè¦†ç›–ä¸¤ç±»"""
        ui_logger.info("â¡ï¸ [é˜¶æ®µ2] å¼€å§‹å¯¹å¾…åŠæ–‡ä»¶è¿›è¡Œåˆ†ç±» (æ–°å¢/è¦†ç›–)...", task_category=task_cat)
        new_files = []
        overwrite_files = []
        
        for item in initial_list:
            key = f"{item['tmdb_id']}-{item['image_type']}"
            remote_info = remote_map.get(key)
            
            if not remote_info:
                new_files.append(item)
            elif self.pm_config.overwrite_remote_files:
                item['remote_info'] = remote_info
                overwrite_files.append(item)
        
        ui_logger.info(f"âœ… [é˜¶æ®µ2] åˆ†ç±»å®Œæˆã€‚æ–°å¢: {len(new_files)} é¡¹, è¦†ç›–: {len(overwrite_files)} é¡¹ã€‚", task_category=task_cat)
        return new_files, overwrite_files

    def _calculate_dispatch_plan(self, new_files: List, overwrite_files: List, task_cat: str) -> Dict:
        """æ ¸å¿ƒç®—æ³•ï¼šè®¡ç®—æ–‡ä»¶åˆ†å‘è®¡åˆ’"""
        ui_logger.info("â¡ï¸ [é˜¶æ®µ3] æ­£åœ¨è¿›è¡Œç²¾ç¡®é¢„è®¡ç®—ï¼Œç”Ÿæˆæ–‡ä»¶åˆ†å‘è®¡åˆ’...", task_category=task_cat)
        
        dispatch_plan = {repo.repo_url: {"new": [], "overwrite": []} for repo in self.pm_config.github_repos}
        
        # åˆå§‹åŒ–ä»“åº“ä¸´æ—¶çŠ¶æ€
        threshold_bytes = self.pm_config.repository_size_threshold_mb * 1024 * 1024
        temp_repo_states = {
            repo.repo_url: threshold_bytes - (repo.state.size_kb * 1024)
            for repo in self.pm_config.github_repos
        }

        # 1. å¤„ç†è¦†ç›–æ–‡ä»¶
        for item in overwrite_files:
            new_size = item['size']
            old_size = item['remote_info']['size']
            delta = new_size - old_size
            repo_url = item['remote_info']['repo_url']
            
            if temp_repo_states.get(repo_url, -1) < delta:
                raise ValueError(f"æ–‡ä»¶è¦†ç›–å¤±è´¥ï¼šè¦†ç›–æ–‡ä»¶ {item['local_path']} å°†å¯¼è‡´ä»“åº“ {repo_url} è¶…å‡ºå®¹é‡é™åˆ¶ã€‚")
            
            dispatch_plan[repo_url]["overwrite"].append(item)
            temp_repo_states[repo_url] -= delta
            ui_logger.info(f"  - [è®¡åˆ’-è¦†ç›–] {os.path.basename(item['local_path'])} -> åˆ†é…è‡³åŸä»“åº“ {repo_url} (ç©ºé—´å˜åŒ–: {delta/1024/1024:+.2f} MB)", task_category=task_cat)

        # 2. å¤„ç†æ–°å¢æ–‡ä»¶
        for item in new_files:
            allocated = False
            for repo in self.pm_config.github_repos: # æŒ‰ä¼˜å…ˆçº§é¡ºåº
                if temp_repo_states.get(repo.repo_url, -1) >= item['size']:
                    dispatch_plan[repo.repo_url]["new"].append(item)
                    temp_repo_states[repo.repo_url] -= item['size']
                    ui_logger.info(f"  - [è®¡åˆ’-æ–°å¢] {os.path.basename(item['local_path'])} ({item['size']/1024/1024:.2f} MB) -> åˆ†é…è‡³ {repo.repo_url}", task_category=task_cat)
                    allocated = True
                    break
            if not allocated:
                raise ValueError(f"æ–‡ä»¶åˆ†é…å¤±è´¥ï¼šæ–‡ä»¶ {item['local_path']} ({item['size']/1024/1024:.2f} MB) è¿‡å¤§ï¼Œæ‰€æœ‰ä»“åº“å‡æ— è¶³å¤Ÿç©ºé—´å®¹çº³ã€‚")

        ui_logger.info("âœ… [é˜¶æ®µ3] æ–‡ä»¶åˆ†å‘è®¡åˆ’åˆ¶å®šæˆåŠŸã€‚", task_category=task_cat)
        return dispatch_plan
    
    def _execute_github_write_request(self, method: str, url: str, pat: str, payload: Optional[Dict] = None) -> Dict:
        """é€šè¿‡ curl æ‰§è¡Œ GitHub å†™å…¥æ“ä½œ"""
        command = [
            'curl', '-L', '-X', method,
            '-H', 'Accept: application/vnd.github.v3+json',
            '-H', f'Authorization: token {pat}',
            '-H', 'Content-Type: application/json'
        ]
        
        json_payload_str = ""
        if payload:
            command.extend(['--data-binary', '@-'])
            json_payload_str = json.dumps(payload)

        proxies = self.proxy_manager.get_proxies(url)
        if proxies.get('https'):
            command.extend(['--proxy', proxies['https']])

        result = subprocess.run(command, input=json_payload_str, capture_output=True, text=True, check=False)
        
        response_data = {}
        try:
            if result.stdout:
                response_data = json.loads(result.stdout)
        except json.JSONDecodeError:
            raise Exception(f"cURL è¿”å›äº†éJSONå“åº”: {result.stdout} | é”™è¯¯: {result.stderr}")

        if result.returncode != 0 or (response_data.get("message") and response_data.get("documentation_url")):
            error_message = response_data.get('message', f"cURL é”™è¯¯: {result.stderr}")
            if response_data.get('status') == '422' and "sha" in error_message:
                error_message = f"æ— æ•ˆè¯·æ±‚ (422)ã€‚æœåŠ¡å™¨æç¤º 'sha' å‚æ•°æœ‰é—®é¢˜ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºåœ¨æ‚¨æ“ä½œæœŸé—´ï¼Œæ–‡ä»¶è¢«å…¶ä»–è¿›ç¨‹ä¿®æ”¹ã€‚è¯·é‡è¯•ã€‚({error_message})"
            elif "409 Conflict" in result.stderr:
                error_message = "GitHub API è¿”å› 409 Conflict é”™è¯¯ï¼Œè¿™é€šå¸¸æ˜¯å¹¶å‘å†™å…¥å†²çªå¯¼è‡´çš„ã€‚è¯·ç¨åé‡è¯•ã€‚"
            raise Exception(f"GitHub API é”™è¯¯: {error_message}")

        return response_data

    def _get_latest_repo_size(self, repo_url: str, pat: str) -> int:
        """è·å–ä»“åº“çš„æœ€æ–°å¤§å° (KB)"""
        match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo_url)
        owner, repo = match.groups()
        api_url = f"https://api.github.com/repos/{owner}/{repo}"
        headers = {"Accept": "application/vnd.github.v3+json"}
        if pat:
            headers["Authorization"] = f"token {pat}"
        
        proxies = self.proxy_manager.get_proxies(api_url)
        response = self.session.get(api_url, headers=headers, timeout=30, proxies=proxies)
        response.raise_for_status()
        return response.json().get('size', 0)

    # TODO: å®ç°é˜¶æ®µå››çš„æ‰§è¡Œé€»è¾‘
    def _execute_dispatch_plan(self, dispatch_plan: Dict, task_cat: str, cancellation_event: threading.Event):
        """æ‰§è¡Œæ–‡ä»¶ä¸Šä¼ å’Œç´¢å¼•æ›´æ–°"""
        ui_logger.info("â¡ï¸ [é˜¶æ®µ4] å¼€å§‹æ‰§è¡Œæ–‡ä»¶ä¸Šä¼ å’Œç´¢å¼•æ›´æ–°...", task_category=task_cat)
        
        # æŒ‰ä¼˜å…ˆçº§å¤„ç†ä»“åº“
        for repo_config in self.pm_config.github_repos:
            repo_url = repo_config.repo_url
            plan = dispatch_plan.get(repo_url)
            if not plan or (not plan['new'] and not plan['overwrite']):
                continue

            if cancellation_event.is_set():
                ui_logger.warning("âš ï¸ ä»»åŠ¡åœ¨æ‰§è¡Œé˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                return

            ui_logger.info(f"  - æ­£åœ¨å¤„ç†ä»“åº“: {repo_url}", task_category=task_cat)
            pat = repo_config.personal_access_token or self.pm_config.global_personal_access_token
            branch = repo_config.branch
            match = re.match(r"httpshttps?://github\.com/([^/]+)/([^/]+)", repo_url)
            owner, repo_name = match.groups()

            # 1. åŠ é”
            lock_path = ".lock"
            lock_api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/{lock_path}"
            lock_payload = {
                "message": f"feat: Acquire lock for task",
                "content": base64.b64encode(f"locked_at: {datetime.now().isoformat()}".encode()).decode(),
                "branch": branch
            }
            self._execute_github_write_request("PUT", lock_api_url, pat, lock_payload)
            ui_logger.info(f"    - ğŸ”’ å·²æˆåŠŸåœ¨ä»“åº“ {repo_url} ä¸­åˆ›å»º .lock æ–‡ä»¶ã€‚", task_category=task_cat)

            try:
                # 2. è·å–æœ€æ–°ç´¢å¼•
                current_index = self._get_repo_index(repo_config.model_dump())
                if current_index is None:
                    raise Exception("è·å–æœ€æ–°ç´¢å¼•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ã€‚")

                # 3. ä¸Šä¼ æ–‡ä»¶
                files_to_process = plan['overwrite'] + plan['new']
                for item in files_to_process:
                    if cancellation_event.is_set(): raise InterruptedError("ä»»åŠ¡è¢«å–æ¶ˆ")
                    
                    time.sleep(self.pm_config.file_upload_cooldown_seconds)
                    
                    with open(item['local_path'], 'rb') as f:
                        content_b64 = base64.b64encode(f.read()).decode()
                    
                    github_path = f"images/{item['tmdb_id']}/{os.path.basename(item['local_path'])}"
                    api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/{github_path}"
                    
                    payload = {
                        "message": f"feat: Add/Update {item['image_type']} for {item['tmdb_id']}",
                        "content": content_b64,
                        "branch": branch
                    }

                    # å¦‚æœæ˜¯è¦†ç›–ï¼Œå¸¦ä¸ŠSHA
                    if 'remote_info' in item:
                        payload['sha'] = item['remote_info']['sha']

                    try:
                        response_data = self._execute_github_write_request("PUT", api_url, pat, payload)
                    except Exception as e:
                        # æ–°å¢è½¬è¦†ç›–å®¹é”™
                        if "422" in str(e) and 'remote_info' not in item:
                            ui_logger.warning(f"    - ğŸ”„ æ–‡ä»¶ {github_path} å·²å­˜åœ¨ï¼Œè§¦å‘â€œæ–°å¢è½¬è¦†ç›–â€å®¹é”™æœºåˆ¶...", task_category=task_cat)
                            # è·å–SHAé‡è¯•
                            get_resp = self.session.get(api_url, headers={"Authorization": f"token {pat}"}, proxies=self.proxy_manager.get_proxies(api_url)).json()
                            payload['sha'] = get_resp['sha']
                            response_data = self._execute_github_write_request("PUT", api_url, pat, payload)
                        else:
                            raise e

                    # 4. æ›´æ–°å†…å­˜ä¸­çš„ç´¢å¼•
                    tmdb_id_str = str(item['tmdb_id'])
                    if tmdb_id_str not in current_index['images']:
                        current_index['images'][tmdb_id_str] = {}
                    
                    current_index['images'][tmdb_id_str][item['image_type']] = {
                        "repo_url": repo_url,
                        "sha": response_data['content']['sha'],
                        "size": response_data['content']['size'],
                        "url": response_data['content']['download_url']
                    }
                    ui_logger.info(f"    - â¬†ï¸ ä¸Šä¼ æˆåŠŸ: {github_path}", task_category=task_cat)

                # 5. æäº¤æ›´æ–°åçš„ç´¢å¼•
                current_index['last_updated'] = datetime.now().isoformat()
                index_api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/database.json"
                
                # è·å–æœ€æ–°çš„ index SHA
                get_index_resp = self.session.get(index_api_url, headers={"Authorization": f"token {pat}"}, proxies=self.proxy_manager.get_proxies(index_api_url)).json()
                index_sha = get_index_resp.get('sha')

                index_payload = {
                    "message": f"chore: Update database index",
                    "content": base64.b64encode(json.dumps(current_index, indent=2).encode()).decode(),
                    "branch": branch
                }
                if index_sha:
                    index_payload['sha'] = index_sha
                
                self._execute_github_write_request("PUT", index_api_url, pat, index_payload)
                ui_logger.info(f"    - ç´¢å¼•æ–‡ä»¶ database.json æ›´æ–°æˆåŠŸã€‚", task_category=task_cat)

                # 6. æ›´æ–°configä¸­çš„ä»“åº“å¤§å°
                latest_size_kb = self._get_latest_repo_size(repo_url, pat)
                current_config = app_config_module.load_app_config()
                for r in current_config.poster_manager_config.github_repos:
                    if r.repo_url == repo_url:
                        r.state.size_kb = latest_size_kb
                        r.state.last_checked = datetime.now().isoformat()
                        break
                app_config_module.save_app_config(current_config)
                ui_logger.info(f"    - ä»“åº“ {repo_url} æœ€æ–°å®¹é‡ {latest_size_kb} KB å·²å›å†™é…ç½®ã€‚", task_category=task_cat)

            finally:
                # 7. è§£é”
                # è·å–é”æ–‡ä»¶çš„SHA
                lock_get_resp = self.session.get(lock_api_url, headers={"Authorization": f"token {pat}"}, proxies=self.proxy_manager.get_proxies(lock_api_url)).json()
                lock_sha = lock_get_resp.get('sha')
                if lock_sha:
                    delete_payload = {
                        "message": "feat: Release lock",
                        "sha": lock_sha,
                        "branch": branch
                    }
                    self._execute_github_write_request("DELETE", lock_api_url, pat, delete_payload)
                    ui_logger.info(f"    - ğŸ”“ å·²æˆåŠŸä»ä»“åº“ {repo_url} ä¸­ç§»é™¤ .lock æ–‡ä»¶ã€‚", task_category=task_cat)

        ui_logger.info("âœ… [é˜¶æ®µ4] æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å’Œç´¢å¼•æ›´æ–°å®Œæˆã€‚", task_category=task_cat)

    def _restore_single_item(self, item_id: str, tmdb_id: str, content_types: List[str], remote_map: Dict, task_cat: str):
        """æ¢å¤å•ä¸ªåª’ä½“é¡¹çš„å›¾ç‰‡"""
        import requests
        
        item_name = ""
        try:
            # å°è¯•è·å– Emby åª’ä½“é¡¹åç§°ç”¨äºæ—¥å¿—è®°å½•
            details = self._get_emby_item_details(item_id, "Name")
            item_name = details.get("Name", f"ID {item_id}")
        except Exception:
            item_name = f"ID {item_id}"

        ui_logger.info(f"  -> æ­£åœ¨ä¸ºã€{item_name}ã€‘æ‰§è¡Œæ¢å¤...", task_category=task_cat)
        
        for image_type in content_types:
            key = f"{tmdb_id}-{image_type}"
            image_info = remote_map.get(key)

            if not image_info:
                ui_logger.debug(f"     - è·³è¿‡: åœ¨è¿œç¨‹å¤‡ä»½ä¸­æœªæ‰¾åˆ°ã€{item_name}ã€‘çš„ {image_type} å›¾ç‰‡ã€‚", task_category=task_cat)
                continue

            image_url = image_info.get("url")
            if not image_url:
                ui_logger.warning(f"     - âš ï¸ è­¦å‘Š: è¿œç¨‹å¤‡ä»½ä¸­ã€{item_name}ã€‘çš„ {image_type} å›¾ç‰‡è®°å½•ç¼ºå°‘ä¸‹è½½URLã€‚", task_category=task_cat)
                continue

            try:
                # åº”ç”¨å›¾ç‰‡ä¸‹è½½å†·å´
                time.sleep(self.pm_config.image_download_cooldown_seconds)
                
                ui_logger.debug(f"     - æ­£åœ¨ä¸‹è½½: {image_url}", task_category=task_cat)
                proxies = self.proxy_manager.get_proxies(image_url)
                image_response = self.session.get(image_url, timeout=60, proxies=proxies)
                image_response.raise_for_status()
                image_data = image_response.content
                
                # å‡†å¤‡ä¸Šä¼ åˆ° Emby
                emby_image_type_map = {
                    "poster": "Primary",
                    "logo": "Logo",
                    "fanart": "Backdrop"
                }
                emby_image_type = emby_image_type_map.get(image_type)
                if not emby_image_type: continue

                upload_url = f"{self.config.server_config.server}/Items/{item_id}/Images/{emby_image_type}"
                
                # åˆ é™¤æ—§å›¾
                try:
                    delete_proxies = self.proxy_manager.get_proxies(upload_url)
                    self.session.delete(upload_url, params={"api_key": self.config.server_config.api_key}, timeout=20, proxies=delete_proxies)
                except requests.RequestException:
                    # åˆ é™¤å¤±è´¥æ˜¯æ­£å¸¸æƒ…å†µï¼ˆå¯èƒ½åŸæœ¬å°±æ²¡æœ‰å›¾ï¼‰
                    pass

                # ä¸Šä¼ æ–°å›¾
                headers = {'Content-Type': image_response.headers.get('Content-Type', 'image/jpeg')}
                upload_proxies = self.proxy_manager.get_proxies(upload_url)
                upload_response = self.session.post(
                    upload_url, 
                    params={"api_key": self.config.server_config.api_key}, 
                    data=image_data, 
                    headers=headers, 
                    timeout=60,
                    proxies=upload_proxies
                )
                upload_response.raise_for_status()
                ui_logger.info(f"     - âœ… æˆåŠŸæ¢å¤ã€{item_name}ã€‘çš„ {image_type} å›¾ç‰‡ã€‚", task_category=task_cat)

            except Exception as e:
                ui_logger.error(f"     - âŒ æ¢å¤ã€{item_name}ã€‘çš„ {image_type} å›¾ç‰‡å¤±è´¥: {e}", task_category=task_cat)


    def start_restore_task(
        self,
        scope: ScheduledTasksTargetScope,
        content_types: List[str],
        cancellation_event: threading.Event,
        task_id: str,
        task_manager: TaskManager
    ):
        """ä» GitHub æ¢å¤å›¾ç‰‡åˆ° Emby çš„ä¸»ä»»åŠ¡æµç¨‹"""
        task_cat = f"æµ·æŠ¥æ¢å¤({scope.mode})"
        ui_logger.info(f"ğŸ‰ ä»»åŠ¡å¯åŠ¨ï¼Œæ¨¡å¼: {scope.mode}, å†…å®¹: {content_types}", task_category=task_cat)

        try:
            # é˜¶æ®µä¸€ï¼šé¢„å¤„ç†
            ui_logger.info("â¡ï¸ [é˜¶æ®µ1/2] æ­£åœ¨è·å–è¿œç¨‹ç´¢å¼•å’Œåª’ä½“åˆ—è¡¨...", task_category=task_cat)
            remote_map = self._get_aggregated_remote_index(task_cat)
            if not remote_map:
                raise ValueError("æ— æ³•è·å–è¿œç¨‹èšåˆç´¢å¼•ï¼Œä»»åŠ¡ä¸­æ­¢ã€‚")

            selector = MediaSelector(self.config)
            media_ids = selector.get_item_ids(scope)
            
            total_items = len(media_ids)
            task_manager.update_task_progress(task_id, 0, total_items)
            ui_logger.info(f"âœ… [é˜¶æ®µ1/2] é¢„å¤„ç†å®Œæˆï¼Œå…±éœ€æ£€æŸ¥ {total_items} ä¸ªåª’ä½“é¡¹ã€‚", task_category=task_cat)
            
            # é˜¶æ®µäºŒï¼šæ‰§è¡Œæ¢å¤
            ui_logger.info("â¡ï¸ [é˜¶æ®µ2/2] å¼€å§‹é€ä¸€æ¢å¤åª’ä½“é¡¹...", task_category=task_cat)
            
            # æ‰¹é‡è·å–TMDB ID
            tmdb_id_map = {}
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_id = {executor.submit(self._get_tmdb_id, item_id): item_id for item_id in media_ids}
                for future in future_to_id:
                    item_id = future_to_id[future]
                    try:
                        tmdb_id = future.result()
                        if tmdb_id:
                            tmdb_id_map[item_id] = tmdb_id
                    except Exception as e:
                        logging.error(f"ã€æµ·æŠ¥æ¢å¤ã€‘è·å– TMDB ID æ—¶å‡ºé”™ (Emby ID: {item_id}): {e}")

            for i, item_id in enumerate(media_ids):
                if cancellation_event.is_set():
                    ui_logger.warning("âš ï¸ ä»»åŠ¡åœ¨æ‰§è¡Œé˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
                    return

                tmdb_id = tmdb_id_map.get(item_id)
                if not tmdb_id:
                    ui_logger.debug(f"  -> è·³è¿‡ Emby ID: {item_id}ï¼Œå› ä¸ºå®ƒæ²¡æœ‰å…³è”çš„ TMDB IDã€‚", task_category=task_cat)
                else:
                    self._restore_single_item(item_id, tmdb_id, content_types, remote_map, task_cat)
                
                task_manager.update_task_progress(task_id, i + 1, total_items)

            ui_logger.info("ğŸ‰ æ¢å¤ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚", task_category=task_cat)

        except Exception as e:
            ui_logger.error(f"âŒ æ¢å¤ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", task_category=task_cat, exc_info=True)
            raise e
        
    def get_stats(self) -> Dict:
        """è·å–çŠ¶æ€ä»ªè¡¨ç›˜æ‰€éœ€çš„æ•°æ®"""
        total_images = 0
        total_size_bytes = 0
        repo_count = len(self.pm_config.github_repos)
        
        remote_map = self._get_aggregated_remote_index("çŠ¶æ€è·å–")
        total_images = len(remote_map)
        
        type_counts = {"poster": 0, "logo": 0, "fanart": 0}
        for key, value in remote_map.items():
            total_size_bytes += value.get("size", 0)
            if "poster" in key: type_counts["poster"] += 1
            elif "logo" in key: type_counts["logo"] += 1
            elif "fanart" in key: type_counts["fanart"] += 1

        repo_details = []
        total_capacity_bytes = 0
        threshold_bytes = self.pm_config.repository_size_threshold_mb * 1024 * 1024
        
        for repo in self.pm_config.github_repos:
            match = re.match(r"https?://github\.com/([^/]+)/([^/]+)", repo.repo_url)
            name = f"{match.group(1)}/{match.group(2)}" if match else repo.repo_url
            used_bytes = repo.state.size_kb * 1024
            repo_details.append({
                "name": name,
                "used_bytes": used_bytes,
                "threshold_bytes": threshold_bytes,
                "last_checked": repo.state.last_checked
            })
            total_capacity_bytes += threshold_bytes

        return {
            "total_images": total_images,
            "total_size_bytes": total_size_bytes,
            "repo_count": repo_count,
            "total_capacity_bytes": total_capacity_bytes,
            "type_counts": type_counts,
            "repo_details": repo_details
        }

    def get_single_item_details(self, item_id: str) -> Dict:
        """è·å–å•ä¸ªåª’ä½“é¡¹åœ¨ Emby å’Œ GitHub ä¸¤ä¾§çš„å›¾ç‰‡è¯¦æƒ…"""
        # 1. è·å– Emby ä¾§ä¿¡æ¯
        emby_details = self._get_emby_item_details(item_id, "ImageTags,Name,ProductionYear,ProviderIds")
        emby_images = {}
        image_tags = emby_details.get("ImageTags", {})
        for img_type, emby_key in {"poster": "Primary", "logo": "Logo", "fanart": "Backdrop"}.items():
            if image_tags.get(emby_key):
                # ä»…è¿”å›å­˜åœ¨å³å¯ï¼Œå…·ä½“URLç”±å‰ç«¯ä»£ç†æ‹¼æ¥
                emby_images[img_type] = {"url": f"/api/emby-image-proxy?path=Items/{item_id}/Images/{emby_key}"}
        
        # 2. è·å– GitHub ä¾§ä¿¡æ¯
        github_images = {}
        tmdb_id = emby_details.get("ProviderIds", {}).get("Tmdb")
        if tmdb_id:
            remote_map = self._get_aggregated_remote_index(f"å•ä½“æŸ¥è¯¢({emby_details.get('Name')})")
            for img_type in ["poster", "logo", "fanart"]:
                key = f"{tmdb_id}-{img_type}"
                if key in remote_map:
                    github_images[img_type] = remote_map[key]

        return {
            "emby": emby_images,
            "github": github_images
        }

    def backup_single_image(self, item_id: str, image_type: str):
        """å•ä½“å¤‡ä»½ï¼šä»Embyä¸‹è½½å›¾ç‰‡ï¼Œå­˜å…¥æœ¬åœ°ç¼“å­˜ï¼Œå†ä¸Šä¼ åˆ°GitHub"""
        task_cat = f"å•ä½“å¤‡ä»½({item_id}-{image_type})"
        ui_logger.info(f"â¡ï¸ å¼€å§‹æ‰§è¡Œå•ä½“å¤‡ä»½...", task_category=task_cat)
        
        # 1. è·å–å¿…è¦ä¿¡æ¯
        tmdb_id = self._get_tmdb_id(item_id)
        if not tmdb_id: raise ValueError("åª’ä½“é¡¹ç¼ºå°‘ TMDB IDã€‚")
        
        # 2. ä»Embyä¸‹è½½å›¾ç‰‡
        emby_key = {"poster": "Primary", "logo": "Logo", "fanart": "Backdrop"}.get(image_type)
        emby_image_url = f"{self.config.server_config.server}/Items/{item_id}/Images/{emby_key}"
        proxies = self.proxy_manager.get_proxies(emby_image_url)
        response = self.session.get(emby_image_url, params={"api_key": self.config.server_config.api_key}, timeout=60, proxies=proxies)
        response.raise_for_status()
        image_data = response.content

        # 3. å†™å…¥æœ¬åœ°ç¼“å­˜
        filename = {"poster": "poster.jpg", "logo": "clearlogo.png", "fanart": "fanart.jpg"}.get(image_type)
        local_dir = os.path.join(self.pm_config.local_cache_path, tmdb_id)
        os.makedirs(local_dir, exist_ok=True)
        local_path = os.path.join(local_dir, filename)
        with open(local_path, 'wb') as f:
            f.write(image_data)
        ui_logger.info(f"  - âœ… å›¾ç‰‡å·²æˆåŠŸä¸‹è½½å¹¶è¦†ç›–æœ¬åœ°ç¼“å­˜: {local_path}", task_category=task_cat)

        # 4. æ‰§è¡Œä¸Šä¼ ï¼ˆå¤ç”¨å¤‡ä»½æµç¨‹ï¼‰
        item_info = {
            "local_path": local_path,
            "tmdb_id": tmdb_id,
            "image_type": image_type,
            "size": len(image_data)
        }
        remote_map = self._get_aggregated_remote_index(task_cat)
        new_files, overwrite_files = self._classify_pending_files([item_info], remote_map, task_cat)
        
        dispatch_plan = self._calculate_dispatch_plan(new_files, overwrite_files, task_cat)
        self._execute_dispatch_plan(dispatch_plan, task_cat, threading.Event())
        
        ui_logger.info(f"ğŸ‰ å•ä½“å¤‡ä»½ä»»åŠ¡å®Œæˆã€‚", task_category=task_cat)

    def delete_single_image(self, item_id: str, image_type: str):
        """å•ä½“åˆ é™¤ï¼šä»GitHubåˆ é™¤å›¾ç‰‡å’Œç´¢å¼•æ¡ç›®"""
        task_cat = f"å•ä½“åˆ é™¤({item_id}-{image_type})"
        ui_logger.info(f"â¡ï¸ å¼€å§‹æ‰§è¡Œå•ä½“åˆ é™¤...", task_category=task_cat)

        tmdb_id = self._get_tmdb_id(item_id)
        if not tmdb_id: raise ValueError("åª’ä½“é¡¹ç¼ºå°‘ TMDB IDã€‚")

        remote_map = self._get_aggregated_remote_index(task_cat)
        key = f"{tmdb_id}-{image_type}"
        image_info = remote_map.get(key)
        if not image_info: raise ValueError("åœ¨è¿œç¨‹å¤‡ä»½ä¸­æœªæ‰¾åˆ°è¯¥å›¾ç‰‡ã€‚")

        repo_url = image_info['repo_url']
        sha = image_info['sha']
        
        repo_config = next((r for r in self.pm_config.github_repos if r.repo_url == repo_url), None)
        if not repo_config: raise ValueError(f"é…ç½®ä¸­æ‰¾ä¸åˆ°ä»“åº“ {repo_url}ã€‚")

        pat = repo_config.personal_access_token or self.pm_config.global_personal_access_token
        branch = repo_config.branch
        match = re.match(r"httpshttps?://github\.com/([^/]+)/([^/]+)", repo_url)
        owner, repo_name = match.groups()
        
        # 1. åˆ é™¤æ–‡ä»¶
        filename = {"poster": "poster.jpg", "logo": "clearlogo.png", "fanart": "fanart.jpg"}.get(image_type)
        github_path = f"images/{tmdb_id}/{filename}"
        api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/{github_path}"
        delete_payload = {"message": f"refactor: Delete {github_path}", "sha": sha, "branch": branch}
        self._execute_github_write_request("DELETE", api_url, pat, delete_payload)
        ui_logger.info(f"  - âœ… å·²æˆåŠŸä» GitHub åˆ é™¤æ–‡ä»¶: {github_path}", task_category=task_cat)

        # 2. æ›´æ–°ç´¢å¼•
        index = self._get_repo_index(repo_config.model_dump())
        if str(tmdb_id) in index['images'] and image_type in index['images'][str(tmdb_id)]:
            del index['images'][str(tmdb_id)][image_type]
            if not index['images'][str(tmdb_id)]:
                del index['images'][str(tmdb_id)]
        
        index['last_updated'] = datetime.now().isoformat()
        index_api_url = f"https://api.github.com/repos/{owner}/{repo_name}/contents/database.json"
        get_index_resp = self.session.get(index_api_url, headers={"Authorization": f"token {pat}"}, proxies=self.proxy_manager.get_proxies(index_api_url)).json()
        index_sha = get_index_resp.get('sha')
        index_payload = {
            "message": "chore: Update index after deletion",
            "content": base64.b64encode(json.dumps(index, indent=2).encode()).decode(),
            "branch": branch,
            "sha": index_sha
        }
        self._execute_github_write_request("PUT", index_api_url, pat, index_payload)
        ui_logger.info(f"  - âœ… ç´¢å¼•æ–‡ä»¶ database.json æ›´æ–°æˆåŠŸã€‚", task_category=task_cat)

        # 3. æ›´æ–°ä»“åº“å¤§å°
        latest_size_kb = self._get_latest_repo_size(repo_url, pat)
        current_config = app_config_module.load_app_config()
        for r in current_config.poster_manager_config.github_repos:
            if r.repo_url == repo_url:
                r.state.size_kb = latest_size_kb
                r.state.last_checked = datetime.now().isoformat()
                break
        app_config_module.save_app_config(current_config)
        ui_logger.info(f"  - âœ… ä»“åº“ {repo_url} æœ€æ–°å®¹é‡ {latest_size_kb} KB å·²å›å†™é…ç½®ã€‚", task_category=task_cat)
        
        # 4. æ¸…ç†èšåˆç¼“å­˜
        if os.path.exists(AGGREGATED_INDEX_CACHE_FILE):
            os.remove(AGGREGATED_INDEX_CACHE_FILE)
            ui_logger.info(f"  - âœ… æœ¬åœ°èšåˆç¼“å­˜å·²æ¸…ç†ã€‚", task_category=task_cat)

        ui_logger.info(f"ğŸ‰ å•ä½“åˆ é™¤ä»»åŠ¡å®Œæˆã€‚", task_category=task_cat)


    def start_backup_task(
        self,
        scope: ScheduledTasksTargetScope,
        content_types: List[str],
        overwrite: bool,
        cancellation_event: threading.Event,
        task_id: str,
        task_manager: TaskManager
    ):
        """ä»æœ¬åœ°ç¼“å­˜å¤‡ä»½åˆ° GitHub çš„ä¸»ä»»åŠ¡æµç¨‹"""
        task_cat = f"æµ·æŠ¥å¤‡ä»½({scope.mode})"
        ui_logger.info(f"ğŸ‰ ä»»åŠ¡å¯åŠ¨ï¼Œæ¨¡å¼: {scope.mode}, å†…å®¹: {content_types}, è¦†ç›–: {overwrite}", task_category=task_cat)

        try:
            # é˜¶æ®µä¸€ï¼šé¢„å¤„ç†
            selector = MediaSelector(self.config)
            media_ids = selector.get_item_ids(scope)
            task_manager.update_task_progress(task_id, 10, 100)
            if cancellation_event.is_set(): return

            initial_list = self._scan_local_cache(media_ids, content_types, task_cat)
            task_manager.update_task_progress(task_id, 25, 100)
            if cancellation_event.is_set(): return

            remote_map = self._get_aggregated_remote_index(task_cat)
            task_manager.update_task_progress(task_id, 40, 100)
            if cancellation_event.is_set(): return

            # é˜¶æ®µäºŒï¼šåˆ†ç±»
            new_files, overwrite_files = self._classify_pending_files(initial_list, remote_map, task_cat)
            task_manager.update_task_progress(task_id, 50, 100)
            if not new_files and not overwrite_files:
                ui_logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å‡å·²æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œæ— éœ€å¤‡ä»½ã€‚", task_category=task_cat)
                task_manager.update_task_progress(task_id, 100, 100)
                return

            # é˜¶æ®µä¸‰ï¼šåˆ†å‘è®¡åˆ’
            dispatch_plan = self._calculate_dispatch_plan(new_files, overwrite_files, task_cat)
            task_manager.update_task_progress(task_id, 60, 100)
            if cancellation_event.is_set(): return

            # é˜¶æ®µå››ï¼šæ‰§è¡Œ
            self._execute_dispatch_plan(dispatch_plan, task_cat, cancellation_event)
            task_manager.update_task_progress(task_id, 100, 100)
            
            ui_logger.info("ğŸ‰ å¤‡ä»½ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚", task_category=task_cat)

        except Exception as e:
            ui_logger.error(f"âŒ å¤‡ä»½ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", task_category=task_cat, exc_info=True)
            raise e