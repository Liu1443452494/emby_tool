# backend/file_scraper_logic.py (å®Œæ•´æ–‡ä»¶è¦†ç›–)

import os
import json
import time
import re
import shutil
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Callable
from filelock import FileLock, Timeout
import cloudscraper
from bs4 import BeautifulSoup
from PIL import Image
import io

from models import AppConfig, FileScraperConfig
from task_manager import TaskManager
from log_manager import ui_logger
from proxy_manager import ProxyManager

CACHE_FILE_PATH = os.path.join('/app/data', 'file_scraper_cache.json')
CACHE_EXPIRATION_DAYS = 7

class FileScraperLogic:
    def __init__(self, config: AppConfig):
        self.config = config
        self.scraper_config = config.file_scraper_config
        self.proxy_manager = ProxyManager(config)
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šç§»é™¤ FlareSolverr é€»è¾‘ï¼Œç»Ÿä¸€åˆ›å»ºä¸€ä¸ª scraper å®ä¾‹ ---
        self.scraper = cloudscraper.create_scraper()
        ui_logger.info(f"âœ… æ–‡ä»¶åˆ®å‰Šå™¨å·²åˆå§‹åŒ–ï¼Œå½“å‰ä½¿ç”¨å†…ç½® Cloudflare è§£æå™¨ã€‚", task_category="æ–‡ä»¶åˆ®å‰Šå™¨-åˆå§‹åŒ–")

    def _make_request(self, url, **kwargs):
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šç§»é™¤å›é€€é€»è¾‘ï¼Œç›´æ¥ä½¿ç”¨å”¯ä¸€çš„ scraper å®ä¾‹ ---
        return self.scraper.get(url, **kwargs)

    def _read_cache(self) -> Dict:
        """å®‰å…¨åœ°è¯»å–ç¼“å­˜æ–‡ä»¶"""
        if not os.path.exists(CACHE_FILE_PATH):
            return {}
        try:
            with open(CACHE_FILE_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (IOError, json.JSONDecodeError) as e:
            ui_logger.error(f"âŒ è¯»å–åˆ®å‰Šå™¨ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}", task_category="æ–‡ä»¶åˆ®å‰Šå™¨")
            return {}

    def _write_cache(self, cache_data: Dict):
        """å®‰å…¨åœ°å†™å…¥ç¼“å­˜æ–‡ä»¶ï¼Œå¸¦æ–‡ä»¶é”"""
        task_cat = "æ–‡ä»¶åˆ®å‰Šå™¨-ç¼“å­˜"
        lock_path = CACHE_FILE_PATH + ".lock"
        try:
            with FileLock(lock_path, timeout=10):
                with open(CACHE_FILE_PATH, 'w', encoding='utf-8') as f:
                    json.dump(cache_data, f, ensure_ascii=False, indent=4)
        except Timeout:
            ui_logger.error("âŒ å†™å…¥ç¼“å­˜æ–‡ä»¶å¤±è´¥ï¼šè·å–æ–‡ä»¶é”è¶…æ—¶ï¼Œå¦ä¸€è¿›ç¨‹å¯èƒ½æ­£åœ¨æ“ä½œã€‚", task_category=task_cat)
        except Exception as e:
            ui_logger.error(f"âŒ å†™å…¥ç¼“å­˜æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", task_category=task_cat)


        # backend/file_scraper_logic.py (å‡½æ•°æ›¿æ¢)
    def _check_metadata_exists(self, path: str, log_details: bool = False) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šè·¯å¾„çš„å…ƒæ•°æ®æ˜¯å¦å­˜åœ¨ï¼Œå¹¶æ ¹æ® log_details å‚æ•°è¾“å‡ºæ€»ç»“æ€§è¯Šæ–­æ—¥å¿—ã€‚
        """
        task_cat = "æ–‡ä»¶åˆ®å‰Šå™¨-æ‰«æ"
        
        def log(message):
            if log_details:
                ui_logger.info(message, task_category=task_cat)

        if os.path.isdir(path):
            poster_path = os.path.join(path, 'poster.jpg')
            nfo_path = os.path.join(path, 'movie.nfo')
            poster_exists = os.path.exists(poster_path)
            nfo_exists = os.path.exists(nfo_path)
            is_complete = poster_exists and nfo_exists
            
            if log_details:
                log(f"  - [æ£€æµ‹] è·¯å¾„ '{os.path.basename(path)}' (æ–‡ä»¶å¤¹)")
                # --- æ ¸å¿ƒä¿®æ”¹ ---
                log(f"    - é€šç”¨æµ·æŠ¥: 'poster.jpg' ({'å­˜åœ¨' if poster_exists else 'ä¸å­˜åœ¨'})")
                log(f"    - é€šç”¨NFO:   'movie.nfo' ({'å­˜åœ¨' if nfo_exists else 'ä¸å­˜åœ¨'})")
                # --- ä¿®æ”¹ç»“æŸ ---
                final_status_icon = "âœ…" if is_complete else ("âš ï¸" if (poster_exists or nfo_exists) else "âŒ")
                final_status_text = "å®Œæ•´" if is_complete else ("ä¸å®Œæ•´" if final_status_icon == "âš ï¸" else "ç¼ºå¤±")
                log(f"  - [ç»“è®º] {final_status_icon} å…ƒæ•°æ®åˆ¤å®šä¸º [{final_status_text}]ã€‚")
                
            return is_complete
        else: # è·¯å¾„æ˜¯æ–‡ä»¶
            # 1. é™é»˜è®¡ç®—æ‰€æœ‰å¯èƒ½çš„å…ƒæ•°æ®è·¯å¾„
            base, _ = os.path.splitext(path)
            parent_dir = os.path.dirname(path)
            
            poster_path_associated = f"{base}-poster.jpg"
            nfo_path_associated = f"{base}.nfo"
            
            poster_path_generic = os.path.join(parent_dir, 'poster.jpg')
            nfo_path_generic = os.path.join(parent_dir, 'movie.nfo')

            # 2. é™é»˜æ£€æŸ¥è¿™äº›è·¯å¾„æ˜¯å¦å­˜åœ¨
            poster_A_exists = os.path.exists(poster_path_associated)
            nfo_A_exists = os.path.exists(nfo_path_associated)
            
            poster_B_exists = os.path.exists(poster_path_generic)
            nfo_B_exists = os.path.exists(nfo_path_generic)
            
            is_independent = not self._is_bare_file(path)

            # 3. æ ¹æ®ä¼˜å…ˆçº§è®¡ç®—æœ€ç»ˆç»“æœ
            is_complete = False
            if poster_A_exists and nfo_A_exists:
                is_complete = True
            elif is_independent and poster_B_exists and nfo_B_exists:
                is_complete = True

            # 4. å¦‚æœéœ€è¦ï¼Œæ‰“å°æ€»ç»“æŠ¥å‘Š
            if log_details:
                log(f"  - [æ£€æµ‹] è·¯å¾„ '{os.path.basename(path)}' (æ–‡ä»¶)")
                # --- æ ¸å¿ƒä¿®æ”¹ ---
                log(f"    - å…³è”æµ·æŠ¥: '{os.path.basename(poster_path_associated)}' ({'å­˜åœ¨' if poster_A_exists else 'ä¸å­˜åœ¨'})")
                log(f"    - å…³è”NFO:   '{os.path.basename(nfo_path_associated)}' ({'å­˜åœ¨' if nfo_A_exists else 'ä¸å­˜åœ¨'})")
                
                if is_independent:
                    log(f"    - (ç‹¬ç«‹ç¯å¢ƒ) é€šç”¨æµ·æŠ¥: 'poster.jpg' ({'å­˜åœ¨' if poster_B_exists else 'ä¸å­˜åœ¨'})")
                    log(f"    - (ç‹¬ç«‹ç¯å¢ƒ) é€šç”¨NFO:   'movie.nfo' ({'å­˜åœ¨' if nfo_B_exists else 'ä¸å­˜åœ¨'})")
                # --- ä¿®æ”¹ç»“æŸ ---
                
                found_any_file = poster_A_exists or nfo_A_exists or (is_independent and (poster_B_exists or nfo_B_exists))
                final_status_icon = "âœ…" if is_complete else ("âš ï¸" if found_any_file else "âŒ")
                final_status_text = "å®Œæ•´" if is_complete else ("ä¸å®Œæ•´" if final_status_icon == "âš ï¸" else "ç¼ºå¤±")
                
                log(f"  - [ç»“è®º] {final_status_icon} å…ƒæ•°æ®åˆ¤å®šä¸º [{final_status_text}]ã€‚")

            return is_complete

    def scan_directory_task(self, cancellation_event: threading.Event, task_id: str, task_manager: TaskManager) -> List[Dict]:
        """
        æ‰«æç›®å½•ã€æ¸…ç†ç¼“å­˜å¹¶è¿”å›æ–‡ä»¶åˆ—è¡¨çš„åå°ä»»åŠ¡ã€‚
        """
        task_cat = "æ–‡ä»¶åˆ®å‰Šå™¨-æ‰«æ"
        scan_dir = self.scraper_config.scan_directory
        extensions = [ext.lower() for ext in self.scraper_config.file_extensions]
        ui_logger.info(f"â¡ï¸ å¼€å§‹æ‰«æä»»åŠ¡ï¼Œç›®æ ‡ç›®å½•: {scan_dir}", task_category=task_cat)

        # 1. è¯»å–å¹¶é¢„å¤„ç†ç¼“å­˜
        full_cache = self._read_cache()
        cleaned_cache = {}
        now = datetime.utcnow()
        expiration_delta = timedelta(days=CACHE_EXPIRATION_DAYS)

        ui_logger.info(f"ğŸ” æ­£åœ¨è¿›è¡Œå…¨å±€ç¼“å­˜æ¸…ç†ï¼Œå°†ç§»é™¤è¶…è¿‡ {CACHE_EXPIRATION_DAYS} å¤©æœªè®¿é—®çš„ç›®å½•æ•°æ®...", task_category=task_cat)
        for dir_path, dir_data in full_cache.items():
            last_accessed_str = dir_data.get("last_accessed")
            if last_accessed_str:
                try:
                    last_accessed_dt = datetime.fromisoformat(last_accessed_str.replace('Z', '+00:00'))
                    if now - last_accessed_dt.replace(tzinfo=None) <= expiration_delta:
                        cleaned_cache[dir_path] = dir_data
                    else:
                        ui_logger.info(f"  - ğŸ—‘ï¸ ç›®å½• '{dir_path}' çš„ç¼“å­˜æ•°æ®å·²è¿‡æœŸ (ä¸Šæ¬¡è®¿é—®äº {last_accessed_str})ï¼Œå·²è¢«æ¸…ç†ã€‚", task_category=task_cat)
                except ValueError:
                     ui_logger.warning(f"  - âš ï¸ æ— æ³•è§£æç›®å½• '{dir_path}' çš„æ—¶é—´æˆ³ï¼Œå°†ä¿ç•™è¯¥æ•°æ®ã€‚", task_category=task_cat)
            else:
                cleaned_cache[dir_path] = dir_data # ä¿ç•™æ²¡æœ‰æ—¶é—´æˆ³çš„æ—§æ•°æ®

        # 2. å®šä½å½“å‰ç›®å½•æ•°æ®
        if scan_dir not in cleaned_cache:
            ui_logger.info(f"â„¹ï¸ ä¸ºæ–°ç›®å½• '{scan_dir}' åˆ›å»ºç¼“å­˜è®°å½•ã€‚", task_category=task_cat)
            cleaned_cache[scan_dir] = {"last_accessed": now.isoformat() + 'Z', "files": {}}
        else:
            ui_logger.info(f"âœ… æ‰¾åˆ°ç›®å½• '{scan_dir}' çš„ç°æœ‰ç¼“å­˜ï¼Œå°†æ›´æ–°å…¶è®¿é—®æ—¶é—´ã€‚", task_category=task_cat)
            cleaned_cache[scan_dir]["last_accessed"] = now.isoformat() + 'Z'
        
        current_dir_cache = cleaned_cache[scan_dir]["files"]

        # 3. éå†æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
        ui_logger.info(f"ğŸš€ å¼€å§‹æ·±åº¦éå†æ–‡ä»¶ç³»ç»Ÿï¼Œè¯·ç¨å€™...", task_category=task_cat)
        current_disk_files = set()
        try:
# --- æ–°å¢/ä¿®æ”¹ ---
            for root, dirs, files in os.walk(scan_dir):
                if cancellation_event.is_set():
                    ui_logger.warning("âš ï¸ æ‰«æä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆã€‚", task_category=task_cat)
                    return []
                
                # åªå¤„ç†æ–‡ä»¶ï¼Œä¸å†å•ç‹¬æ·»åŠ æ–‡ä»¶å¤¹
                for file_name in files:
                    if os.path.splitext(file_name)[1].lower() in extensions:
                        file_path = os.path.join(root, file_name)
                        current_disk_files.add(file_path)
# --- æ–°å¢/ä¿®æ”¹ç»“æŸ ---
        except Exception as e:
            ui_logger.error(f"âŒ éå†ç›®å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}", task_category=task_cat)
            return []

        ui_logger.info(f"âœ… æ–‡ä»¶ç³»ç»Ÿéå†å®Œæˆï¼Œå…±å‘ç° {len(current_disk_files)} ä¸ªæœ‰æ•ˆé¡¹ç›®ã€‚", task_category=task_cat)

        # 4. æ¸…ç†ç¼“å­˜ä¸­å·²ä¸å­˜åœ¨çš„æ–‡ä»¶æ¡ç›®
        cached_paths = set(current_dir_cache.keys())
        paths_to_remove = cached_paths - current_disk_files
        if paths_to_remove:
            ui_logger.info(f"ğŸ”„ æ­£åœ¨æ¸…ç†ç¼“å­˜ä¸­ {len(paths_to_remove)} ä¸ªå·²å¤±æ•ˆçš„æ–‡ä»¶è®°å½•...", task_category=task_cat)
            for path in paths_to_remove:
                del current_dir_cache[path]

        # 5. æ›´æ–°å…ƒæ•°æ®çŠ¶æ€å¹¶æ„å»ºè¿”å›åˆ—è¡¨
        final_file_list = []
        total_files = len(current_disk_files)
        task_manager.update_task_progress(task_id, 0, total_files)
        
        ui_logger.info("ğŸ” æ­£åœ¨æ£€æŸ¥æ¯ä¸ªé¡¹ç›®çš„å…ƒæ•°æ®çŠ¶æ€...", task_category=task_cat)
        for i, path in enumerate(sorted(list(current_disk_files))):
            if cancellation_event.is_set():
                ui_logger.warning("âš ï¸ æ‰«æä»»åŠ¡åœ¨å…ƒæ•°æ®æ£€æŸ¥é˜¶æ®µè¢«ç”¨æˆ·å–æ¶ˆã€‚", task_category=task_cat)
                return []

            # --- æ–°å¢/ä¿®æ”¹ï¼šè§£è€¦å®Œæ•´æ€§åˆ¤æ–­å’Œè·¯å¾„è·å– ---
            # a. åˆ¤æ–­å®Œæ•´æ€§ï¼Œç”¨äºUIæ˜¾ç¤º Tag
            has_metadata = self._check_metadata_exists(path, log_details=True)
            
            # b. æ— è®ºå…ƒæ•°æ®æ˜¯å¦å®Œæ•´ï¼Œéƒ½å°è¯•è·å–å•ä¸ªæ–‡ä»¶çš„è·¯å¾„
            nfo_path, _ = self.get_media_file(path, 'nfo')
            poster_path, _ = self.get_media_file(path, 'poster')
            # --- æ–°å¢/ä¿®æ”¹ç»“æŸ ---
            
            if path not in current_dir_cache:
                current_dir_cache[path] = {}
            
            current_dir_cache[path]["has_metadata"] = has_metadata
            
            file_info = {
                "path": path,
                "type": 'dir' if os.path.isdir(path) else 'file',
                "urls": current_dir_cache[path].get("urls", {}),
                "has_metadata": has_metadata,
                "last_scraped": current_dir_cache[path].get("last_scraped_timestamp"),
                "nfo_path": nfo_path,
                "poster_path": poster_path
            }
            final_file_list.append(file_info)
            
            if (i + 1) % 100 == 0:
                task_manager.update_task_progress(task_id, i + 1, total_files)

        task_manager.update_task_progress(task_id, total_files, total_files)
        
        # 6. å†™å›ç¼“å­˜
        self._write_cache(cleaned_cache)
        ui_logger.info("ğŸ‰ æ‰«æä»»åŠ¡å®Œæˆï¼ç¼“å­˜å·²æ›´æ–°ã€‚", task_category=task_cat)
        
        return final_file_list

    # --- åˆ®å‰Šé€»è¾‘ ---

    def _parse_xchina_html(self, html_content: str, task_cat: str) -> Optional[Dict]:
        """ä½¿ç”¨ BeautifulSoup è§£æ xchina.co çš„HTMLé¡µé¢ï¼Œæå–æ‰€éœ€ä¿¡æ¯ã€‚"""
        try:
            soup = BeautifulSoup(html_content, 'lxml')
            data = {
                'title': None, 'actors': [], 'tags': [], 'plot': None,
                'poster_url': None, 'fanart_url': None
            }

            title_tag = soup.find('h1', class_='hero-title-item')
            if title_tag: data['title'] = title_tag.get_text(strip=True)

            actor_tag = soup.find('div', class_='model-item')
            if actor_tag:
                actor_name = actor_tag.find('div').get_text(strip=True) if actor_tag.find('div') else None
                actor_thumb_url = None
                style_attr = actor_tag.get('style', '')
                match = re.search(r"url\('(.+?)'\)", style_attr)
                if match: actor_thumb_url = match.group(1)
                if actor_name: data['actors'].append({'name': actor_name, 'thumb': actor_thumb_url})

            tags = []
            info_card = soup.find('div', class_='info-card video-detail')
            if info_card:
                camera_icon_item = info_card.find('i', class_='fa-video-camera')
                if camera_icon_item and (text_div := camera_icon_item.find_next('div', class_='text')):
                    for a_tag in text_div.find_all('a'): tags.append(a_tag.get_text(strip=True))
                
                file_icon_item = info_card.find('i', class_='fa-file')
                if file_icon_item and (text_div := file_icon_item.find_next('div', class_='text')):
                    tags.append(text_div.get_text(strip=True))
            data['tags'] = tags

            screenshot_container = soup.find('div', class_='screenshot-container')
            if screenshot_container and (img_tags := screenshot_container.find_all('img')):
                if len(img_tags) > 0:
                    data['poster_url'] = img_tags[0].get('src')
                    data['plot'] = img_tags[0].get('alt')
                if len(img_tags) > 1:
                    data['fanart_url'] = img_tags[1].get('src')

            ui_logger.info("  - âœ… [xchina.co] HTMLå†…å®¹è§£ææˆåŠŸã€‚", task_category=task_cat)
            return data
        except Exception as e:
            ui_logger.error(f"  - âŒ [xchina.co] è§£æHTMLæ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}", task_category=task_cat)
            return None
        
    def _parse_javday_html(self, html_content: str, task_cat: str) -> Optional[Dict]:
        """ä½¿ç”¨ BeautifulSoup å’Œæ­£åˆ™è¡¨è¾¾å¼è§£æ javday.app çš„HTMLé¡µé¢ã€‚"""
        try:
            soup = BeautifulSoup(html_content, 'lxml')
            data = {
                'title': None, 'actors': [], 'tags': [], 'plot': None,
                'poster_url': None, 'fanart_url': None
            }

            # 1. æå–æ ‡é¢˜
            title_tag = soup.find('h1', class_='video-title')
            if title_tag:
                data['title'] = title_tag.get_text(strip=True)

            # 2. æå–æ¼”å‘˜
            actor_tag = soup.find('span', class_='vod_actor')
            if actor_tag and actor_tag.find('a'):
                actor_name = actor_tag.find('a').get_text(strip=True)
                if actor_name:
                    data['actors'].append({'name': actor_name, 'thumb': None})

            # 3. æå–æ ‡ç­¾ (ç•ªå·å’Œå‚å•†)
            tags = []
            jpnum_tag = soup.find('span', class_='jpnum')
            if jpnum_tag:
                tags.append(jpnum_tag.get_text(strip=True))
            
            producer_tag = soup.find('span', class_='producer')
            if producer_tag and producer_tag.find('a'):
                tags.append(producer_tag.find('a').get_text(strip=True))
            
            data['tags'] = tags

            # 4. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æµ·æŠ¥URL
            match = re.search(r"pic:\s*'([^']*)'", html_content)
            if match:
                relative_path = match.group(1)
                if relative_path.startswith('/'):
                    data['poster_url'] = f"https://javday.app{relative_path}"
                else:
                    data['poster_url'] = f"https://javday.app/{relative_path}"
            
            ui_logger.info("  - âœ… [javday.app] HTMLå†…å®¹è§£ææˆåŠŸã€‚", task_category=task_cat)
            return data
        except Exception as e:
            ui_logger.error(f"  - âŒ [javday.app] è§£æHTMLæ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}", task_category=task_cat)
            return None

    def _parse_madou_club_html(self, html_content: str, task_cat: str) -> Optional[Dict]:
        """ä½¿ç”¨ BeautifulSoup å’Œæ­£åˆ™è¡¨è¾¾å¼è§£æ madou.club çš„HTMLé¡µé¢ã€‚"""
        try:
            soup = BeautifulSoup(html_content, 'lxml')
            data = {
                'title': None, 'actors': [], 'tags': [], 'plot': None,
                'poster_url': None, 'fanart_url': None
            }

            # 1. æå–æ ‡é¢˜
            title_tag = soup.find('h1', class_='article-title')
            if title_tag:
                data['title'] = title_tag.get_text(strip=True)

            # 2. æå–æ ‡ç­¾
            tags = set()
            # ä» meta keywords ä¸­æå–
            keywords_meta = soup.find('meta', attrs={'name': 'keywords'})
            if keywords_meta and keywords_meta.get('content'):
                keywords = [tag.strip() for tag in keywords_meta.get('content').split(',') if tag.strip()]
                tags.update(keywords)
            
            # ä» article-tags ä¸­æå–
            tags_div = soup.find('div', class_='article-tags')
            if tags_div:
                for a_tag in tags_div.find_all('a'):
                    tags.add(a_tag.get_text(strip=True))
            
            data['tags'] = list(tags)

            # 3. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æµ·æŠ¥URL
            match = re.search(r"shareimage\s*:\s*'([^']*)'", html_content)
            if match:
                data['poster_url'] = match.group(1)
            
            ui_logger.info("  - âœ… [madou.club] HTMLå†…å®¹è§£ææˆåŠŸã€‚", task_category=task_cat)
            return data
        except Exception as e:
            ui_logger.error(f"  - âŒ [madou.club] è§£æHTMLæ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}", task_category=task_cat)
            return None
    # --- æ–°å¢ç»“æŸ ---

    def _parse_madouqu_html(self, html_content: str, task_cat: str) -> Optional[Dict]:
        """ä½¿ç”¨ BeautifulSoup è§£æ madouqu.com çš„HTMLé¡µé¢ã€‚"""
        try:
            soup = BeautifulSoup(html_content, 'lxml')
            data = {
                'title': None, 'actors': [], 'tags': [], 'plot': None,
                'poster_url': None, 'fanart_url': None
            }

            # 1. æå–æ ‡é¢˜
            title_tag = soup.find('h1', class_='entry-title')
            if title_tag:
                data['title'] = title_tag.get_text(strip=True)

            # 2. æå–æ¼”å‘˜
            p_tags = soup.find_all('p')
            for p in p_tags:
                p_text = p.get_text(strip=True)
                if p_text.startswith('éº»è±†å¥³éƒï¼š'):
                    actor_name = p_text.replace('éº»è±†å¥³éƒï¼š', '').strip()
                    if actor_name:
                        data['actors'].append({'name': actor_name, 'thumb': None})
                    break
            
            # 3. æå–æ ‡ç­¾
            tags = set()
            # ä»ç•ªå·å’Œç‰‡åä¸­æå–
            for p in p_tags:
                p_text = p.get_text(strip=True)
                if p_text.startswith('æ‰£æ‰£ç•ªè™Ÿï¼š'):
                    tags.add(p_text.replace('æ‰£æ‰£ç•ªè™Ÿï¼š', '').strip())
                elif p_text.startswith('æ‰£æ‰£ç‰‡åï¼š'):
                    tags.add(p_text.replace('æ‰£æ‰£ç‰‡åï¼š', '').strip())
            
            # ä»é¢åŒ…å±‘å¯¼èˆªä¸­æå–åˆ†ç±»
            breadcrumbs = soup.find('div', class_='breadcrumbs')
            if breadcrumbs:
                links = breadcrumbs.find_all('a')
                # è·³è¿‡ç¬¬ä¸€ä¸ª("éº»è±†åŒº")
                if len(links) > 1:
                    for link in links[1:]:
                        tags.add(link.get_text(strip=True))

            data['tags'] = list(tags)

            # 4. æå–æµ·æŠ¥URL
            poster_meta = soup.find('meta', property='og:image')
            if poster_meta and poster_meta.get('content'):
                data['poster_url'] = poster_meta.get('content')
            
            ui_logger.info("  - âœ… [madouqu.com] HTMLå†…å®¹è§£ææˆåŠŸã€‚", task_category=task_cat)
            return data
        except Exception as e:
            ui_logger.error(f"  - âŒ [madouqu.com] è§£æHTMLæ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}", task_category=task_cat)
            return None
    # --- æ–°å¢ç»“æŸ ---

    def _get_scraper_for_domain(self, domain: str) -> Optional[Callable]:
        """æ ¹æ®åŸŸåè¿”å›å¯¹åº”çš„åˆ®å‰Šè§£æå‡½æ•°"""
        if 'xchina.co' in domain:
            return self._parse_xchina_html
        elif 'javday.app' in domain:
            return self._parse_javday_html
        elif 'madou.club' in domain:
            return self._parse_madou_club_html
        # --- æ–°å¢ ---
        elif 'madouqu.com' in domain:
            return self._parse_madouqu_html
        # --- æ–°å¢ç»“æŸ ---
        # ... æœªæ¥å¯åœ¨æ­¤å¤„æ·»åŠ å…¶ä»–ç½‘ç«™çš„ elif ...
        return None
    
    def _is_bare_file(self, file_path: str) -> bool:
        """
        åˆ¤æ–­ä¸€ä¸ªè§†é¢‘æ–‡ä»¶æ˜¯å¦ä¸ºâ€œè£¸æ–‡ä»¶â€ã€‚
        å¦‚æœåœ¨å…¶çˆ¶ç›®å½•ä¸­æ‰¾åˆ°é™¤è‡ªèº«å¤–çš„ä»»ä½•ä¸€ä¸ªå…¶ä»–è§†é¢‘æ–‡ä»¶ï¼Œå°±åˆ¤å®šä¸ºè£¸æ–‡ä»¶ã€‚
        """
        task_cat = "æ–‡ä»¶åˆ®å‰Šå™¨-å·¥å…·"
        video_extensions = [ext.lower() for ext in self.scraper_config.file_extensions]

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šå¢åŠ å¯¹è¾“å…¥æ–‡ä»¶ç±»å‹çš„æ£€æŸ¥ ---
        if os.path.splitext(file_path)[1].lower() not in video_extensions:
            # å¦‚æœè¾“å…¥è·¯å¾„æœ¬èº«ä¸æ˜¯è§†é¢‘æ–‡ä»¶ï¼Œåˆ™ä¸è¿›è¡Œè£¸æ–‡ä»¶åˆ¤æ–­ï¼Œç›´æ¥è¿”å›False
            return False
        # --- ä¿®æ”¹ç»“æŸ ---

        try:
            parent_dir = os.path.dirname(file_path)
            my_filename = os.path.basename(file_path)

            for entry in os.scandir(parent_dir):
                if entry.is_file() and entry.name != my_filename:
                    if os.path.splitext(entry.name)[1].lower() in video_extensions:
                        ui_logger.info(f"  - â„¹ï¸ [ç‹¬ç«‹æ€§æ£€æµ‹] æ£€æµ‹åˆ°åŒçº§ç›®å½•ä¸‹å­˜åœ¨å¦ä¸€ä¸ªè§†é¢‘æ–‡ä»¶: {entry.name}ã€‚åˆ¤å®š '{my_filename}' ä¸ºè£¸æ–‡ä»¶ã€‚", task_category=task_cat)
                        return True
            
            ui_logger.info(f"  - â„¹ï¸ [ç‹¬ç«‹æ€§æ£€æµ‹] æœªåœ¨åŒçº§ç›®å½•ä¸‹å‘ç°å…¶ä»–è§†é¢‘æ–‡ä»¶ã€‚åˆ¤å®š '{my_filename}' ä¸ºç‹¬ç«‹æ–‡ä»¶ã€‚", task_category=task_cat)
            return False
        except Exception as e:
            ui_logger.error(f"  - âŒ [ç‹¬ç«‹æ€§æ£€æµ‹] æ‰§è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}ã€‚å°†é»˜è®¤å…¶ä¸ºç‹¬ç«‹æ–‡ä»¶ä»¥ä¿è¯å®‰å…¨ã€‚", task_category=task_cat)
            return False

        # backend/file_scraper_logic.py (å‡½æ•°æ›¿æ¢)
    def scrape_url_task(self, file_path: str, urls: Dict[str, str], cancellation_event: threading.Event) -> Dict:
        """
        å•ä¸ªæ–‡ä»¶çš„åˆ®å‰Šã€èšåˆå’Œæ–‡ä»¶ä¿å­˜ä»»åŠ¡ã€‚
        """
        item_name = os.path.basename(file_path)
        task_cat = f"æ–‡ä»¶åˆ®å‰Šå™¨-{item_name}"
        ui_logger.info(f"â¡ï¸ å¼€å§‹ä¸º '{item_name}' æ‰§è¡Œåˆ®å‰Šä»»åŠ¡...", task_category=task_cat)

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šç§»é™¤åˆ®å‰Šæ¨¡å¼çš„æ—¥å¿—è¾“å‡º ---
        # --- ä¿®æ”¹ç»“æŸ ---

        # 1. å‰ç½®å†³ç­–åˆ¤æ–­
        if not urls:
            ui_logger.warning(f"âš ï¸ è·³è¿‡ '{item_name}'ï¼ŒåŸå› ï¼šæœªè®¾ç½®ä»»ä½•æœ‰æ•ˆçš„åˆ®å‰Šç½‘å€ã€‚", task_category=task_cat)
            return {"success": False, "message": "æœªè®¾ç½®URL"}

        if not self.scraper_config.overwrite_existing and self._check_metadata_exists(file_path):
            ui_logger.info(f"â„¹ï¸ è·³è¿‡ '{item_name}'ï¼ŒåŸå› ï¼šå…ƒæ•°æ®å·²å­˜åœ¨ä¸”æœªå¼€å¯è¦†ç›–æ¨¡å¼ã€‚", task_category=task_cat)
            return {"success": True, "message": "å·²è·³è¿‡"}

        # 2. å¤šæºèšåˆåˆ®å‰Š
        scraped_data = {}
        priority_list = self.scraper_config.source_priority
        
        ui_logger.info(f"ğŸ” æŒ‰ä¼˜å…ˆçº§é¡ºåºå¼€å§‹åˆ®å‰Šï¼Œé¡ºåº: {', '.join(priority_list)}", task_category=task_cat)
        for domain in priority_list:
            if cancellation_event.is_set(): return {"success": False, "message": "ä»»åŠ¡è¢«å–æ¶ˆ"}
            url = urls.get(domain)
            if not url:
                continue

            parser = self._get_scraper_for_domain(domain)

            
                
            if not parser:
                ui_logger.warning(f"  - âš ï¸ åŸŸå '{domain}' æ²¡æœ‰åŒ¹é…çš„è§£æå™¨ï¼Œå·²è·³è¿‡ã€‚", task_category=task_cat)
                continue

            ui_logger.info(f"  - ğŸ”„ æ­£åœ¨ä½¿ç”¨ '{domain}' åˆ®å‰Š: {url}", task_category=task_cat)
            try:
                proxies = self.proxy_manager.get_proxies(url)
                # --- æ ¸å¿ƒä¿®æ”¹ï¼šè°ƒç”¨æ–°çš„è¯·æ±‚æ–¹æ³• ---
                response = self._make_request(url, timeout=20, proxies=proxies)
                # --- ä¿®æ”¹ç»“æŸ ---
                response.raise_for_status()
                
                partial_data = parser(response.text, task_cat)
                if partial_data:
                    # --- æ ¸å¿ƒä¿®æ”¹ï¼šé‡æ„èšåˆé€»è¾‘ï¼Œä¿ç•™URLæ¥æº ---
                    # 1. èšåˆ tags (ä¿æŒä¸å˜)
                    new_tags = partial_data.get('tags')
                    if new_tags:
                        if 'tags' not in scraped_data: scraped_data['tags'] = []
                        existing_tags = set(scraped_data['tags'])
                        tags_to_add = set(new_tags)
                        updated_tags = sorted(list(existing_tags.union(tags_to_add)))
                        if len(updated_tags) > len(existing_tags):
                            ui_logger.info(f"  - [èšåˆ] ä¸º 'tags' å­—æ®µæ–°å¢äº† {len(updated_tags) - len(existing_tags)} ä¸ªæ ‡ç­¾ã€‚", task_category=task_cat)
                        scraped_data['tags'] = updated_tags

                    # 2. èšåˆå›¾ç‰‡URLï¼ŒæŒ‰æ¥æºå­˜å‚¨
                    for img_key in ['poster_url', 'fanart_url']:
                        # æ„é€ æŒ‰æ¥æºå­˜å‚¨çš„å­—å…¸é”®
                        storage_key = f"{img_key}s_by_source" 
                        if storage_key not in scraped_data:
                            scraped_data[storage_key] = {}
                        
                        new_url = partial_data.get(img_key)
                        if new_url and domain not in scraped_data[storage_key]:
                            scraped_data[storage_key][domain] = new_url
                            ui_logger.info(f"  - [èšåˆ] ä» '{domain}' æ”¶é›†åˆ° '{img_key}': {new_url}", task_category=task_cat)

                    # 3. å¯¹æ‰€æœ‰å…¶ä»–å­—æ®µï¼Œåº”ç”¨â€œå…ˆåˆ°å…ˆå¾—â€åŸåˆ™
                    for key, value in partial_data.items():
                        if key in ['tags', 'poster_url', 'fanart_url']:
                            continue
                        
                        if key not in scraped_data or not scraped_data[key]:
                            if isinstance(value, list) and not value: continue
                            if isinstance(value, str) and not value.strip(): continue
                            scraped_data[key] = value
                            ui_logger.info(f"  - [èšåˆ] å¡«å……äº†å­—æ®µ: '{key}'ã€‚", task_category=task_cat)
                    # --- ä¿®æ”¹ç»“æŸ ---
            except Exception as e:
                ui_logger.error(f"  - âŒ è®¿é—®æˆ–è§£æ '{url}' å¤±è´¥: {e}", task_category=task_cat)
        
        if not scraped_data.get('title'):
            ui_logger.error(f"âŒ åˆ®å‰Šå¤±è´¥ï¼åœ¨æ‰€æœ‰æºä¸­éƒ½æœªèƒ½è·å–åˆ°æœ€å…³é”®çš„'æ ‡é¢˜'ä¿¡æ¯ã€‚", task_category=task_cat)
            return {"success": False, "message": "æœªèƒ½è·å–åˆ°æ ‡é¢˜"}

        # 3. æ–‡ä»¶ä¿å­˜ä¸ç›®å½•ç»“æ„å†³ç­–
        ui_logger.info("ğŸ’¾ å¼€å§‹å¤„ç†æ–‡ä»¶ä¿å­˜ä¸ç›®å½•ç»“æ„...", task_category=task_cat)
        original_path = file_path
        new_path = original_path
        save_dir = ""
        
        is_bare = self._is_bare_file(original_path)
        
        if os.path.isdir(original_path):
            save_dir = original_path
            ui_logger.info(f"  - è·¯å¾„ç±»å‹ï¼šæ–‡ä»¶å¤¹ã€‚å…ƒæ•°æ®å°†ä¿å­˜åœ¨: {save_dir}", task_category=task_cat)
        elif is_bare:
            ui_logger.info(f"  - è·¯å¾„ç±»å‹ï¼šè£¸æ–‡ä»¶ã€‚å°†ä¸ºå…¶åˆ›å»ºç‹¬ç«‹æ–‡ä»¶å¤¹ã€‚", task_category=task_cat)
            base_filename, _ = os.path.splitext(os.path.basename(original_path))
            scraped_title = scraped_data.get('title', '').strip()
            safe_title = re.sub(r'[\\/*?:"<>|]', '_', scraped_title) if scraped_title else ''
            
            if safe_title:
                new_folder_name = safe_title
                ui_logger.info(f"  - å‘½åç­–ç•¥ï¼šå°†ä½¿ç”¨åˆ®å‰Šåˆ°çš„æ ‡é¢˜ '{safe_title}' ä½œä¸ºæ–°æ–‡ä»¶å¤¹åã€‚", task_category=task_cat)
            else:
                new_folder_name = base_filename
                ui_logger.warning(f"  - å‘½åç­–ç•¥ï¼šâš ï¸ æœªèƒ½è·å–åˆ°æœ‰æ•ˆæ ‡é¢˜ï¼Œå°†å›é€€ä½¿ç”¨åŸæ–‡ä»¶å '{base_filename}' ä½œä¸ºæ–‡ä»¶å¤¹åã€‚", task_category=task_cat)

            parent_dir = os.path.dirname(original_path)
            new_folder_path = os.path.join(parent_dir, new_folder_name)

            if os.path.exists(new_folder_path) and not os.path.isdir(new_folder_path):
                ui_logger.error(f"  - âŒ æ— æ³•åˆ›å»ºæ–‡ä»¶å¤¹ '{new_folder_path}'ï¼Œå› ä¸ºå·²å­˜åœ¨åŒåæ–‡ä»¶ã€‚", task_category=task_cat)
                return {"success": False, "message": "æ— æ³•åˆ›å»ºæ–‡ä»¶å¤¹ï¼Œå­˜åœ¨åŒåæ–‡ä»¶"}
            
            os.makedirs(new_folder_path, exist_ok=True)
            new_path = os.path.join(new_folder_path, os.path.basename(original_path))
            
            try:
                if original_path != new_path:
                    shutil.move(original_path, new_path)
                    ui_logger.info(f"  - âœ… è§†é¢‘æ–‡ä»¶å·²æˆåŠŸç§»åŠ¨åˆ°: {new_path}", task_category=task_cat)
                else:
                    ui_logger.info(f"  - â„¹ï¸ è§†é¢‘æ–‡ä»¶å·²åœ¨ç›®æ ‡ä½ç½®ï¼Œæ— éœ€ç§»åŠ¨ã€‚", task_category=task_cat)
            except Exception as e:
                ui_logger.error(f"  - âŒ ç§»åŠ¨è§†é¢‘æ–‡ä»¶å¤±è´¥: {e}", task_category=task_cat)
                return {"success": False, "message": f"ç§»åŠ¨æ–‡ä»¶å¤±è´¥: {e}"}

            if self.scraper_config.overwrite_existing:
                old_poster = f"{os.path.splitext(original_path)[0]}-poster.jpg"
                old_nfo = f"{os.path.splitext(original_path)[0]}.nfo"
                if os.path.exists(old_poster): os.remove(old_poster)
                if os.path.exists(old_nfo): os.remove(old_nfo)
                ui_logger.info("  - â„¹ï¸ å·²æ¸…ç†æ•£è½çš„æ—§å…ƒæ•°æ®æ–‡ä»¶ã€‚", task_category=task_cat)

            save_dir = new_folder_path
        else: # ç‹¬ç«‹æ–‡ä»¶
            ui_logger.info(f"  - è·¯å¾„ç±»å‹ï¼šç‹¬ç«‹æ–‡ä»¶ã€‚å…ƒæ•°æ®å°†ä¿å­˜åœ¨å…¶çˆ¶ç›®å½•ä¸­ã€‚", task_category=task_cat)
            save_dir = os.path.dirname(original_path)

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šç»Ÿä¸€ base_name_for_meta ---
        base_name_for_meta = "movie"
        # --- ä¿®æ”¹ç»“æŸ ---

        # 4. æ‰§è¡Œæ–‡ä»¶å†™å…¥
        try:
            overwrite = self.scraper_config.overwrite_existing
            final_nfo_path = None
            final_poster_path = None

            nfo_path = os.path.join(save_dir, f"{base_name_for_meta}.nfo")
            nfo_exists = os.path.exists(nfo_path)
            
            if overwrite or not nfo_exists:
                # --- æ ¸å¿ƒä¿®æ”¹ï¼šæŒ‰ä¼˜å…ˆçº§é¡ºåºå†™å…¥æ‰€æœ‰å›¾ç‰‡URL ---
                nfo_parts = ["<?xml version='1.0' encoding='utf-8' standalone='yes'?>", "<movie>"]
                
                # (title, plot, actors, tags çš„é€»è¾‘ä¿æŒä¸å˜)
                title = scraped_data.get('title')
                if title: nfo_parts.append(f"  <title>{title}</title>")
                plot = scraped_data.get('plot')
                if plot: nfo_parts.append(f"  <plot>{plot}</plot>")
                actors = scraped_data.get('actors')
                if actors:
                    for actor in actors:
                        actor_name = actor.get('name')
                        if actor_name:
                            actor_xml = f"  <actor>\n    <name>{actor_name}</name>\n"
                            if actor.get('thumb'): actor_xml += f"    <thumb>{actor.get('thumb')}</thumb>\n"
                            actor_xml += "  </actor>"
                            nfo_parts.append(actor_xml)
                tags = scraped_data.get('tags')
                if tags:
                    for tag in tags:
                        if tag: nfo_parts.append(f"  <tag>{tag}</tag>")

                # æŒ‰æºä¼˜å…ˆçº§é¡ºåºå†™å…¥å›¾ç‰‡URL
                poster_urls_by_source = scraped_data.get('poster_urls_by_source', {})
                fanart_urls_by_source = scraped_data.get('fanart_urls_by_source', {})

                for domain in self.scraper_config.source_priority:
                    if domain in poster_urls_by_source:
                        nfo_parts.append(f'  <thumb aspect="poster">{poster_urls_by_source[domain]}</thumb>')
                    if domain in fanart_urls_by_source:
                        nfo_parts.append(f'  <thumb aspect="fanart">{fanart_urls_by_source[domain]}</thumb>')

                nfo_parts.append("</movie>")
                nfo_content = "\n".join(nfo_parts)
                # --- ä¿®æ”¹ç»“æŸ ---

                with open(nfo_path, 'w', encoding='utf-8') as f:
                    f.write(nfo_content)
                log_action = "è¦†ç›–" if nfo_exists else "ä¿å­˜"
                ui_logger.info(f"    - âœ… NFO æ–‡ä»¶å·²{log_action}: {nfo_path}", task_category=task_cat)
            else:
                ui_logger.info(f"    - â„¹ï¸ NFO æ–‡ä»¶å·²å­˜åœ¨ä¸”æœªå¼€å¯è¦†ç›–ï¼Œè·³è¿‡å†™å…¥ã€‚", task_category=task_cat)
            final_nfo_path = nfo_path

                
            main_page_url = next((urls[domain] for domain in priority_list if domain in urls), None)

            # --- æ ¸å¿ƒä¿®æ”¹ï¼šæŒ‰ä¼˜å…ˆçº§å’Œå®¹é”™æœºåˆ¶ä¸‹è½½å›¾ç‰‡ ---
            image_map = {
                'poster': ('poster_urls_by_source', 'poster.jpg'),
                'fanart': ('fanart_urls_by_source', 'fanart.jpg')
            }

            for img_type, (source_key, file_name) in image_map.items():
                img_path = os.path.join(save_dir, file_name)
                img_exists = os.path.exists(img_path)

                if overwrite or not img_exists:
                    download_success = False
                    # éå†ä¼˜å…ˆçº§åˆ—è¡¨
                    for domain in self.scraper_config.source_priority:
                        urls_by_source = scraped_data.get(source_key, {})
                        img_url = urls_by_source.get(domain)
                        
                        if not img_url:
                            continue # å½“å‰ä¼˜å…ˆçº§çš„æºæ²¡æœ‰æä¾›æ­¤å›¾ç‰‡ï¼Œè·³åˆ°ä¸‹ä¸€ä¸ª

                        ui_logger.info(f"    - ğŸ”„ å°è¯•ä»æº '{domain}' ä¸‹è½½ {img_type} å›¾ç‰‡...", task_category=task_cat)
                        try:
                            proxies = self.proxy_manager.get_proxies(img_url)
                            headers = {'Referer': main_page_url} if main_page_url else {}
                            # --- æ ¸å¿ƒä¿®æ”¹ï¼šè°ƒç”¨æ–°çš„è¯·æ±‚æ–¹æ³• ---
                            img_response = self._make_request(img_url, timeout=30, proxies=proxies, stream=True, headers=headers)
                            # --- ä¿®æ”¹ç»“æŸ ---
                            img_response.raise_for_status()
                            
                            image_data = io.BytesIO(img_response.content)
                            with Image.open(image_data) as img:
                                if img.mode in ['RGBA', 'P']:
                                    img = img.convert('RGB')
                                img.save(img_path, 'JPEG', quality=95)

                            log_action = "è¦†ç›–" if img_exists else "ä¸‹è½½"
                            ui_logger.info(f"    - âœ… {img_type.capitalize()} å›¾ç‰‡å·²æˆåŠŸ{log_action}å¹¶è½¬ä¸ºJPG: {img_path}", task_category=task_cat)
                            if img_type == 'poster':
                                final_poster_path = img_path
                            
                            download_success = True
                            break # ä¸‹è½½æˆåŠŸï¼Œè·³å‡ºå¾ªç¯
                        
                        except Exception as e:
                            ui_logger.warning(f"    - âš ï¸ ä»æº '{domain}' ä¸‹è½½å¤±è´¥: {e}ã€‚å°†å°è¯•ä¸‹ä¸€ä¸ªæº...", task_category=task_cat)
                    
                    if not download_success:
                         ui_logger.error(f"    - âŒ å°è¯•äº†æ‰€æœ‰æºï¼Œä»æœªèƒ½æˆåŠŸä¸‹è½½ {img_type} å›¾ç‰‡ã€‚", task_category=task_cat)
                else:
                    ui_logger.info(f"    - â„¹ï¸ {img_type.capitalize()} å›¾ç‰‡å·²å­˜åœ¨ä¸”æœªå¼€å¯è¦†ç›–ï¼Œè·³è¿‡ä¸‹è½½ã€‚", task_category=task_cat)
                    if img_type == 'poster':
                        final_poster_path = img_path
                
        except Exception as e:
            ui_logger.error(f"  - âŒ å†™å…¥å…ƒæ•°æ®æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", task_category=task_cat)
            return {"success": False, "message": f"å†™å…¥æ–‡ä»¶å¤±è´¥: {e}"}

        # 5. æ›´æ–°ç¼“å­˜
        ui_logger.info("ğŸ”„ æ­£åœ¨æ›´æ–°ç¼“å­˜çŠ¶æ€...", task_category=task_cat)
        final_metadata_exists = self._check_metadata_exists(new_path)
        if not final_metadata_exists:
            ui_logger.warning(f"  - âš ï¸ åˆ®å‰Šä»»åŠ¡å·²æ‰§è¡Œï¼Œä½†æœ€ç»ˆæ£€æŸ¥å‘ç°å…ƒæ•°æ®ä¸å®Œæ•´ (å¯èƒ½éƒ¨åˆ†å›¾ç‰‡ä¸‹è½½å¤±è´¥)ï¼Œ'has_metadata' çŠ¶æ€å°†ä¸ä¼šæ›´æ–°ä¸º trueã€‚", task_category=task_cat)

        full_cache = self._read_cache()
        scan_dir = self.scraper_config.scan_directory
        if scan_dir in full_cache:
            target_path_in_cache = new_path if new_path in full_cache[scan_dir]["files"] else original_path
            
            if target_path_in_cache in full_cache[scan_dir]["files"]:
                if new_path == original_path:
                    if final_metadata_exists:
                        full_cache[scan_dir]["files"][original_path]["has_metadata"] = True
                    full_cache[scan_dir]["files"][original_path]["last_scraped_timestamp"] = datetime.utcnow().isoformat() + 'Z'
                else:
                    original_data = full_cache[scan_dir]["files"].pop(original_path, {})
                    if final_metadata_exists:
                        original_data["has_metadata"] = True
                    else:
                        original_data["has_metadata"] = False
                    original_data["last_scraped_timestamp"] = datetime.utcnow().isoformat() + 'Z'
                    full_cache[scan_dir]["files"][new_path] = original_data
                
                self._write_cache(full_cache)
                ui_logger.info("  - âœ… ç¼“å­˜æ›´æ–°æˆåŠŸã€‚", task_category=task_cat)
            else:
                ui_logger.warning(f"  - âš ï¸ æœªåœ¨ç¼“å­˜ä¸­æ‰¾åˆ°å¯¹åº”æ¡ç›® ('{original_path}' æˆ– '{new_path}')ï¼Œè·³è¿‡ç¼“å­˜æ›´æ–°ã€‚", task_category=task_cat)
        else:
            ui_logger.warning("  - âš ï¸ æœªåœ¨ç¼“å­˜ä¸­æ‰¾åˆ°å½“å‰æ‰«æç›®å½•ï¼Œè·³è¿‡ç¼“å­˜æ›´æ–°ã€‚", task_category=task_cat)

        return {
            "success": True, 
            "message": "åˆ®å‰ŠæˆåŠŸ", 
            "data": scraped_data, 
            "new_path": new_path if new_path != original_path else None,
            "final_metadata_status": final_metadata_exists,
            "nfo_file_path": final_nfo_path,
            "poster_file_path": final_poster_path
        }
    
    def batch_scrape_task(self, cancellation_event: threading.Event, task_id: str, task_manager: TaskManager):
        """
        æ‰¹é‡åˆ®å‰Šæ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶ã€‚
        """
        task_cat = "æ–‡ä»¶åˆ®å‰Šå™¨-æ‰¹é‡"
        ui_logger.info("â¡ï¸ å¼€å§‹æ‰¹é‡åˆ®å‰Šä»»åŠ¡...", task_category=task_cat)

        scan_dir = self.scraper_config.scan_directory
        full_cache = self._read_cache()

        if scan_dir not in full_cache:
            ui_logger.error("âŒ ä»»åŠ¡ä¸­æ­¢ï¼šåœ¨ç¼“å­˜ä¸­æœªæ‰¾åˆ°å½“å‰æ‰«æç›®å½•çš„æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œä¸€æ¬¡æ‰«æã€‚", task_category=task_cat)
            return

        files_to_process = []
        all_files_in_cache = full_cache[scan_dir].get("files", {})
        
        ui_logger.info("ğŸ” æ­£åœ¨ç­›é€‰éœ€è¦å¤„ç†çš„æ–‡ä»¶...", task_category=task_cat)
        for path, data in all_files_in_cache.items():
            # æ¡ä»¶1: å¿…é¡»æœ‰URL
            if not data.get("urls"):
                continue
            # æ¡ä»¶2: å¦‚æœä¸è¦†ç›–ï¼Œåˆ™å…ƒæ•°æ®å¿…é¡»ä¸å­˜åœ¨
            if not self.scraper_config.overwrite_existing and data.get("has_metadata"):
                continue
            
            files_to_process.append({"path": path, "urls": data["urls"]})
        
        if not files_to_process:
            ui_logger.info("âœ… ä»»åŠ¡å®Œæˆï¼šæ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶ã€‚", task_category=task_cat)
            return

        total_items = len(files_to_process)
        ui_logger.info(f"å…±æ‰¾åˆ° {total_items} ä¸ªæ–‡ä»¶å¾…å¤„ç†ã€‚", task_category=task_cat)
        task_manager.update_task_progress(task_id, 0, total_items)
        
        cooldown = self.scraper_config.batch_cooldown
        success_count = 0
        skipped_count = 0
        failed_count = 0

        for i, item in enumerate(files_to_process):
            if cancellation_event.is_set():
                ui_logger.warning("âš ï¸ æ‰¹é‡ä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆã€‚", task_category=task_cat)
                break
            
            ui_logger.info(f"--- ( {i+1} / {total_items} ) ---", task_category=task_cat)
            result = self.scrape_url_task(item["path"], item["urls"], cancellation_event)
            
            if result["success"]:
                if result["message"] == "å·²è·³è¿‡":
                    skipped_count += 1
                else:
                    success_count += 1
            else:
                failed_count += 1

            task_manager.update_task_progress(task_id, i + 1, total_items)

            if i < total_items - 1:
                ui_logger.info(f"â±ï¸ ç­‰å¾…å†·å´æ—¶é—´: {cooldown} ç§’...", task_category=task_cat)
                time.sleep(cooldown)

        ui_logger.info("ğŸ‰ æ‰¹é‡åˆ®å‰Šä»»åŠ¡å…¨éƒ¨æ‰§è¡Œå®Œæ¯•ï¼", task_category=task_cat)
        ui_logger.info(f"  - æˆåŠŸ: {success_count} é¡¹", task_category=task_cat)
        ui_logger.info(f"  - å¤±è´¥: {failed_count} é¡¹", task_category=task_cat)
        ui_logger.info(f"  - è·³è¿‡: {skipped_count} é¡¹", task_category=task_cat)
    
        # backend/file_scraper_logic.py (å‡½æ•°æ›¿æ¢)
    def get_media_file(self, path: str, file_type: str) -> Tuple[Optional[str], Optional[str]]:
        """
        è·å–æŒ‡å®šè·¯å¾„å…³è”çš„åª’ä½“æ–‡ä»¶ï¼ˆå›¾ç‰‡æˆ–NFOï¼‰ï¼Œæ™ºèƒ½åˆ¤æ–­å¤šç§å­˜æ”¾æ–¹å¼ã€‚
        è¿”å› (æ–‡ä»¶è·¯å¾„, MIMEç±»å‹) æˆ– (None, None)ã€‚
        """
        task_cat = "æ–‡ä»¶åˆ®å‰Šå™¨-æ–‡ä»¶æœåŠ¡"
        
        try:
            if not os.path.exists(path):
                ui_logger.warning(f"  - âš ï¸ è¯·æ±‚çš„æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„ä¸å­˜åœ¨: {path}", task_category=task_cat)
                return None, None

            target_path = None
            
            if os.path.isdir(path):
                if file_type == 'poster': target_path = os.path.join(path, 'poster.jpg')
                elif file_type == 'nfo': target_path = os.path.join(path, 'movie.nfo')
            else: # è·¯å¾„æ˜¯æ–‡ä»¶
                # æ£€æŸ¥ç‚¹ A: ä¼˜å…ˆæŸ¥æ‰¾å¼ºå…³è”å…ƒæ•°æ®
                base, _ = os.path.splitext(path)
                if file_type == 'poster': temp_path = f"{base}-poster.jpg"
                elif file_type == 'nfo': temp_path = f"{base}.nfo"
                else: temp_path = None
                
                if temp_path and os.path.exists(temp_path):
                    target_path = temp_path
                
                # æ£€æŸ¥ç‚¹ B: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå†åœ¨ç‹¬ç«‹ç¯å¢ƒä¸‹æŸ¥æ‰¾é€šç”¨å…ƒæ•°æ®
                if not target_path and not self._is_bare_file(path):
                    parent_dir = os.path.dirname(path)
                    if file_type == 'poster': temp_path = os.path.join(parent_dir, 'poster.jpg')
                    elif file_type == 'nfo': temp_path = os.path.join(parent_dir, 'movie.nfo')
                    
                    if os.path.exists(temp_path):
                        target_path = temp_path

            if target_path:
                mime_type = 'image/jpeg' if file_type == 'poster' else 'text/plain'
                return target_path, mime_type
            else:
                return None, None
                
        except Exception as e:
            ui_logger.error(f"  - âŒ åœ¨æŸ¥æ‰¾åª’ä½“æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", task_category=task_cat)
            return None, None