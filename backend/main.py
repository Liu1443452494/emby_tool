import sys
import os
import requests
import httpx
import logging
import asyncio
import threading
import json
import time
import re
import shutil
from contextlib import asynccontextmanager
from datetime import datetime
from typing import List, Dict, Optional, Literal, Tuple, Any

from fastapi import FastAPI, HTTPException, BackgroundTasks, WebSocket, WebSocketDisconnect, Query, Path
from fastapi.responses import Response
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

from task_manager import TaskManager, task_manager
from models import EpisodeRenamerConfig
from episode_renamer_router import router as episode_renamer_router
from poster_manager_router import router as poster_manager_router
from actor_role_mapper_router import router as actor_role_mapper_router
from episode_role_sync_router import router as episode_role_sync_router
from actor_avatar_mapper_router import router as actor_avatar_mapper_router
from chasing_center_router import router as chasing_center_router
from upcoming_router import router as upcoming_router
from file_scraper_router import router as file_scraper_router
from media_tagger_router import router as media_tagger_router

from media_selector import MediaSelector
from models import ScheduledTasksConfig, ScheduledTasksTargetScope
from douban_poster_updater_logic import DoubanPosterUpdaterLogic
from models import DoubanPosterUpdaterConfig, EpisodeRefresherConfig
from episode_refresher_logic import EpisodeRefresherLogic
from local_extractor import extract_local_media_task
from models import LocalExtractRequest
from models import (
    ServerConfig, DownloadConfig, AppConfig, MediaSearchQuery, 
    DownloadRequest, BatchDownloadRequest, DoubanConfig, DoubanCacheStatus,
    ActorLocalizerConfig, ActorLocalizerPreviewRequest, ActorLocalizerApplyRequest,
    SuggestRolesRequest, UpdateRolesRequest,
    TencentApiConfig, SiliconflowApiConfig,
    TmdbConfig, ProxyConfig,
    DoubanFixerConfig,
    EmbyWebhookPayload,
    PreciseScreenshotUpdateRequest,
    WebhookConfig
)
import config as app_config
from emby_downloader import EmbyDownloader, batch_download_task
from log_manager import setup_logging, broadcaster as log_broadcaster, ui_logger
from genre_logic import GenreLogic
from douban_manager import scan_douban_directory_task, DOUBAN_CACHE_FILE
from actor_localizer_logic import ActorLocalizerLogic
from actor_gallery_router import router as actor_gallery_router
from douban_fixer_logic import DoubanFixerLogic
from douban_fixer_router import router as douban_fixer_router
from webhook_logic import WebhookLogic
from proxy_manager import ProxyManager
from episode_renamer_logic import EpisodeRenamerLogic
from episode_role_sync_logic import EpisodeRoleSyncLogic

setup_logging()
scheduler = AsyncIOScheduler(timezone="Asia/Shanghai")

episode_sync_queue: Dict[str, Dict[str, Any]] = {}
episode_sync_queue_lock = threading.Lock()
episode_sync_scheduler_task = None

id_map_update_request_time: Optional[float] = None
id_map_update_lock = threading.Lock()
id_map_update_scheduler_task = None

scan_and_rename_queue: Dict[str, Dict[str, Any]] = {}
scan_and_rename_queue_lock = threading.Lock()
library_scan_scheduler_task = None

main_task_completed_series: set[str] = set()

def generate_id_map_task(cancellation_event: threading.Event, task_id: str, task_manager: TaskManager):
    """
    æ‰«æå…¨åº“ï¼Œç”Ÿæˆ TMDB ID åˆ° Emby Item ID çš„æ˜ å°„æ–‡ä»¶ã€‚
    """
    task_cat = "IDæ˜ å°„è¡¨ç”Ÿæˆ"
    ui_logger.info(f"â¡ï¸ å¼€å§‹æ‰«æå…¨åº“ï¼Œç”Ÿæˆ TMDB-Emby ID æ˜ å°„è¡¨...", task_category=task_cat)
    
    config = app_config.load_app_config()
    selector = MediaSelector(config)
    
    all_media_scope = ScheduledTasksTargetScope(mode='all')
    all_item_ids = selector.get_item_ids(all_media_scope)

    if not all_item_ids:
        ui_logger.warning("âš ï¸ æœªåœ¨ Emby ä¸­æ‰¾åˆ°ä»»ä½•åª’ä½“é¡¹ï¼Œä»»åŠ¡ä¸­æ­¢ã€‚", task_category=task_cat)
        return

    total_items = len(all_item_ids)
    task_manager.update_task_progress(task_id, 0, total_items)
    ui_logger.info(f"ğŸ” å·²è·å–åˆ° {total_items} ä¸ªåª’ä½“é¡¹å®ä¾‹ï¼Œæ­£åœ¨æ‰¹é‡å¤„ç†...", task_category=task_cat)

    # --- æ ¸å¿ƒä¿®æ”¹ 1: åˆå§‹åŒ– id_map ---
    id_map = {}
    processed_count = 0
    skipped_count = 0
    failed_count = 0

    from concurrent.futures import ThreadPoolExecutor, as_completed

    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_id = {executor.submit(selector._get_emby_item_details, item_id, "ProviderIds,Name,Type"): item_id for item_id in all_item_ids}
        
        for future in as_completed(future_to_id):
            if cancellation_event.is_set():
                ui_logger.warning("âš ï¸ ä»»åŠ¡åœ¨å¤„ç†ä¸­è¢«å–æ¶ˆã€‚", task_category=task_cat)
                for f in future_to_id:
                    f.cancel()
                return
            
            item_id = future_to_id[future]
            try:
                details = future.result()
                provider_ids = details.get("ProviderIds", {})
                provider_ids_lower = {k.lower(): v for k, v in provider_ids.items()}
                tmdb_id = provider_ids_lower.get("tmdb")
                
                # --- æ ¸å¿ƒä¿®æ”¹ 2: è·å–åª’ä½“ç±»å‹å¹¶æ„å»ºæ–°çš„å¸¦å‰ç¼€çš„é”® ---
                item_type = details.get("Type") # "Movie" or "Series"
                
                if tmdb_id and item_type:
                    prefix = 'tv' if item_type == 'Series' else 'movie'
                    map_key = f"{prefix}-{tmdb_id}"
                    
                    # åˆå§‹åŒ–é”®
                    if map_key not in id_map:
                        id_map[map_key] = []
                    
                    # è¿½åŠ  Emby ID
                    id_map[map_key].append(item_id)
                # --- ä¿®æ”¹ç»“æŸ ---
                else:
                    item_name = details.get("Name", f"ID {item_id}")
                    reason = "ç¼ºå°‘ TMDB ID" if not tmdb_id else "ç¼ºå°‘åª’ä½“ç±»å‹ä¿¡æ¯"
                    ui_logger.info(f"   - [è·³è¿‡] åª’ä½“ã€{item_name}ã€‘(ID: {item_id}) å›  {reason} è€Œè¢«å¿½ç•¥ã€‚", task_category=task_cat)
                    skipped_count += 1
            except Exception as e:
                ui_logger.error(f"   - âŒ å¤„ç†åª’ä½“ {item_id} æ—¶å‡ºé”™: {e}", task_category=task_cat)
                failed_count += 1
            
            processed_count += 1
            if processed_count % 100 == 0 or processed_count == total_items:
                task_manager.update_task_progress(task_id, processed_count, total_items)

    ID_MAP_FILE = os.path.join('/app/data', 'id_map.json')
    try:
        with open(ID_MAP_FILE, 'w', encoding='utf-8') as f:
            json.dump(id_map, f, indent=4)
        
        # --- æ ¸å¿ƒä¿®æ”¹ 3: æ›´æ–°æœ€ç»ˆçš„æ—¥å¿—è¾“å‡º ---
        total_emby_ids_mapped = sum(len(v) for v in id_map.values())
        ui_logger.info(f"âœ… æ˜ å°„è¡¨ç”Ÿæˆå®Œæ¯•ã€‚å…±å¤„ç† {total_items} ä¸ªåª’ä½“é¡¹ï¼Œæ˜ å°„ {len(id_map)} ä¸ªå”¯ä¸€çš„ TMDB-ID-ç±»å‹ ç»„åˆï¼Œå…³è” {total_emby_ids_mapped} ä¸ªEmbyåª’ä½“é¡¹ã€‚è·³è¿‡: {skipped_count} é¡¹, å¤±è´¥: {failed_count} é¡¹ã€‚", task_category=task_cat)
        # --- ä¿®æ”¹ç»“æŸ ---
    except IOError as e:
        ui_logger.error(f"âŒ å†™å…¥æ˜ å°„è¡¨æ–‡ä»¶å¤±è´¥: {e}", task_category=task_cat)
        raise e
    
async def id_map_update_scheduler():
    """
    ç‹¬ç«‹çš„åå°è°ƒåº¦å™¨ï¼Œç”¨äºå¤„ç† ID æ˜ å°„è¡¨æ›´æ–°çš„å»¶è¿Ÿè§¦å‘ä»»åŠ¡ã€‚
    """
    task_cat = "IDæ˜ å°„è°ƒåº¦å™¨"
    logging.info(f"ã€{task_cat}ã€‘å·²å¯åŠ¨ï¼Œå°†æ¯ 60 ç§’æ£€æŸ¥ä¸€æ¬¡æ›´æ–°è¯·æ±‚...")
    
    while True:
        try:
            await asyncio.sleep(60) # è°ƒåº¦å™¨æ£€æŸ¥å‘¨æœŸ
            
            global id_map_update_request_time
            if id_map_update_request_time is None:
                continue

            now = time.time()
            silence_duration = now - id_map_update_request_time
            
            # æ£€æŸ¥æ˜¯å¦æœ‰IDæ˜ å°„ä»»åŠ¡æ­£åœ¨è¿è¡Œ
            is_task_running = any("ID æ˜ å°„è¡¨" in task['name'] for task in task_manager.get_all_tasks())
            if is_task_running:
                logging.info(f"ã€{task_cat}ã€‘â±ï¸ æ£€æµ‹åˆ°å·²æœ‰IDæ˜ å°„è¡¨ç”Ÿæˆä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œæœ¬æ¬¡è°ƒåº¦è·³è¿‡ï¼Œç­‰å¾…å…¶å®Œæˆåå†é‡æ–°è®¡æ—¶ã€‚")
                continue

            if silence_duration >= 300: # 300ç§’ (5åˆ†é’Ÿ) é™é»˜æœŸ
                ui_logger.info(f"â¡ï¸ã€{task_cat}ã€‘æ£€æµ‹åˆ° Webhook è¯·æ±‚å·²é™é»˜ {silence_duration:.1f} ç§’ (>=300s)ï¼Œå¼€å§‹æ‰§è¡Œ ID æ˜ å°„è¡¨å…¨é‡æ›´æ–°...", task_category=task_cat)
                
                # è§¦å‘ä»»åŠ¡
                task_manager.register_task(generate_id_map_task, "Webhookè§¦å‘-IDæ˜ å°„è¡¨æ›´æ–°")
                
                # é‡ç½®è®¡æ—¶å™¨
                with id_map_update_lock:
                    id_map_update_request_time = None
                logging.info(f"ã€{task_cat}ã€‘âœ… ä»»åŠ¡å·²æ´¾å‘ï¼Œæ›´æ–°è¯·æ±‚è®¡æ—¶å™¨å·²é‡ç½®ã€‚")
            else:
                remaining_time = 300 - silence_duration
                logging.info(f"ã€{task_cat}ã€‘â±ï¸ æ”¶åˆ°æ›´æ–°è¯·æ±‚ï¼Œå½“å‰å·²é™é»˜ {silence_duration:.1f} ç§’ï¼Œç­‰å¾…å‰©ä½™ {remaining_time:.1f} ç§’...")

        except asyncio.CancelledError:
            logging.info(f"ã€{task_cat}ã€‘æ”¶åˆ°å…³é—­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
            break
        except Exception as e:
            logging.error(f"ã€{task_cat}ã€‘è¿è¡Œæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            await asyncio.sleep(120) # å‘ç”Ÿé”™è¯¯æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´

# backend/main.py (å‡½æ•°æ›¿æ¢)

async def library_scan_scheduler():
    """
    ç‹¬ç«‹çš„åå°è°ƒåº¦å™¨ï¼Œç”¨äºå¤„ç†åª’ä½“åº“æ–‡ä»¶æ‰«æçš„é˜²æŠ–è§¦å‘ä»»åŠ¡ã€‚
    åœ¨æ‰«æå‰ï¼Œä¼šå…ˆå¤„ç†è¯¥åª’ä½“åº“ä¸‹å¾…é‡å‘½åçš„ç”µå½±ï¼Œå¹¶æ ¹æ®å¤„ç†ç»“æœå†³å®šæ˜¯å¦éœ€è¦æ‰«æã€‚
    """
    task_cat = "åª’ä½“åº“æ‰«æè°ƒåº¦å™¨"
    logging.info(f"ã€{task_cat}ã€‘å·²å¯åŠ¨ï¼Œå°†æ¯ 60 ç§’æ£€æŸ¥ä¸€æ¬¡å¾…å¤„ç†é˜Ÿåˆ—...")
    
    while True:
        try:
            await asyncio.sleep(60) # è°ƒåº¦å™¨æ£€æŸ¥å‘¨æœŸ
            
            now = time.time()
            ready_to_process_tasks = {}
            
            with scan_and_rename_queue_lock:
                if not scan_and_rename_queue:
                    continue

                logging.info(f"ã€{task_cat}ã€‘ğŸ” å¼€å§‹æ£€æŸ¥é˜Ÿåˆ—ï¼Œå½“å‰æœ‰ {len(scan_and_rename_queue)} ä¸ªåª’ä½“åº“å¾…å¤„ç†...")
                
                libs_to_pop = []
                for lib_id, entry in scan_and_rename_queue.items():
                    last_update_time = entry.get('last_update', 0)
                    silence_duration = now - last_update_time
                    
                    if silence_duration >= 90: # 90ç§’é™é»˜æœŸ
                        logging.info(f"   - âœ… åª’ä½“åº“ (ID: {lib_id}) æ¡ä»¶æ»¡è¶³ï¼šé™é»˜ {silence_duration:.1f} ç§’ (>=90s)ã€‚å‡†å¤‡å¤„ç†ã€‚")
                        ready_to_process_tasks[lib_id] = entry
                        libs_to_pop.append(lib_id)
                    else:
                        remaining_time = 90 - silence_duration
                        logging.info(f"   - â±ï¸ åª’ä½“åº“ (ID: {lib_id}) é™é»˜æ—¶é•¿ {silence_duration:.1f} ç§’ï¼Œç­‰å¾…å‰©ä½™ {remaining_time:.1f} ç§’...")

                # ä»ä¸»é˜Ÿåˆ—ä¸­å®‰å…¨åœ°å¼¹å‡ºè¿™äº›åª’ä½“åº“
                for lib_id in libs_to_pop:
                    scan_and_rename_queue.pop(lib_id, None)

            # åœ¨é”å¤–æ‰§è¡Œè€—æ—¶ä»»åŠ¡
            if ready_to_process_tasks:
                config = app_config.load_app_config()
                from movie_renamer_logic import MovieRenamerLogic
                renamer_logic = MovieRenamerLogic(config)
                
                for lib_id, entry in ready_to_process_tasks.items():
                    try:
                        lib_info = renamer_logic._get_library_for_item_by_id(lib_id)
                        lib_name = lib_info.get("Name") if lib_info else f"ID {lib_id}"
                    except Exception:
                        lib_name = f"ID {lib_id}"

                    items_to_rename = entry.get('items_to_rename')
                    
                    # --- æ ¸å¿ƒä¿®æ”¹ï¼šå¼•å…¥æŒ‰éœ€æ‰«æé€»è¾‘ ---
                    should_scan = False
                    if items_to_rename:
                        # åœºæ™¯1: è¿™æ˜¯ç”± Webhook è§¦å‘çš„ã€å¸¦é‡å‘½åä»»åŠ¡çš„æµç¨‹
                        ui_logger.info(f"â¡ï¸ [æ‰«æå‰ç½®ä»»åŠ¡] æ£€æµ‹åˆ°åª’ä½“åº“ã€{lib_name}ã€‘æœ‰ {len(items_to_rename)} ä¸ªç”µå½±å¾…é‡å‘½åï¼Œå¼€å§‹å¤„ç†...", task_category=task_cat)
                        
                        renamed_any_file = False
                        movie_details_list = renamer_logic._get_movie_details_batch(list(items_to_rename), task_cat)
                        
                        for movie_details in movie_details_list:
                            result = renamer_logic.process_single_movie(movie_details, task_cat)
                            if result is not None:
                                renamed_any_file = True
                        
                        if renamed_any_file:
                            ui_logger.info(f"âœ… [æ‰«æå†³ç­–] æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´ï¼Œå°†ä¸ºåª’ä½“åº“ã€{lib_name}ã€‘æ‰§è¡Œæ‰«æã€‚", task_category=task_cat)
                            should_scan = True
                        else:
                            ui_logger.info(f"âœ… [æ‰«æå†³ç­–] æ‰€æœ‰å¾…å¤„ç†ç”µå½±çš„æ–‡ä»¶åå‡åˆæ ¼ï¼Œæ— éœ€ä¸ºåª’ä½“åº“ã€{lib_name}ã€‘æ‰§è¡Œæ‰«æã€‚", task_category=task_cat)
                    else:
                        # åœºæ™¯2: è¿™æ˜¯â€œçº¯æ‰«æâ€è¯·æ±‚
                        logging.info(f"ã€{task_cat}ã€‘åª’ä½“åº“ã€{lib_name}ã€‘æ”¶åˆ°ä¸€ä¸ªçº¯æ‰«æè¯·æ±‚ï¼Œå°†ç›´æ¥æ‰§è¡Œæ‰«æã€‚")
                        should_scan = True
                    
                    if should_scan:
                        renamer_logic._trigger_library_scan(lib_id, lib_name, task_cat)
                    # --- ä¿®æ”¹ç»“æŸ ---

        except asyncio.CancelledError:
            logging.info(f"ã€{task_cat}ã€‘æ”¶åˆ°å…³é—­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
            break
        except Exception as e:
            logging.error(f"ã€{task_cat}ã€‘è¿è¡Œæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            await asyncio.sleep(120)

async def episode_sync_scheduler():
    """
    ç‹¬ç«‹çš„åå°è°ƒåº¦å™¨ï¼Œç”¨äºå¤„ç†åˆ†é›†è§’è‰²åŒæ­¥çš„å»¶è¿Ÿè§¦å‘ä»»åŠ¡ã€‚
    """
    task_cat = "åˆ†é›†åŒæ­¥è°ƒåº¦å™¨"
    logging.info(f"ã€{task_cat}ã€‘å·²å¯åŠ¨ï¼Œå°†æ¯ 60 ç§’æ£€æŸ¥ä¸€æ¬¡å¾…å¤„ç†é˜Ÿåˆ—...")
    
    while True:
        try:
            await asyncio.sleep(60) # è°ƒåº¦å™¨æ£€æŸ¥å‘¨æœŸ
            
            now = time.time()
            series_to_process = {}
            
            with episode_sync_queue_lock:
                if not episode_sync_queue:
                    continue

                logging.info(f"ã€{task_cat}ã€‘ğŸ” å¼€å§‹æ£€æŸ¥é˜Ÿåˆ—ï¼Œå½“å‰æœ‰ {len(episode_sync_queue)} ä¸ªå‰§é›†å¾…å¤„ç†...")
                
                ready_series_ids = []
                for series_id, data in episode_sync_queue.items():
                    last_update_time = data['last_update']
                    series_name = data['series_name']
                    silence_duration = now - last_update_time
                    
                    is_silent_enough = silence_duration >= 90
                    is_main_task_done = series_id in main_task_completed_series

                    if is_silent_enough and is_main_task_done:
                        logging.info(f"   - âœ… å‰§é›†ã€Š{series_name}ã€‹ (ID: {series_id}) æ¡ä»¶æ»¡è¶³ï¼šé™é»˜ {silence_duration:.1f} ç§’ (>=90s) ä¸”ä¸»æµç¨‹å·²å®Œæˆã€‚å‡†å¤‡å¤„ç†ã€‚")
                        ready_series_ids.append(series_id)
                    elif is_silent_enough and not is_main_task_done:
                        logging.info(f"   - â±ï¸ å‰§é›†ã€Š{series_name}ã€‹ (ID: {series_id}) å·²æ»¡è¶³é™é»˜æ¡ä»¶ï¼Œä½†å…¶ä¸»æµç¨‹ä»»åŠ¡ï¼ˆå¦‚æ¼”å‘˜ä¸­æ–‡åŒ–ï¼‰å°šæœªå®Œæˆï¼Œç»§ç»­ç­‰å¾…...")
                    else: # not silent enough
                        remaining_time = 90 - silence_duration
                        logging.info(f"   - â±ï¸ å‰§é›†ã€Š{series_name}ã€‹ (ID: {series_id}) é™é»˜æ—¶é•¿ {silence_duration:.1f} ç§’ï¼Œç­‰å¾…å‰©ä½™ {remaining_time:.1f} ç§’...")

                # ä»ä¸»é˜Ÿåˆ—å’Œå®Œæˆæ ‡è®°ä¸­å®‰å…¨åœ°å¼¹å‡ºè¿™äº›å‰§é›†çš„æ•°æ®
                for series_id in ready_series_ids:
                    series_to_process[series_id] = episode_sync_queue.pop(series_id)
                    if series_id in main_task_completed_series:
                        main_task_completed_series.remove(series_id)

            # åœ¨é”å¤–æ‰§è¡Œè€—æ—¶ä»»åŠ¡
            if series_to_process:
                ui_logger.info(f"â¡ï¸ {len(series_to_process)} ä¸ªå‰§é›†å·²æ»¡è¶³æ‰€æœ‰æ¡ä»¶ï¼Œå¼€å§‹æ´¾å‘ç²¾å‡†åŒæ­¥ä»»åŠ¡...", task_category=task_cat)
                config = app_config.load_app_config()
                logic = EpisodeRoleSyncLogic(config)

                for series_id, data in series_to_process.items():
                    series_name = data['series_name']
                    episode_ids = list(data['episode_ids'])
                    task_name = f"ç²¾å‡†åˆ†é›†è§’è‰²åŒæ­¥ -ã€Š{series_name}ã€‹({len(episode_ids)}é›†)"
                    
                    task_manager.register_task(
                        logic.run_sync_for_specific_episodes,
                        task_name,
                        series_id=series_id,
                        episode_ids=episode_ids,
                        config=config.episode_role_sync_config,
                        task_category=task_name
                    )
        except asyncio.CancelledError:
            logging.info(f"ã€{task_cat}ã€‘æ”¶åˆ°å…³é—­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
            break
        except Exception as e:
            logging.error(f"ã€{task_cat}ã€‘è¿è¡Œæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            await asyncio.sleep(120)


webhook_queue = asyncio.Queue()
webhook_processing_set = set()

async def webhook_worker():
    # Webhook æ—¥å¿—å·²åœ¨ webhook_logic.py ä¸­å¤„ç†ï¼Œæ­¤å¤„ä¿ç•™åº•å±‚æ—¥å¿—
    logging.info("ã€Webhookå·¥ä½œè€…ã€‘å·²å¯åŠ¨ï¼Œç­‰å¾…å¤„ç†ä»»åŠ¡...")
    while True:
        try:
            item_id, item_name = await webhook_queue.get()
            
            task_id = task_manager.register_task(
                _webhook_task_runner, 
                f"Webhook-è‡ªåŠ¨å¤„ç†-ã€{item_name}ã€‘",
                item_id=item_id
            )
            
            while task_id in task_manager.tasks:
                await asyncio.sleep(1)

        except asyncio.CancelledError:
            logging.info("ã€Webhookå·¥ä½œè€…ã€‘æ”¶åˆ°å…³é—­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
            break
        except Exception as e:
            logging.error(f"ã€Webhookå·¥ä½œè€…ã€‘å¤„ç†ä»»åŠ¡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            await asyncio.sleep(5)
        finally:
            if item_id in webhook_processing_set:
                webhook_processing_set.remove(item_id)
            webhook_queue.task_done()

def _webhook_task_runner(item_id: str, cancellation_event: threading.Event, task_id: str, task_manager: TaskManager):
    current_config = app_config.load_app_config()
    logic = WebhookLogic(current_config)
    logic.process_new_media_task(item_id, cancellation_event, series_id=item_id)


def trigger_douban_refresh():
    task_cat = "å®šæ—¶ä»»åŠ¡-è±†ç“£æ•°æ®"
    ui_logger.info("å¼€å§‹æ‰§è¡Œè±†ç“£æ•°æ®å¼ºåˆ¶åˆ·æ–°...", task_category=task_cat)
    config = app_config.load_app_config()
    douban_conf = config.douban_config
    if douban_conf.directory and os.path.isdir(douban_conf.directory):
        task_manager.register_task(scan_douban_directory_task, "å®šæ—¶åˆ·æ–°è±†ç“£æ•°æ®", douban_conf.directory, douban_conf.extra_fields)
    else:
        ui_logger.warning("æœªé…ç½®æœ‰æ•ˆçš„è±†ç“£ç›®å½•ï¼Œè·³è¿‡å®šæ—¶åˆ·æ–°ã€‚", task_category=task_cat)

def trigger_douban_fixer_scan():
    task_cat = "å®šæ—¶ä»»åŠ¡-è±†ç“£ä¿®å¤"
    ui_logger.info("å¼€å§‹æ‰§è¡Œè±†ç“£IDä¿®å¤å™¨å…¨é‡æ‰«æ...", task_category=task_cat)
    config = app_config.load_app_config()
    for task in task_manager.get_all_tasks():
        if task['name'].startswith("è±†ç“£IDä¿®å¤"):
            ui_logger.warning("æ£€æµ‹åˆ°å·²æœ‰è±†ç“£IDä¿®å¤ä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œæœ¬æ¬¡è°ƒåº¦è·³è¿‡ã€‚", task_category=task_cat)
            return
    logic = DoubanFixerLogic(config)
    task_manager.register_task(logic.scan_and_match_task, "è±†ç“£IDä¿®å¤-all", "all", None, None)

def trigger_actor_localizer_apply():
    task_cat = "å®šæ—¶ä»»åŠ¡-æ¼”å‘˜ä¸­æ–‡åŒ–"
    ui_logger.info("å¼€å§‹æ‰§è¡Œæ¼”å‘˜ä¸­æ–‡åŒ–è‡ªåŠ¨åº”ç”¨ä»»åŠ¡...", task_category=task_cat)
    config = app_config.load_app_config()
    for task in task_manager.get_all_tasks():
        if task['name'].startswith("æ¼”å‘˜ä¸­æ–‡åŒ–"):
            ui_logger.warning("æ£€æµ‹åˆ°å·²æœ‰æ¼”å‘˜ä¸­æ–‡åŒ–ä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œæœ¬æ¬¡è°ƒåº¦è·³è¿‡ã€‚", task_category=task_cat)
            return
    logic = ActorLocalizerLogic(config)
    task_manager.register_task(logic.apply_actor_changes_directly_task, "æ¼”å‘˜ä¸­æ–‡åŒ–-å®šæ—¶è‡ªåŠ¨åº”ç”¨", config.actor_localizer_config)

def _episode_refresher_task_runner(
    series_ids: List[str], 
    config: AppConfig, 
    cancellation_event: threading.Event, 
    task_id: str, 
    task_manager: TaskManager,
    task_name: str 
):
    task_cat = task_name
    ui_logger.info(f"å¼€å§‹ä» {len(series_ids)} ä¸ªå‰§é›†(Series)ä¸­è·å–æ‰€æœ‰åˆ†é›†(Episode)...", task_category=task_cat)
    task_manager.update_task_progress(task_id, 0, len(series_ids))

    all_episode_ids = []
    session = requests.Session()
    for i, series_id in enumerate(series_ids):
        if cancellation_event.is_set():
            ui_logger.warning("åœ¨è·å–åˆ†é›†é˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
            return

        try:
            episodes_url = f"{config.server_config.server}/Items"
            episodes_params = {
                "api_key": config.server_config.api_key,
                "ParentId": series_id,
                "IncludeItemTypes": "Episode",
                "Recursive": "true",
                "Fields": "Id"
            }
            episodes_resp = session.get(episodes_url, params=episodes_params, timeout=30)
            if episodes_resp.ok:
                episodes = episodes_resp.json().get("Items", [])
                all_episode_ids.extend([ep['Id'] for ep in episodes])
        except Exception as e:
            ui_logger.error(f"è·å–å‰§é›† {series_id} çš„åˆ†é›†æ—¶å¤±è´¥: {e}", task_category=task_cat)
        
        task_manager.update_task_progress(task_id, i + 1, len(series_ids))

    ui_logger.info(f"åˆ†é›†è·å–å®Œæ¯•ï¼Œå…±æ‰¾åˆ° {len(all_episode_ids)} ä¸ªåˆ†é›†éœ€è¦åˆ·æ–°ã€‚", task_category=task_cat)
    
    logic = EpisodeRefresherLogic(config)
    logic.run_refresh_for_episodes(
        all_episode_ids,
        config.episode_refresher_config,
        cancellation_event,
        task_id,
        task_manager,
        task_cat
    )

def _episode_screenshot_backup_task_runner(
    series_ids: List[str], 
    config: AppConfig, 
    cancellation_event: threading.Event, 
    task_id: str, 
    task_manager: TaskManager,
    task_name: str 
):
    task_cat = task_name
    logic = EpisodeRefresherLogic(config)
    logic.backup_screenshots_from_emby_task(
        series_ids,
        config.episode_refresher_config,
        cancellation_event,
        task_id,
        task_manager
    )

def _episode_renamer_task_runner(
    series_ids: List[str], 
    config: AppConfig, 
    cancellation_event: threading.Event, 
    task_id: str, 
    task_manager: TaskManager,
    task_name: str 
):
    task_cat = task_name
    ui_logger.info(f"å¼€å§‹ä» {len(series_ids)} ä¸ªå‰§é›†(Series)ä¸­è·å–æ‰€æœ‰åˆ†é›†(Episode)...", task_category=task_cat)
    task_manager.update_task_progress(task_id, 0, len(series_ids))

    all_episode_ids = []
    session = requests.Session()
    for i, series_id in enumerate(series_ids):
        if cancellation_event.is_set():
            ui_logger.warning("åœ¨è·å–åˆ†é›†é˜¶æ®µè¢«å–æ¶ˆã€‚", task_category=task_cat)
            return

        try:
            episodes_url = f"{config.server_config.server}/Items"
            episodes_params = {
                "api_key": config.server_config.api_key,
                "ParentId": series_id,
                "IncludeItemTypes": "Episode",
                "Recursive": "true",
                "Fields": "Id"
            }
            episodes_resp = session.get(episodes_url, params=episodes_params, timeout=30)
            if episodes_resp.ok:
                episodes = episodes_resp.json().get("Items", [])
                all_episode_ids.extend([ep['Id'] for ep in episodes])
        except Exception as e:
            ui_logger.error(f"è·å–å‰§é›† {series_id} çš„åˆ†é›†æ—¶å¤±è´¥: {e}", task_category=task_cat)
        
        task_manager.update_task_progress(task_id, i + 1, len(series_ids))

    ui_logger.info(f"åˆ†é›†è·å–å®Œæ¯•ï¼Œå…±æ‰¾åˆ° {len(all_episode_ids)} ä¸ªåˆ†é›†éœ€è¦å¤„ç†ã€‚", task_category=task_cat)
    
    logic = EpisodeRenamerLogic(config)
    logic.run_rename_for_episodes(
        all_episode_ids,
        cancellation_event,
        task_id,
        task_manager,
        task_cat
    )


def trigger_scheduled_task(task_id: str):
    task_name_map = {
        "actor_localizer": "æ¼”å‘˜ä¸­æ–‡åŒ–",
        "douban_fixer": "è±†ç“£IDä¿®å¤",
        "douban_poster_updater": "è±†ç“£æµ·æŠ¥æ›´æ–°",
        "episode_refresher": "å‰§é›†å…ƒæ•°æ®åˆ·æ–°",
        "episode_renamer": "å‰§é›†æ–‡ä»¶é‡å‘½å",
        "movie_renamer": "ç”µå½±æ–‡ä»¶é‡å‘½å",
        "episode_role_sync": "å‰§é›†è§’è‰²åŒæ­¥åˆ°åˆ†é›†",
        # --- æ–°å¢è¡Œ ---
        "id_mapper": "TMDB-Emby ID æ˜ å°„è¡¨"
    }
    task_display_name = task_name_map.get(task_id, task_id)
    task_cat = f"å®šæ—¶ä»»åŠ¡-{task_display_name}"
    ui_logger.info(f"å¼€å§‹æ‰§è¡Œå®šæ—¶ä»»åŠ¡...", task_category=task_cat)
    
    # --- æ ¸å¿ƒä¿®æ”¹ï¼šIDæ˜ å°„ä»»åŠ¡ä¸ä¾èµ–é€šç”¨èŒƒå›´ ---
    if task_id == "id_mapper":
        task_manager.register_task(generate_id_map_task, f"å®šæ—¶ä»»åŠ¡-{task_display_name}")
        return

    config = app_config.load_app_config()
    scope = config.scheduled_tasks_config.target_scope
    selector = MediaSelector(config)
    
    target_collection_type = None
    if task_id == "episode_refresher" or task_id == "episode_renamer":
        target_collection_type = "tvshows"

    elif task_id == "movie_renamer":
        target_collection_type = "movies"
        
    item_ids = selector.get_item_ids(scope, target_collection_type=target_collection_type)

    if not item_ids:
        ui_logger.info(f"æœªæ ¹æ®èŒƒå›´æ‰¾åˆ°ä»»ä½•åª’ä½“é¡¹ï¼Œä»»åŠ¡ç»“æŸã€‚", task_category=task_cat)
        return

    if task_id == "actor_localizer":
        logic = ActorLocalizerLogic(config)
        task_name = f"å®šæ—¶ä»»åŠ¡-æ¼”å‘˜ä¸­æ–‡åŒ–({scope.mode})"
        task_manager.register_task(
            logic.run_localization_for_items, 
            task_name, 
            item_ids, 
            config.actor_localizer_config,
            task_category=task_name
        )
    elif task_id == "douban_fixer":
        logic = DoubanFixerLogic(config)
        task_name = f"å®šæ—¶ä»»åŠ¡-è±†ç“£IDä¿®å¤({scope.mode})"
        task_manager.register_task(
            logic.run_fixer_for_items,
            task_name,
            item_ids,
            task_category=task_name
        )
    elif task_id == "douban_poster_updater":
        logic = DoubanPosterUpdaterLogic(config)
        task_name = f"å®šæ—¶ä»»åŠ¡-è±†ç“£æµ·æŠ¥æ›´æ–°({scope.mode})"
        task_manager.register_task(
            logic.run_poster_update_for_items,
            task_name,
            item_ids,
            config.douban_poster_updater_config
        )
    elif task_id == "episode_refresher":
        task_name = f"å®šæ—¶ä»»åŠ¡-å‰§é›†å…ƒæ•°æ®åˆ·æ–°({scope.mode})"
        task_manager.register_task(
            _episode_refresher_task_runner,
            task_name,
            series_ids=item_ids,
            config=config,
            task_name=task_name
        )
    elif task_id == "episode_renamer":
        task_name = f"å®šæ—¶ä»»åŠ¡-å‰§é›†æ–‡ä»¶é‡å‘½å({scope.mode})"
        task_manager.register_task(
            _episode_renamer_task_runner,
            task_name,
            series_ids=item_ids,
            config=config,
            task_name=task_name
        )
    elif task_id == "episode_role_sync":
        logic = EpisodeRoleSyncLogic(config)
        task_name = f"å®šæ—¶ä»»åŠ¡-å‰§é›†è§’è‰²åŒæ­¥({scope.mode})"
        task_manager.register_task(
            logic.run_sync_for_items,
            task_name,
            item_ids,
            config.episode_role_sync_config,
            task_category=task_name
        )
    elif task_id == "movie_renamer":
        from movie_renamer_logic import MovieRenamerLogic
        logic = MovieRenamerLogic(config)
        task_name = f"å®šæ—¶ä»»åŠ¡-ç”µå½±æ–‡ä»¶é‡å‘½å({scope.mode})"
        task_manager.register_task(
            logic.run_rename_task_for_items,
            task_name,
            item_ids=item_ids,
            task_category=task_name
        )
    else:
        ui_logger.warning(f"æœªçŸ¥çš„ä»»åŠ¡ID: {task_id}", task_category=task_cat)

def trigger_chasing_workflow():
    """å†…ç½®çš„æ¯æ—¥è¿½æ›´å·¥ä½œæµè§¦å‘å™¨"""
    task_cat = "å®šæ—¶ä»»åŠ¡-è¿½æ›´ç»´æŠ¤"
    config = app_config.load_app_config()
    if not config.chasing_center_config.enabled:
        logging.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘è¿½æ›´å·¥ä½œæµæœªå¯ç”¨ï¼Œè·³è¿‡æ¯æ—¥ç»´æŠ¤ã€‚")
        return
    
    ui_logger.info("å¼€å§‹æ‰§è¡Œå†…ç½®çš„æ¯æ—¥è¿½æ›´ç»´æŠ¤ä»»åŠ¡...", task_category=task_cat)
    from chasing_center_logic import ChasingCenterLogic
    logic = ChasingCenterLogic(config)
    task_manager.register_task(logic.run_chasing_workflow_task, "å®šæ—¶ä»»åŠ¡-è¿½æ›´æ¯æ—¥ç»´æŠ¤")

def trigger_upcoming_notification():
    """å³å°†ä¸Šæ˜ è®¢é˜…é€šçŸ¥è§¦å‘å™¨"""
    task_cat = "å®šæ—¶ä»»åŠ¡-è®¢é˜…é€šçŸ¥"
    config = app_config.load_app_config()
    if not config.upcoming_config.enabled:
        logging.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘å³å°†ä¸Šæ˜ åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡è®¢é˜…é€šçŸ¥ã€‚")
        return
    
    ui_logger.info("å¼€å§‹æ‰§è¡Œè®¢é˜…åˆ—è¡¨é€šçŸ¥ä»»åŠ¡...", task_category=task_cat)
    from upcoming_logic import UpcomingLogic
    logic = UpcomingLogic(config)
    # è¿™æ˜¯ä¸€ä¸ªå¿«é€Ÿä»»åŠ¡ï¼Œç›´æ¥åœ¨ä¸»çº¿ç¨‹æ‰§è¡Œï¼Œä¸æ³¨å†Œåˆ° task_manager
    logic.check_and_notify()

def trigger_calendar_notification():
    """è¿½å‰§æ—¥å†é€šçŸ¥è§¦å‘å™¨"""
    task_cat = "å®šæ—¶ä»»åŠ¡-è¿½å‰§æ—¥å†"
    config = app_config.load_app_config()
    if not config.chasing_center_config.enabled or not config.telegram_config.enabled:
        logging.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘è¿½æ›´å·¥ä½œæµæˆ–Telegramé€šçŸ¥æœªå¯ç”¨ï¼Œè·³è¿‡æ—¥å†å‘é€ã€‚")
        return
    
    ui_logger.info("å¼€å§‹æ‰§è¡Œè¿½å‰§æ—¥å†é€šçŸ¥ä»»åŠ¡...", task_category=task_cat)
    from chasing_center_logic import ChasingCenterLogic
    logic = ChasingCenterLogic(config)
    task_manager.register_task(logic.send_calendar_notification_task, "å®šæ—¶ä»»åŠ¡-è¿½å‰§æ—¥å†é€šçŸ¥")

def trigger_media_tagger_task():
    """åª’ä½“æ ‡ç­¾å™¨å®šæ—¶ä»»åŠ¡è§¦å‘å™¨"""
    task_cat = "å®šæ—¶ä»»åŠ¡-åª’ä½“æ ‡ç­¾å™¨"
    config = app_config.load_app_config()
    if not config.media_tagger_config.enabled:
        logging.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘åª’ä½“æ ‡ç­¾å™¨å®šæ—¶ä»»åŠ¡æœªå¯ç”¨ï¼Œè·³è¿‡æ‰§è¡Œã€‚")
        return
    
    ui_logger.info("å¼€å§‹æ‰§è¡Œåª’ä½“æ ‡ç­¾å™¨å®šæ—¶ä»»åŠ¡...", task_category=task_cat)
    from media_tagger_logic import MediaTaggerLogic
    logic = MediaTaggerLogic(config)
    task_manager.register_task(logic.run_tagging_task, "åª’ä½“æ ‡ç­¾å™¨-å®šæ—¶ä»»åŠ¡")

def update_chasing_scheduler():
    """æ›´æ–°è¿½æ›´ä¸­å¿ƒçš„å®šæ—¶ä»»åŠ¡"""
    task_cat = "ç³»ç»Ÿé…ç½®"
    ui_logger.info("ã€è°ƒåº¦ä»»åŠ¡ã€‘æ£€æµ‹åˆ°è¿½æ›´ä¸­å¿ƒé…ç½®å˜æ›´ï¼Œæ­£åœ¨æ›´æ–°è°ƒåº¦å™¨...", task_category=task_cat)
    config = app_config.load_app_config()
    
    # æ¯æ—¥ç»´æŠ¤ä»»åŠ¡ (å†…ç½®ï¼Œä¸å¯é…ç½®)
    job_id_workflow = "chasing_workflow_daily"
    # --- ä¿®æ”¹ ---
    if config.chasing_center_config.enabled and config.chasing_center_config.maintenance_cron:
        try:
            scheduler.add_job(
                trigger_chasing_workflow, 
                CronTrigger.from_crontab(config.chasing_center_config.maintenance_cron), 
                id=job_id_workflow, 
                replace_existing=True
            )
            ui_logger.info(f"  - å·²æ›´æ–°æ¯æ—¥è¿½æ›´ç»´æŠ¤ä»»åŠ¡ (CRON: {config.chasing_center_config.maintenance_cron})", task_category=task_cat)
        except Exception as e:
            ui_logger.error(f"  - âŒ æ›´æ–°æ¯æ—¥è¿½æ›´ç»´æŠ¤ä»»åŠ¡å¤±è´¥: {e}", task_category=task_cat)
    # --- ä¿®æ”¹ç»“æŸ ---
    elif scheduler.get_job(job_id_workflow):
        scheduler.remove_job(job_id_workflow)
        ui_logger.info(f"  - å·²ç¦ç”¨å¹¶ç§»é™¤æ¯æ—¥è¿½æ›´ç»´æŠ¤ä»»åŠ¡ã€‚", task_category=task_cat)

    # æ—¥å†é€šçŸ¥ä»»åŠ¡ (å¯é…ç½®)
    job_id_calendar = "chasing_calendar_notification"
    if config.chasing_center_config.enabled and config.chasing_center_config.notification_cron:
        try:
            scheduler.add_job(
                trigger_calendar_notification, 
                CronTrigger.from_crontab(config.chasing_center_config.notification_cron), 
                id=job_id_calendar, 
                replace_existing=True
            )
            ui_logger.info(f"  - å·²æ›´æ–°è¿½å‰§æ—¥å†é€šçŸ¥ä»»åŠ¡ (CRON: {config.chasing_center_config.notification_cron})", task_category=task_cat)
        except Exception as e:
            ui_logger.error(f"  - âŒ æ›´æ–°è¿½å‰§æ—¥å†ä»»åŠ¡å¤±è´¥: {e}", task_category=task_cat)
    elif scheduler.get_job(job_id_calendar):
        scheduler.remove_job(job_id_calendar)
        ui_logger.info(f"  - å·²ç§»é™¤è¿½å‰§æ—¥å†é€šçŸ¥ä»»åŠ¡ã€‚", task_category=task_cat)


def update_upcoming_scheduler():
    """æ›´æ–°å³å°†ä¸Šæ˜ åŠŸèƒ½çš„å®šæ—¶ä»»åŠ¡"""
    task_cat = "ç³»ç»Ÿé…ç½®"
    ui_logger.info("ã€è°ƒåº¦ä»»åŠ¡ã€‘æ£€æµ‹åˆ°å³å°†ä¸Šæ˜ åŠŸèƒ½é…ç½®å˜æ›´ï¼Œæ­£åœ¨æ›´æ–°è°ƒåº¦å™¨...", task_category=task_cat)
    config = app_config.load_app_config()
    
    # --- æ–°å¢ï¼šæ¸…ç†ä»»åŠ¡çš„é€»è¾‘ ---
    from upcoming_logic import UpcomingLogic
    logic = UpcomingLogic(config)

    def trigger_pruning_task():
        """å³å°†ä¸Šæ˜ è¿‡æœŸé¡¹ç›®æ¸…ç†è§¦å‘å™¨"""
        task_cat_prune = "å®šæ—¶ä»»åŠ¡-è®¢é˜…æ¸…ç†"
        if not config.upcoming_config.enabled:
            logging.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘å³å°†ä¸Šæ˜ åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡è¿‡æœŸé¡¹ç›®æ¸…ç†ã€‚")
            return
        ui_logger.info("å¼€å§‹æ‰§è¡Œè®¢é˜…åˆ—è¡¨è¿‡æœŸé¡¹ç›®æ¸…ç†ä»»åŠ¡...", task_category=task_cat_prune)
        logic.prune_expired_items()
    # --- æ–°å¢ç»“æŸ ---

    # è®¢é˜…é€šçŸ¥ä»»åŠ¡
    job_id_notify = "upcoming_notification"
    if config.upcoming_config.enabled and config.upcoming_config.notification_cron:
        try:
            scheduler.add_job(
                trigger_upcoming_notification, 
                CronTrigger.from_crontab(config.upcoming_config.notification_cron), 
                id=job_id_notify, 
                replace_existing=True
            )
            ui_logger.info(f"  - å·²æ›´æ–°è®¢é˜…é€šçŸ¥ä»»åŠ¡ (CRON: {config.upcoming_config.notification_cron})", task_category=task_cat)
        except Exception as e:
            ui_logger.error(f"  - âŒ æ›´æ–°è®¢é˜…é€šçŸ¥ä»»åŠ¡å¤±è´¥: {e}", task_category=task_cat)
    elif scheduler.get_job(job_id_notify):
        scheduler.remove_job(job_id_notify)
        ui_logger.info(f"  - å·²ç¦ç”¨å¹¶ç§»é™¤è®¢é˜…é€šçŸ¥ä»»åŠ¡ã€‚", task_category=task_cat)

    # --- æ–°å¢ï¼šæ¸…ç†ä»»åŠ¡çš„è°ƒåº¦ ---
    job_id_prune = "upcoming_pruning"
    if config.upcoming_config.enabled and config.upcoming_config.pruning_cron:
        try:
            scheduler.add_job(
                trigger_pruning_task, 
                CronTrigger.from_crontab(config.upcoming_config.pruning_cron), 
                id=job_id_prune, 
                replace_existing=True
            )
            ui_logger.info(f"  - å·²æ›´æ–°è¿‡æœŸé¡¹ç›®æ¸…ç†ä»»åŠ¡ (CRON: {config.upcoming_config.pruning_cron})", task_category=task_cat)
        except Exception as e:
            ui_logger.error(f"  - âŒ æ›´æ–°è¿‡æœŸé¡¹ç›®æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}", task_category=task_cat)
    elif scheduler.get_job(job_id_prune):
        scheduler.remove_job(job_id_prune)
        ui_logger.info(f"  - å·²ç¦ç”¨å¹¶ç§»é™¤è¿‡æœŸé¡¹ç›®æ¸…ç†ä»»åŠ¡ã€‚", task_category=task_cat)
    # --- æ–°å¢ç»“æŸ ---



@asynccontextmanager
async def lifespan(app: FastAPI):
    global episode_sync_scheduler_task, id_map_update_scheduler_task # å£°æ˜æˆ‘ä»¬è¦ä¿®æ”¹å…¨å±€å˜é‡
    task_cat = "ç³»ç»Ÿå¯åŠ¨"
    # --- æ ¸å¿ƒä¿®æ”¹ 1: åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œä½†ä¸åŒ…æ‹¬ WebSocket å¤„ç†å™¨ ---
    # è¿™æ˜¯ä¸ºäº†è®©æ—¥å¿—ç³»ç»Ÿåœ¨åº”ç”¨å¯åŠ¨çš„æœ€æ—©æœŸå°±èƒ½å·¥ä½œ
    from log_manager import setup_logging, WebSocketLogHandler
    setup_logging(add_websocket_handler=False)
    
    # --- æ ¸å¿ƒä¿®æ”¹ 2: åœ¨ä¸»çº¿ç¨‹ï¼ˆlifespanå†…ï¼‰åˆ›å»ºå¹¶æ·»åŠ  WebSocket å¤„ç†å™¨ ---
    # æ­¤æ—¶ get_running_loop() ä¸€å®šèƒ½æˆåŠŸ
    websocket_handler = WebSocketLogHandler()
    logging.getLogger().addHandler(websocket_handler)
    ui_logger.info("åº”ç”¨å¯åŠ¨...", task_category=task_cat)
    required_tools = ['ffmpeg', 'ffprobe']
    for tool in required_tools:
        if not shutil.which(tool):
            ui_logger.warning(f"ã€å¯åŠ¨æ£€æŸ¥ã€‘æœªæ‰¾åˆ° '{tool}' å‘½ä»¤ï¼Œè§†é¢‘æˆªå›¾åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚è¯·ç¡®ä¿å·²åœ¨ Docker ç¯å¢ƒæˆ–ä¸»æœºä¸Šå®‰è£… ffmpegã€‚", task_category=task_cat)
    task_manager_consumer = asyncio.create_task(task_manager.broadcast_consumer())
    webhook_worker_task = asyncio.create_task(webhook_worker())
    # --- æ–°å¢ ---
    episode_sync_scheduler_task = asyncio.create_task(episode_sync_scheduler())
    # --- æ–°å¢ï¼šå¯åŠ¨IDæ˜ å°„è¡¨æ›´æ–°è°ƒåº¦å™¨ ---
    id_map_update_scheduler_task = asyncio.create_task(id_map_update_scheduler())

    library_scan_scheduler_task = asyncio.create_task(library_scan_scheduler())
    # --- æ–°å¢ç»“æŸ ---

    config = app_config.load_app_config()
    
    douban_conf = config.douban_config
    if douban_conf.directory and os.path.isdir(douban_conf.directory):
        if not os.path.exists(DOUBAN_CACHE_FILE):
            ui_logger.info("ã€å¯åŠ¨æ£€æŸ¥ã€‘æœªå‘ç°è±†ç“£ç¼“å­˜æ–‡ä»¶ï¼Œå°†è‡ªåŠ¨æ‰§è¡Œé¦–æ¬¡æ‰«æã€‚", task_category=task_cat)
            task_manager.register_task(scan_douban_directory_task, "é¦–æ¬¡å¯åŠ¨è±†ç“£æ‰«æ", douban_conf.directory, douban_conf.extra_fields)
        else:
            logging.info(f"ã€å¯åŠ¨æ£€æŸ¥ã€‘å·²æ‰¾åˆ°è±†ç“£ç¼“å­˜æ–‡ä»¶: {DOUBAN_CACHE_FILE}ï¼Œè·³è¿‡è‡ªåŠ¨æ‰«æã€‚")
    else:
        logging.warning("ã€å¯åŠ¨æ£€æŸ¥ã€‘æœªé…ç½®æœ‰æ•ˆçš„è±†ç“£ç›®å½•ï¼Œæ— æ³•æ‰§è¡Œæ‰«æã€‚")

    if douban_conf.refresh_cron:
        try:
            scheduler.add_job(trigger_douban_refresh, CronTrigger.from_crontab(douban_conf.refresh_cron), id="douban_refresh_job", replace_existing=True)
            ui_logger.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘å·²æˆåŠŸè®¾ç½®è±†ç“£æ•°æ®å®šæ—¶åˆ·æ–°ä»»åŠ¡ï¼ŒCRONè¡¨è¾¾å¼: '{douban_conf.refresh_cron}'", task_category=task_cat)
        except Exception as e:
            ui_logger.error(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘è®¾ç½®å®šæ—¶åˆ·æ–°ä»»åŠ¡å¤±è´¥ï¼ŒCRONè¡¨è¾¾å¼å¯èƒ½æ— æ•ˆ: {e}", task_category=task_cat)

    fixer_conf = config.douban_fixer_config
    if fixer_conf.scan_cron:
        try:
            scheduler.add_job(trigger_douban_fixer_scan, CronTrigger.from_crontab(fixer_conf.scan_cron), id="douban_fixer_scan_job", replace_existing=True)
            ui_logger.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘å·²æˆåŠŸè®¾ç½®è±†ç“£IDä¿®å¤å™¨å®šæ—¶æ‰«æä»»åŠ¡ï¼ŒCRONè¡¨è¾¾å¼: '{fixer_conf.scan_cron}'", task_category=task_cat)
        except Exception as e:
            ui_logger.error(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘è®¾ç½®è±†ç“£IDä¿®å¤å™¨å®šæ—¶æ‰«æä»»åŠ¡å¤±è´¥ï¼ŒCRONè¡¨è¾¾å¼å¯èƒ½æ— æ•ˆ: {e}", task_category=task_cat)

    actor_conf = config.actor_localizer_config
    if actor_conf.apply_cron:
        try:
            scheduler.add_job(trigger_actor_localizer_apply, CronTrigger.from_crontab(actor_conf.apply_cron), id="actor_localizer_apply_job", replace_existing=True)
            ui_logger.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘å·²æˆåŠŸè®¾ç½®æ¼”å‘˜ä¸­æ–‡åŒ–è‡ªåŠ¨åº”ç”¨ä»»åŠ¡ï¼ŒCRONè¡¨è¾¾å¼: '{actor_conf.apply_cron}'", task_category=task_cat)
        except Exception as e:
            ui_logger.error(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘è®¾ç½®æ¼”å‘˜ä¸­æ–‡åŒ–è‡ªåŠ¨åº”ç”¨ä»»åŠ¡å¤±è´¥ï¼ŒCRONè¡¨è¾¾å¼å¯èƒ½æ— æ•ˆ: {e}", task_category=task_cat)

    
    ui_logger.info("ã€è°ƒåº¦ä»»åŠ¡ã€‘å¼€å§‹è®¾ç½®é€šç”¨å®šæ—¶ä»»åŠ¡...", task_category=task_cat)
    scheduled_conf = config.scheduled_tasks_config
    for task in scheduled_conf.tasks:
        if task.enabled and task.cron:
            try:
                scheduler.add_job(
                    trigger_scheduled_task, 
                    CronTrigger.from_crontab(task.cron), 
                    id=f"scheduled_{task.id}", 
                    replace_existing=True,
                    args=[task.id]
                )
                ui_logger.info(f"  - å·²æˆåŠŸè®¾ç½®å®šæ—¶ä»»åŠ¡ '{task.name}'ï¼ŒCRONè¡¨è¾¾å¼: '{task.cron}'", task_category=task_cat)
            except Exception as e:
                ui_logger.error(f"  - è®¾ç½®å®šæ—¶ä»»åŠ¡ '{task.name}' å¤±è´¥ï¼ŒCRONè¡¨è¾¾å¼å¯èƒ½æ— æ•ˆ: {e}", task_category=task_cat)

    ui_logger.info("ã€è°ƒåº¦ä»»åŠ¡ã€‘å¼€å§‹è®¾ç½®è¿½æ›´ä¸­å¿ƒä»»åŠ¡...", task_category=task_cat)
    update_chasing_scheduler()

    ui_logger.info("ã€è°ƒåº¦ä»»åŠ¡ã€‘å¼€å§‹è®¾ç½®å³å°†ä¸Šæ˜ ä»»åŠ¡...", task_category=task_cat)
    update_upcoming_scheduler()

    media_tagger_conf = config.media_tagger_config
    if media_tagger_conf.enabled and media_tagger_conf.cron:
        try:
            scheduler.add_job(
                trigger_media_tagger_task,
                CronTrigger.from_crontab(media_tagger_conf.cron),
                id="media_tagger_job",
                replace_existing=True
            )
            ui_logger.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘å·²æˆåŠŸè®¾ç½®åª’ä½“æ ‡ç­¾å™¨ä»»åŠ¡ï¼ŒCRONè¡¨è¾¾å¼: '{media_tagger_conf.cron}'", task_category=task_cat)
        except Exception as e:
            ui_logger.error(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘è®¾ç½®åª’ä½“æ ‡ç­¾å™¨ä»»åŠ¡å¤±è´¥ï¼ŒCRONè¡¨è¾¾å¼å¯èƒ½æ— æ•ˆ: {e}", task_category=task_cat)

    if not scheduler.running:
        scheduler.start()

    yield
    
    logging.info("åº”ç”¨å…³é—­ï¼Œæ­£åœ¨åœæ­¢åå°æœåŠ¡...")
    
    if scheduler.running:
        scheduler.shutdown(wait=False)
        logging.info("APScheduler å·²è¢«æŒ‡ä»¤å…³é—­ã€‚")

    webhook_worker_task.cancel()
    task_manager_consumer.cancel()
    # --- æ–°å¢ ---
    if episode_sync_scheduler_task:
        episode_sync_scheduler_task.cancel()
    if id_map_update_scheduler_task:
        id_map_update_scheduler_task.cancel()
    # --- æ–°å¢ï¼šå–æ¶ˆåª’ä½“åº“æ‰«æè°ƒåº¦å™¨ ---
    if library_scan_scheduler_task:
        library_scan_scheduler_task.cancel()
    await asyncio.gather(
        webhook_worker_task, 
        task_manager_consumer, 
        episode_sync_scheduler_task, 
        id_map_update_scheduler_task, 
        library_scan_scheduler_task, # æ·»åŠ åˆ° gather
        return_exceptions=True
    )
    # --- ä¿®æ”¹ç»“æŸ ---
    logging.info("æ‰€æœ‰åå°ä»»åŠ¡å·²æˆåŠŸå–æ¶ˆã€‚")

app = FastAPI(lifespan=lifespan)
origins = ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(actor_gallery_router, prefix="/api/gallery")
app.include_router(douban_fixer_router, prefix="/api/douban-fixer")
app.include_router(episode_renamer_router, prefix="/api/episode-renamer")
app.include_router(poster_manager_router, prefix="/api/poster-manager")
app.include_router(actor_role_mapper_router, prefix="/api/actor-role-mapper")
app.include_router(episode_role_sync_router, prefix="/api/episode-role-sync")
app.include_router(actor_avatar_mapper_router, prefix="/api/actor-avatar-mapper")
app.include_router(chasing_center_router, prefix="/api/chasing-center")
app.include_router(upcoming_router, prefix="/api/upcoming")
app.include_router(file_scraper_router, prefix="/api/file-scraper")
app.include_router(media_tagger_router, prefix="/api/media-tagger")

from models import TraktConfig
from trakt_manager import TraktManager

@app.post("/api/config/trakt")
def save_trakt_config_api(config: TraktConfig):
    """ä¿å­˜ Trakt.tv é…ç½®"""
    try:
        ui_logger.info("æ­£åœ¨ä¿å­˜ Trakt.tv è®¾ç½®...", task_category="ç³»ç»Ÿé…ç½®")
        current_app_config = app_config.load_app_config()
        current_app_config.trakt_config = config
        app_config.save_app_config(current_app_config)
        ui_logger.info("âœ… Trakt.tv è®¾ç½®ä¿å­˜æˆåŠŸï¼", task_category="ç³»ç»Ÿé…ç½®")
        return {"success": True, "message": "è®¾ç½®å·²ä¿å­˜ï¼"}
    except Exception as e:
        logging.error(f"ä¿å­˜ Trakt.tv è®¾ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜è®¾ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")

@app.post("/api/trakt/test")
def test_trakt_api(config: TraktConfig):
    """æµ‹è¯• Trakt.tv API è¿æ¥"""
    task_cat = "Trakt-æµ‹è¯•"
    if not config.enabled:
        return {"success": True, "message": "Trakt æœªå¯ç”¨ï¼Œæ— éœ€æµ‹è¯•ã€‚"}
    if not config.client_id:
        raise HTTPException(status_code=400, detail="Client ID ä¸èƒ½ä¸ºç©ºã€‚")
    
    ui_logger.info(f"â¡ï¸ [Trakt] å¼€å§‹æµ‹è¯• Client ID...", task_category=task_cat)
    
    temp_app_config = app_config.load_app_config()
    temp_app_config.trakt_config = config
    trakt_manager = TraktManager(temp_app_config)
    
    # ä½¿ç”¨ä¸€ä¸ªå¸¸è§çš„ TMDB ID è¿›è¡Œæµ‹è¯•
    test_tmdb_id = "1399" # Game of Thrones
    result = trakt_manager.get_show_seasons_with_episodes(test_tmdb_id)
    
    if result is not None:
        ui_logger.info(f"âœ… [Trakt] æµ‹è¯•æˆåŠŸï¼æˆåŠŸè·å–åˆ°ã€ŠæƒåŠ›çš„æ¸¸æˆã€‹çš„ {len(result)} æ¡åˆ†é›†æ•°æ®ã€‚", task_category=task_cat)
        return {"success": True, "message": "Trakt API è¿æ¥æˆåŠŸï¼"}
    else:
        ui_logger.error(f"âŒ [Trakt] æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ Client ID æ˜¯å¦æ­£ç¡®ä»¥åŠç½‘ç»œè¿æ¥ã€‚", task_category=task_cat)
        raise HTTPException(status_code=500, detail="æ— æ³•ä» Trakt.tv è·å–æ•°æ®ï¼Œè¯·æ£€æŸ¥ Client ID æˆ–ä»£ç†è®¾ç½®ã€‚")


@app.get("/api/image-proxy")
async def image_proxy(url: str):
    """
    é€šç”¨çš„å¤–éƒ¨å›¾ç‰‡ä»£ç†ï¼Œç”¨äºè§£å†³å‰ç«¯æ··åˆå†…å®¹é—®é¢˜ã€‚
    æ­¤å®ç°éµå¾ªç”¨æˆ·å®šä¹‰çš„ä»£ç†è§„åˆ™ã€‚
    """
    task_cat = "å›¾ç‰‡ä»£ç†-é€šç”¨"
    try:
        config = app_config.load_app_config()
        proxy_manager = ProxyManager(config)
        # --- æ ¸å¿ƒä¿®æ­£ï¼šä½¿ç”¨ mounts å‚æ•° ---
        mounts = proxy_manager.get_proxies_for_httpx(url)

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',
        }
        if 'doubanio.com' in url:
            headers['Referer'] = 'https://movie.douban.com/'

        async with httpx.AsyncClient(mounts=mounts or {}, follow_redirects=True) as client:
            req = await client.get(url, headers=headers, timeout=20)
        # --- ä¿®æ­£ç»“æŸ ---
            req.raise_for_status()

        content_type = req.headers.get('Content-Type', 'image/jpeg')
        if not content_type.startswith('image/'):
            ui_logger.warning(f"âš ï¸ ä»£ç†è¯·æ±‚è¿”å›çš„ Content-Type ä¸æ˜¯å›¾ç‰‡: {content_type}ã€‚URL: {url}", task_category=task_cat)
            raise HTTPException(status_code=400, detail="ä»£ç†ç›®æ ‡è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡å†…å®¹ã€‚")

        return Response(content=req.content, media_type=content_type)
        
    except httpx.RequestError as e:
        ui_logger.error(f"âŒ è¯·æ±‚å¤–éƒ¨å›¾ç‰‡å¤±è´¥: {e}", task_category=task_cat, exc_info=True)
        raise HTTPException(status_code=502, detail=f"è¯·æ±‚å¤–éƒ¨å›¾ç‰‡URLå¤±è´¥: {e}")
    except Exception as e:
        ui_logger.error(f"âŒ ä»£ç†å›¾ç‰‡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", task_category=task_cat, exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.websocket("/ws/logs")
async def websocket_logs_endpoint(websocket: WebSocket):
    await log_broadcaster.connect(websocket)
    try:
        while True: await websocket.receive_text()
    except WebSocketDisconnect:
        log_broadcaster.disconnect(websocket)
        logging.info("æ—¥å¿— WebSocket å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ã€‚")
@app.websocket("/ws/tasks")
async def websocket_tasks_endpoint(websocket: WebSocket):
    await task_manager.broadcaster.connect(websocket)
    await websocket.send_json(task_manager.get_all_tasks())
    try:
        while True: await websocket.receive_text()
    except WebSocketDisconnect:
        task_manager.broadcaster.disconnect(websocket)
        logging.info("ä»»åŠ¡ WebSocket å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ã€‚")

@app.get("/api/emby-image-proxy")
async def emby_image_proxy(path: str):
    task_cat = "å›¾ç‰‡ä»£ç†-Emby"
    try:
        config = app_config.load_app_config()
        server_conf = config.server_config
        if not server_conf.server:
            raise HTTPException(status_code=400, detail="EmbyæœåŠ¡å™¨æœªé…ç½®ã€‚")

        if 'api_key=' not in path.lower():
            separator = '&' if '?' in path else '?'
            path_with_auth = f"{path}{separator}api_key={server_conf.api_key}"
        else:
            path_with_auth = path

        full_url = f"{server_conf.server}/{path_with_auth}"
        
        proxy_manager = ProxyManager(config)
        # --- æ ¸å¿ƒä¿®æ­£ï¼šä½¿ç”¨ mounts å‚æ•° ---
        mounts = proxy_manager.get_proxies_for_httpx(full_url)

        async with httpx.AsyncClient(mounts=mounts or {}, follow_redirects=True) as client:
            req = await client.get(full_url, timeout=20)
        # --- ä¿®æ­£ç»“æŸ ---
        
        req.raise_for_status()
        
        content_type = req.headers.get('Content-Type', 'image/jpeg')
        if not content_type.startswith('image/'):
            ui_logger.warning(f"âš ï¸ ä»£ç†è¯·æ±‚è¿”å›çš„ Content-Type ä¸æ˜¯å›¾ç‰‡: {content_type}ã€‚URL: {full_url}", task_category=task_cat)
            raise HTTPException(status_code=400, detail="ä»£ç†ç›®æ ‡è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡å†…å®¹ã€‚")

        return Response(content=req.content, media_type=content_type)
        
    except httpx.RequestError as e:
        ui_logger.error(f"âŒ è¯·æ±‚ Emby å›¾ç‰‡æ—¶å‘ç”Ÿç½‘ç»œå¼‚å¸¸: {e}", task_category=task_cat, exc_info=True)
        raise HTTPException(status_code=502, detail=f"è¯·æ±‚ Emby æœåŠ¡å™¨å¤±è´¥: {e}")
    except Exception as e:
        ui_logger.error(f"âŒ ä»£ç† Emby å›¾ç‰‡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", task_category=task_cat, exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
def read_root(): return {"message": "æ¬¢è¿æ¥åˆ° Emby å·¥å…·ç®±åç«¯!"}
@app.get("/api/config", response_model=AppConfig)
def get_app_config_api():
    config = app_config.load_app_config()
    is_scanning = any("è±†ç“£" in task['name'] for task in task_manager.get_all_tasks())
    if not os.path.exists(DOUBAN_CACHE_FILE):
        status = DoubanCacheStatus(exists=False, item_count=0, last_modified=None, is_scanning=is_scanning)
    else:
        try:
            mtime = os.path.getmtime(DOUBAN_CACHE_FILE)
            last_modified = datetime.fromtimestamp(mtime).isoformat()
            with open(DOUBAN_CACHE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
            item_count = len(data)
            status = DoubanCacheStatus(exists=True, item_count=item_count, last_modified=last_modified, is_scanning=is_scanning)
        except Exception:
            status = DoubanCacheStatus(exists=True, item_count=0, last_modified=None, is_scanning=is_scanning)
    config.douban_cache_status = status
    return config
@app.post("/api/config/server")
def save_server_config_api(server_config: ServerConfig):
    try:
        logging.info("æ­£åœ¨æµ‹è¯•å¹¶ä¿å­˜ Emby æœåŠ¡å™¨é…ç½®...")
        current_app_config = app_config.load_app_config()
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨æ–°çš„ ProxyManager ---
        test_url = f"{server_config.server}/Users/{server_config.user_id}"
        proxy_manager = ProxyManager(current_app_config)
        proxies = proxy_manager.get_proxies(test_url)
        
        if proxies:
            logging.info(f"ã€è¿æ¥æµ‹è¯•ã€‘å°†é€šè¿‡ä»£ç† {proxies.get('http')} è¿æ¥ Emby æœåŠ¡å™¨ã€‚")
        else:
            logging.info("ã€è¿æ¥æµ‹è¯•ã€‘å°†ç›´æ¥è¿æ¥ Emby æœåŠ¡å™¨ã€‚")
        # --- ç»“æŸä¿®æ”¹ ---

        params = {"api_key": server_config.api_key}
        response = requests.get(test_url, params=params, timeout=15, proxies=proxies)
        response.raise_for_status()
        user_data = response.json()
        if not user_data.get("Name"): raise ValueError("æœåŠ¡å™¨å“åº”å¼‚å¸¸ï¼Œæœªæ‰¾åˆ°æœ‰æ•ˆçš„ç”¨æˆ·ä¿¡æ¯ã€‚")
        
        system_info_url = f"{server_config.server}/System/Info"
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä¸ºç¬¬äºŒä¸ªè¯·æ±‚ä¹Ÿåº”ç”¨ä»£ç†é€»è¾‘ ---
        proxies_system = proxy_manager.get_proxies(system_info_url)
        response_system = requests.get(system_info_url, params=params, timeout=15, proxies=proxies_system)
        # --- ç»“æŸä¿®æ”¹ ---
        
        response_system.raise_for_status()
        system_info = response_system.json()
        current_app_config.server_config = server_config
        app_config.save_app_config(current_app_config)
        logging.info("Emby æœåŠ¡å™¨é…ç½®ä¿å­˜æˆåŠŸï¼")
        return {"success": True, "message": "é…ç½®å·²ä¿å­˜ï¼Œè¿æ¥æˆåŠŸï¼", "details": {"serverName": system_info.get("ServerName", "æœªçŸ¥"), "serverVersion": system_info.get("Version", "æœªçŸ¥"), "userName": user_data.get("Name", "æœªçŸ¥")}}
    except Exception as e:
        logging.error(f"ä¿å­˜æœåŠ¡å™¨é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))
@app.post("/api/config/download")
def save_download_config_api(download_config: DownloadConfig):
    try:
        logging.info("æ­£åœ¨ä¿å­˜ä¸‹è½½è®¾ç½®...")
        current_app_config = app_config.load_app_config()
        current_app_config.download_config = download_config
        app_config.save_app_config(current_app_config)
        logging.info("ä¸‹è½½è®¾ç½®ä¿å­˜æˆåŠŸï¼")
        return {"success": True, "message": "ä¸‹è½½è®¾ç½®å·²ä¿å­˜ï¼"}
    except Exception as e:
        logging.error(f"ä¿å­˜ä¸‹è½½è®¾ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜ä¸‹è½½è®¾ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")

@app.post("/api/config/proxy")
def save_proxy_config_api(proxy_config: ProxyConfig):
    try:
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ ui_logger å‘é€ä¸­æ–‡æ—¥å¿—ç»™å‰ç«¯ ---
        ui_logger.info("æ­£åœ¨ä¿å­˜ç½‘ç»œä»£ç†è®¾ç½®...", task_category="ç³»ç»Ÿé…ç½®")
        # --- ä½¿ç”¨ logging.debug è®°å½•è¯¦ç»†çš„æŠ€æœ¯æ—¥å¿—åˆ°åç«¯ ---
        logging.debug(f"æ¥æ”¶åˆ°çš„ä»£ç†é…ç½®åŸå§‹æ•°æ®: {proxy_config.model_dump_json()}")
        
        if proxy_config.url and not (proxy_config.url.startswith("http://") or proxy_config.url.startswith("https://")):
             raise ValueError("ä»£ç†åœ°å€æ ¼å¼ä¸æ­£ç¡®ï¼Œå¿…é¡»ä»¥ http:// æˆ– https:// å¼€å¤´ã€‚")
        
        current_app_config = app_config.load_app_config()
        current_app_config.proxy_config = proxy_config
        app_config.save_app_config(current_app_config)
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ ui_logger å‘é€ä¸­æ–‡æˆåŠŸæ—¥å¿—ç»™å‰ç«¯ ---
        ui_logger.info("ä»£ç†è®¾ç½®ä¿å­˜æˆåŠŸï¼", task_category="ç³»ç»Ÿé…ç½®")
        
        return {"success": True, "message": "ä»£ç†è®¾ç½®å·²ä¿å­˜ï¼"}
    except Exception as e:
        logging.error(f"ä¿å­˜ä»£ç†è®¾ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜ä»£ç†è®¾ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")

@app.post("/api/config/proxy/test")
def test_proxy_config_api(proxy_config: ProxyConfig):
    task_cat = "ä»£ç†æµ‹è¯•" # --- å®šä¹‰ä»»åŠ¡ç±»åˆ« ---
    if not proxy_config.enabled:
        ui_logger.info("ä»£ç†æœªå¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•ã€‚", task_category=task_cat)
        return {"success": True, "message": "ä»£ç†æœªå¯ç”¨ï¼Œæ— éœ€æµ‹è¯•ã€‚"}
    proxy_url = proxy_config.url
    if not proxy_url:
        raise HTTPException(status_code=400, detail="ä»£ç†å·²å¯ç”¨ï¼Œä½†ä»£ç†åœ°å€ä¸èƒ½ä¸ºç©ºã€‚")
    if not (proxy_url.startswith("http://") or proxy_url.startswith("https://")):
        raise HTTPException(status_code=400, detail="ä»£ç†åœ°å€æ ¼å¼ä¸æ­£ç¡®ï¼Œå¿…é¡»ä»¥ http:// æˆ– https:// å¼€å¤´ã€‚")
    ui_logger.info(f"å¼€å§‹æµ‹è¯•ï¼Œå°†é€šè¿‡ä»£ç† '{proxy_url}' è¿æ¥å¤–éƒ¨ç½‘ç»œ...", task_category=task_cat)
    proxies = {'http': proxy_url, 'https': proxy_url}
    test_target_url = "https://www.baidu.com" 
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}
    try:
        start_time = time.perf_counter()
        response = requests.head(test_target_url, proxies=proxies, headers=headers, timeout=10, allow_redirects=True)
        end_time = time.perf_counter()
        latency_ms = (end_time - start_time) * 1000
        response.raise_for_status()
        success_msg = f"ä»£ç†è¿æ¥æˆåŠŸï¼èƒ½å¤Ÿè®¿é—® {test_target_url}ã€‚"
        ui_logger.info(f"æˆåŠŸï¼æœåŠ¡å™¨è¿”å›çŠ¶æ€ç : {response.status_code}ã€‚å»¶è¿Ÿ: {latency_ms:.2f} msã€‚", task_category=task_cat)
        return {"success": True, "message": success_msg, "latency": f"{latency_ms:.2f} ms"}
    except requests.exceptions.ProxyError as e:
        ui_logger.error(f"å¤±è´¥ï¼ä»£ç†é”™è¯¯: {e}", task_category=task_cat, exc_info=True)
        raise HTTPException(status_code=500, detail=f"ä»£ç†æœåŠ¡å™¨é”™è¯¯ï¼Œè¯·æ£€æŸ¥åœ°å€å’Œç«¯å£æ˜¯å¦æ­£ç¡®ï¼Œä»¥åŠä»£ç†æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œã€‚")
    except requests.exceptions.ConnectTimeout:
        ui_logger.error(f"å¤±è´¥ï¼è¿æ¥ä»£ç†æœåŠ¡å™¨è¶…æ—¶ã€‚", task_category=task_cat)
        raise HTTPException(status_code=500, detail="è¿æ¥ä»£ç†æœåŠ¡å™¨è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ã€‚")
    except requests.exceptions.RequestException as e:
        ui_logger.error(f"å¤±è´¥ï¼å‘ç”Ÿç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}", task_category=task_cat, exc_info=True)
        raise HTTPException(status_code=500, detail=f"é€šè¿‡ä»£ç†è®¿é—®å¤–éƒ¨ç½‘ç»œå¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç†æ˜¯å¦èƒ½è®¿é—®å…¬ç½‘ã€‚é”™è¯¯: {e}")
    except Exception as e:
        ui_logger.error(f"å¤±è´¥ï¼å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", task_category=task_cat, exc_info=True)
        raise HTTPException(status_code=500, detail=f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    
@app.post("/api/config/tmdb")
def save_and_test_tmdb_config_api(tmdb_config: TmdbConfig):
    try:
        logging.info(f"æ­£åœ¨æµ‹è¯• TMDB API Key...")
        current_app_config = app_config.load_app_config()
        proxies = {}
        if current_app_config.proxy_config.enabled and current_app_config.proxy_config.url:
            proxy_url = current_app_config.proxy_config.url
            proxies = {'http': proxy_url, 'https': proxy_url}
            logging.info(f"ã€TMDBæµ‹è¯•ã€‘å°†é€šè¿‡ä»£ç† {proxy_url} è¿æ¥ TMDBã€‚")
        if tmdb_config.custom_api_domain_enabled and tmdb_config.custom_api_domain:
            base_domain = tmdb_config.custom_api_domain.rstrip('/')
            logging.info(f"ã€TMDBæµ‹è¯•ã€‘ä½¿ç”¨è‡ªå®šä¹‰APIåŸŸå: {base_domain}")
        else:
            base_domain = "https://api.themoviedb.org"
            logging.info(f"ã€TMDBæµ‹è¯•ã€‘ä½¿ç”¨é»˜è®¤APIåŸŸå: {base_domain}")
        test_url = f"{base_domain}/3/configuration"
        params = {"api_key": tmdb_config.api_key}
        response = requests.get(test_url, params=params, timeout=10, proxies=proxies)
        if response.status_code == 401:
            logging.error("ã€TMDBæµ‹è¯•ã€‘TMDB API Key æµ‹è¯•å¤±è´¥: æ— æ•ˆçš„ Keyã€‚")
            raise HTTPException(status_code=500, detail="TMDB API Key æ— æ•ˆæˆ–å·²è¿‡æœŸã€‚")
        response.raise_for_status()
        logging.info("ã€TMDBæµ‹è¯•ã€‘TMDB API Key æµ‹è¯•æˆåŠŸï¼")
        logging.info("æ­£åœ¨ä¿å­˜ TMDB é…ç½®...")
        current_app_config.tmdb_config = tmdb_config
        app_config.save_app_config(current_app_config)
        logging.info("TMDB é…ç½®å·²ä¿å­˜ã€‚")
        return {"success": True, "message": "TMDB é…ç½®å·²ä¿å­˜ï¼ŒAPI Key æœ‰æ•ˆï¼"}
    except HTTPException as e:
        raise e
    except requests.exceptions.RequestException as e:
        logging.error(f"ã€TMDBæµ‹è¯•ã€‘TMDB API æµ‹è¯•å¤±è´¥: ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=f"æ— æ³•è¿æ¥åˆ° TMDB æœåŠ¡ï¼Œè¯·æ£€æŸ¥ç½‘ç»œã€ä»£ç†æˆ–è‡ªå®šä¹‰åŸŸåè®¾ç½®ã€‚")
    except Exception as e:
        logging.error(f"ã€TMDBæµ‹è¯•ã€‘ä¿å­˜ TMDB é…ç½®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=f"å¤„ç† TMDB é…ç½®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    
@app.post("/api/config/douban")
def save_douban_config_api(douban_config: DoubanConfig):
    try:
        logging.info("æ­£åœ¨ä¿å­˜è±†ç“£æ•°æ®æºè®¾ç½®...")
        current_app_config = app_config.load_app_config()
        current_app_config.douban_config = douban_config
        app_config.save_app_config(current_app_config)
        if scheduler.running:
            existing_job = scheduler.get_job("douban_refresh_job")
            if existing_job:
                scheduler.remove_job("douban_refresh_job")
            if douban_config.refresh_cron:
                try:
                    scheduler.add_job(trigger_douban_refresh, CronTrigger.from_crontab(douban_config.refresh_cron), id="douban_refresh_job", replace_existing=True)
                    logging.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘å·²æ›´æ–°å®šæ—¶åˆ·æ–°ä»»åŠ¡ï¼Œæ–°CRONè¡¨è¾¾å¼: '{douban_config.refresh_cron}'")
                except Exception as e:
                    logging.error(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘æ›´æ–°å®šæ—¶ä»»åŠ¡å¤±è´¥: {e}")
                    raise HTTPException(status_code=400, detail=f"CRONè¡¨è¾¾å¼æ— æ•ˆ: {e}")
        logging.info("è±†ç“£æ•°æ®æºè®¾ç½®ä¿å­˜æˆåŠŸï¼")
        return {"success": True, "message": "è±†ç“£è®¾ç½®å·²ä¿å­˜ï¼ç›®å½•ä¿®æ”¹å°†åœ¨ä¸‹æ¬¡åˆ·æ–°æˆ–é‡å¯åç”Ÿæ•ˆã€‚"}
    except HTTPException as e:
        raise e
    except Exception as e:
        logging.error(f"ä¿å­˜è±†ç“£è®¾ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜è±†ç“£è®¾ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")

@app.post("/api/config/douban-fixer")
def save_douban_fixer_config_api(config: DoubanFixerConfig):
    try:
        logging.info("æ­£åœ¨ä¿å­˜è±†ç“£IDä¿®å¤å™¨è®¾ç½®...")
        current_app_config = app_config.load_app_config()
        current_app_config.douban_fixer_config = config
        app_config.save_app_config(current_app_config)
        if scheduler.running:
            job_id = "douban_fixer_scan_job"
            existing_job = scheduler.get_job(job_id)
            if existing_job:
                scheduler.remove_job(job_id)
                logging.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘å·²ç§»é™¤æ—§çš„è±†ç“£IDä¿®å¤å™¨ä»»åŠ¡ã€‚")
            if config.scan_cron:
                try:
                    scheduler.add_job(trigger_douban_fixer_scan, CronTrigger.from_crontab(config.scan_cron), id=job_id, replace_existing=True)
                    logging.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘å·²æ›´æ–°è±†ç“£IDä¿®å¤å™¨å®šæ—¶æ‰«æä»»åŠ¡ï¼Œæ–°CRONè¡¨è¾¾å¼: '{config.scan_cron}'")
                except Exception as e:
                    logging.error(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘æ›´æ–°è±†ç“£IDä¿®å¤å™¨ä»»åŠ¡å¤±è´¥: {e}")
                    raise HTTPException(status_code=400, detail=f"CRONè¡¨è¾¾å¼æ— æ•ˆ: {e}")

        logging.info("è±†ç“£IDä¿®å¤å™¨è®¾ç½®ä¿å­˜æˆåŠŸï¼")
        return {"success": True, "message": "è±†ç“£IDä¿®å¤å™¨è®¾ç½®å·²ä¿å­˜ï¼"}
    except HTTPException as e:
        raise e
    except Exception as e:
        logging.error(f"ä¿å­˜è±†ç“£IDä¿®å¤å™¨è®¾ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜è®¾ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")

@app.post("/api/douban/force-refresh")
def force_refresh_douban_data_api():
    config = app_config.load_app_config()
    douban_conf = config.douban_config
    if not douban_conf.directory or not os.path.isdir(douban_conf.directory):
        raise HTTPException(status_code=400, detail="è¯·å…ˆåœ¨é…ç½®é¡µé¢è®¾ç½®ä¸€ä¸ªæœ‰æ•ˆçš„è±†ç“£æ•°æ®æ ¹ç›®å½•ã€‚")
    task_id = task_manager.register_task(scan_douban_directory_task, "æ‰‹åŠ¨å¼ºåˆ¶åˆ·æ–°è±†ç“£æ•°æ®", douban_conf.directory, douban_conf.extra_fields)
    return {"status": "success", "message": "å¼ºåˆ¶åˆ·æ–°ä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·åœ¨â€œè¿è¡Œä»»åŠ¡â€é¡µé¢æŸ¥çœ‹è¿›åº¦ã€‚", "task_id": task_id}
LOG_FILE = os.path.join('/app/data', "app.log")


@app.get("/api/logs")
def get_logs_api(
    page: int = Query(1, ge=1), 
    page_size: int = Query(1000, ge=1), 
    level: str = Query("INFO"), 
    category: Optional[str] = Query(None),
    date: Optional[str] = Query(None, description="æŸ¥è¯¢æŒ‡å®šæ—¥æœŸçš„æ—¥å¿—ï¼Œæ ¼å¼ YYYY-MM-DD")
):
   
    LOGS_DIR = "/app/data/logs"
    
    log_file_path = ""
    if date:
        log_file_path = os.path.join(LOGS_DIR, f"app.log.{date}")
    else:
        log_file_path = os.path.join(LOGS_DIR, "app.log")

    

    if not os.path.exists(log_file_path):
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å›ç©ºç»“æœï¼Œé¿å…åç»­é”™è¯¯
        return {"total": 0, "logs": [], "totalPages": 0, "currentPage": page}

    try:
        with open(log_file_path, "r", encoding="utf-8") as f:
            all_lines = f.readlines()
        
        log_pattern = re.compile(
            r"^(?P<level>\w+):\s+"
            r"(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3})\s+-\s+"
            r"(?P<category>.+?)\s+â†’\s+"
            r"(?P<message>.*)$"
        )

        parsed_logs = []
        current_log_entry = None

        for line in all_lines:
            match = log_pattern.match(line.strip())
            if match:
                if current_log_entry:
                    parsed_logs.append(current_log_entry)
                current_log_entry = match.groupdict()
            elif current_log_entry:
                current_log_entry['message'] += '\n' + line.rstrip()
        
        if current_log_entry:
            parsed_logs.append(current_log_entry)

        # --- æ ¸å¿ƒä¿®æ”¹ 2: è¿‡æ»¤é€»è¾‘ä¿æŒä¸å˜ï¼Œä½†ä½œç”¨äºå•ä¸ªæ—¥å¿—æ–‡ä»¶ ---
        filtered_logs = []
        if level != "ALL":
            level_to_match = level.upper()
            for log in parsed_logs:
                if log.get('level', '').upper() == level_to_match:
                    filtered_logs.append(log)
        else:
            filtered_logs = parsed_logs
        
        if category:
            final_filtered_logs = [log for log in filtered_logs if log.get('category', '').strip() == category]
        else:
            final_filtered_logs = filtered_logs
        
        total_logs = len(final_filtered_logs)
        total_pages = (total_logs + page_size - 1) // page_size
        
        # --- æ ¸å¿ƒä¿®æ”¹ 3: åˆ†é¡µé€»è¾‘ä¿æŒä¸å˜ï¼Œä½†ç°åœ¨æ˜¯å€’åºåˆ†é¡µ ---
        # æœ€æ–°çš„æ—¥å¿—åœ¨æœ€å‰é¢
        start_index = (page - 1) * page_size
        end_index = start_index + page_size
        
        # ä»åè½¬åçš„åˆ—è¡¨ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰ä¸­åˆ‡ç‰‡
        paginated_logs = final_filtered_logs[::-1][start_index:end_index]
            
        return {"total": total_logs, "logs": paginated_logs, "totalPages": total_pages, "currentPage": page}
    except Exception as e:
        logging.error(f"âŒ è¯»å–æ—¥å¿—æ–‡ä»¶ '{log_file_path}' å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
    


@app.get("/api/logs/dates")
def get_log_dates_api():
    """æ‰«ææ—¥å¿—ç›®å½•å¹¶è¿”å›æ‰€æœ‰å¯ç”¨çš„æ—¥å¿—æ—¥æœŸåˆ—è¡¨"""
    LOGS_DIR = "/app/data/logs"
    dates = []
    try:
        if not os.path.exists(LOGS_DIR):
            logging.warning(f"âš ï¸ æ—¥å¿—ç›®å½• '{LOGS_DIR}' ä¸å­˜åœ¨ï¼Œæ— æ³•è·å–å†å²æ—¥æœŸã€‚")
            return []

        # æ£€æŸ¥å½“å¤©çš„æ—¥å¿—æ˜¯å¦å­˜åœ¨
        if os.path.exists(os.path.join(LOGS_DIR, "app.log")):
            today_str = datetime.now().strftime('%Y-%m-%d')
            dates.append(today_str)

        # æ‰«æå†å²æ—¥å¿—æ–‡ä»¶
        for filename in os.listdir(LOGS_DIR):
            if filename.startswith("app.log."):
                # æ–‡ä»¶åæ ¼å¼ä¸º app.log.YYYY-MM-DD
                date_part = filename.split('.')[-1]
                try:
                    # éªŒè¯æ—¥æœŸæ ¼å¼æ˜¯å¦æ­£ç¡®
                    datetime.strptime(date_part, '%Y-%m-%d')
                    dates.append(date_part)
                except ValueError:
                    # å¿½ç•¥æ ¼å¼ä¸æ­£ç¡®çš„æ–‡ä»¶
                    continue
        
        # æŒ‰æ—¥æœŸé™åºæ’åºï¼Œæœ€æ–°çš„æ—¥æœŸåœ¨æœ€å‰é¢
        dates.sort(reverse=True)
        return dates
    except Exception as e:
        logging.error(f"âŒ æ‰«ææ—¥å¿—æ—¥æœŸå¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ‰«ææ—¥å¿—æ—¥æœŸå¤±è´¥: {e}")


@app.get("/api/logs/categories")
def get_log_categories_api():
    """æ‰«ææ—¥å¿—ç›®å½•å¹¶è¿”å›æ‰€æœ‰å”¯ä¸€çš„ä»»åŠ¡ç±»åˆ«"""
    LOGS_DIR = "/app/data/logs"
    categories = set()
    
    try:
        if not os.path.exists(LOGS_DIR):
            logging.warning(f"âš ï¸ æ—¥å¿—ç›®å½• '{LOGS_DIR}' ä¸å­˜åœ¨ï¼Œæ— æ³•è·å–ä»»åŠ¡ç±»åˆ«ã€‚")
            return []

        log_pattern = re.compile(r"-\s+(.+?)\s+â†’")

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šéå†æ—¥å¿—ç›®å½•ä¸‹çš„æ‰€æœ‰ app.log* æ–‡ä»¶ ---
        for filename in os.listdir(LOGS_DIR):
            if filename.startswith("app.log"):
                file_path = os.path.join(LOGS_DIR, filename)
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        for line in f:
                            match = log_pattern.search(line)
                            if match:
                                # strip() ç”¨äºå»é™¤å¯èƒ½å­˜åœ¨çš„å‰åç©ºæ ¼
                                categories.add(match.group(1).strip())
                except Exception as file_error:
                    logging.error(f"âŒ è¯»å–æ—¥å¿—æ–‡ä»¶ '{file_path}' æ—¶å‡ºé”™: {file_error}")
                    # å•ä¸ªæ–‡ä»¶è¯»å–å¤±è´¥ä¸åº”ä¸­æ–­æ•´ä¸ªæµç¨‹
                    continue
        
        # è¿”å›æ’åºåçš„åˆ—è¡¨ä»¥ä¿è¯å‰ç«¯æ˜¾ç¤ºé¡ºåºç¨³å®š
        return sorted(list(categories))
    except Exception as e:
        logging.error(f"âŒ æ‰«ææ—¥å¿—ç±»åˆ«å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ‰«ææ—¥å¿—ç±»åˆ«å¤±è´¥: {e}")
    

# backend/main.py (å‡½æ•°æ›¿æ¢)

@app.delete("/api/logs")
def clear_logs_api():
    LOGS_DIR = "/app/data/logs"
    
    try:
        root_logger = logging.getLogger()
        
        # --- æ ¸å¿ƒä¿®æ”¹ 1: åªå…³é—­å’Œç§»é™¤æ–‡ä»¶å¤„ç†å™¨ ---
        # éå†å¤„ç†å™¨åˆ—è¡¨çš„ä¸€ä¸ªå‰¯æœ¬ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šåœ¨å¾ªç¯ä¸­ä¿®æ”¹åŸåˆ—è¡¨
        handlers_to_remove = []
        for handler in root_logger.handlers:
            # é€šè¿‡æ£€æŸ¥ handler çš„ç±»å‹æ¥ç²¾ç¡®è¯†åˆ«æ–‡ä»¶å¤„ç†å™¨
            if isinstance(handler, logging.handlers.TimedRotatingFileHandler):
                handlers_to_remove.append(handler)

        for handler in handlers_to_remove:
            handler.close()
            root_logger.removeHandler(handler)
        
        # --- æ–‡ä»¶åˆ é™¤é€»è¾‘ä¿æŒä¸å˜ ---
        if not os.path.exists(LOGS_DIR):
            # å³ä½¿ç›®å½•ä¸å­˜åœ¨ï¼Œä¹Ÿåº”è¯¥é‡æ–°æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
            pass
        else:
            cleared_count = 0
            for filename in os.listdir(LOGS_DIR):
                if filename.startswith("app.log"):
                    file_path = os.path.join(LOGS_DIR, filename)
                    try:
                        os.remove(file_path)
                        cleared_count += 1
                    except OSError as e:
                        logging.error(f"âŒ åˆ é™¤æ—¥å¿—æ–‡ä»¶ '{file_path}' å¤±è´¥: {e}", exc_info=True)
            logging.info(f"âœ… æ—¥å¿—å·²æ¸…ç©ºï¼Œå…±åˆ é™¤ {cleared_count} ä¸ªæ—¥å¿—æ–‡ä»¶ã€‚")

        # --- æ ¸å¿ƒä¿®æ”¹ 2: åªé‡æ–°æ·»åŠ æ–‡ä»¶å¤„ç†å™¨ ---
        # åˆ›å»ºä¸€ä¸ªæ–°çš„æ–‡ä»¶å¤„ç†å™¨å¹¶æ·»åŠ åˆ°æ ¹ logger
        log_file_path = os.path.join(LOGS_DIR, "app.log")
        file_handler = logging.handlers.TimedRotatingFileHandler(
            log_file_path, 
            when='D', 
            interval=1, 
            backupCount=14, 
            encoding='utf-8'
        )
        # éœ€è¦ä» log_manager å¯¼å…¥ CustomLogFormatter
        from log_manager import CustomLogFormatter
        file_handler.setFormatter(CustomLogFormatter())
        root_logger.addHandler(file_handler)

        ui_logger.info("æ—¥å¿—æ–‡ä»¶å·²æ¸…ç©ºå¹¶é‡å»ºã€‚", task_category="æ—¥å¿—ç®¡ç†")
        return {"success": True, "message": "æ‰€æœ‰æ—¥å¿—æ–‡ä»¶å·²æ¸…ç©º"}
    
    except Exception as e:
        ui_logger.error(f"âŒ æ¸…ç©ºæ—¥å¿—æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", task_category="æ—¥å¿—ç®¡ç†", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ¸…ç©ºæ—¥å¿—å¤±è´¥: {e}")
    
# backend/main.py (å‡½æ•°æ›¿æ¢)

@app.get("/api/media/libraries")
def get_libraries_api():
    config = app_config.load_app_config()
    server_conf = config.server_config
    if not all([server_conf.server, server_conf.api_key, server_conf.user_id]): raise HTTPException(status_code=400, detail="EmbyæœåŠ¡å™¨æœªé…ç½®")
    try:
        logging.info(f"æ­£åœ¨ä» {server_conf.server} è·å–åª’ä½“åº“åˆ—è¡¨...")
        url = f"{server_conf.server}/Users/{server_conf.user_id}/Views"
        
        proxy_manager = ProxyManager(config)
        proxies = proxy_manager.get_proxies(url)

        response = requests.get(url, params={"api_key": server_conf.api_key}, timeout=15, proxies=proxies)
        response.raise_for_status()
        views = response.json().get("Items", [])

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šå¢åŠ å¥å£®æ€§ï¼Œå¹¶æ·»åŠ è°ƒè¯•æ—¥å¿— ---
        # ä¸´æ—¶è°ƒè¯•æ—¥å¿—ï¼Œç”¨äºè§‚å¯Ÿæ‰€æœ‰è¿”å›çš„è§†å›¾åŠå…¶ç±»å‹
        for v in views:
            logging.debug(f"ã€åª’ä½“åº“APIè°ƒè¯•ã€‘æ‰¾åˆ°è§†å›¾: \"{v.get('Name')}\", CollectionType: \"{v.get('CollectionType')}\", Type: \"{v.get('Type')}\"")

        # æ›´å¥å£®çš„åª’ä½“åº“è¯†åˆ«é€»è¾‘ï¼š
        # 1. CollectionType åœ¨æˆ‘ä»¬çš„ç™½åå•ä¸­
        # 2. æˆ–è€…ï¼Œå®ƒçš„ Type æ˜¯ 'CollectionFolder' (è¿™æ˜¯åª’ä½“åº“çš„æ ¹æœ¬ç±»å‹)
        valid_collection_types = ["movies", "tvshows", "homevideos", "music", "mixed"]
        libraries = [
            {"id": v["Id"], "name": v["Name"]} 
            for v in views 
            if v.get("CollectionType") in valid_collection_types or v.get("Type") == "CollectionFolder"
        ]
        # --- ä¿®æ”¹ç»“æŸ ---
        
        logging.info(f"æˆåŠŸè·å–åˆ° {len(libraries)} ä¸ªåª’ä½“åº“ã€‚")
        return libraries
    except Exception as e:
        logging.error(f"è·å–åª’ä½“åº“åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–åª’ä½“åº“åˆ—è¡¨æ—¶å‡ºé”™: {e}")
    
@app.post("/api/media/search")
def search_media_api(query: MediaSearchQuery):
    config = app_config.load_app_config()
    server_conf = config.server_config
    if not all([server_conf.server, server_conf.api_key, server_conf.user_id]): raise HTTPException(status_code=400, detail="EmbyæœåŠ¡å™¨æœªé…ç½®")
    logging.info(f"æ­£åœ¨æœç´¢åª’ä½“: '{query.query}'")
    url = f"{server_conf.server}/Users/{server_conf.user_id}/Items"
    params = {"api_key": server_conf.api_key, "Recursive": "true", "IncludeItemTypes": "Movie,Series", "SearchTerm": query.query, "Fields": "ProviderIds,ProductionYear,Genres"}
    try:
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨æ–°çš„ ProxyManager ---
        proxy_manager = ProxyManager(config)
        proxies = proxy_manager.get_proxies(url)
        # --- ç»“æŸä¿®æ”¹ ---

        response = requests.get(url, params=params, timeout=20, proxies=proxies)
        response.raise_for_status()
        items = response.json().get("Items", [])
        if not items and query.query.isdigit():
            try:
                item_url = f"{server_conf.server}/Users/{server_conf.user_id}/Items/{query.query}"
                item_params = { "api_key": server_conf.api_key, "Fields": "ProviderIds,ProductionYear,Genres" }
                
                # --- æ ¸å¿ƒä¿®æ”¹ï¼šä¸ºç¬¬äºŒä¸ªè¯·æ±‚ä¹Ÿåº”ç”¨ä»£ç†é€»è¾‘ ---
                proxies_item = proxy_manager.get_proxies(item_url)
                item_resp = requests.get(item_url, params=item_params, timeout=10, proxies=proxies_item)
                # --- ç»“æŸä¿®æ”¹ ---

                if item_resp.ok: items = [item_resp.json()]
            except Exception: pass
        logging.info(f"æœç´¢åˆ° {len(items)} ä¸ªç»“æœã€‚")
        return items
    except Exception as e:
        logging.error(f"æœç´¢åª’ä½“æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=f"æœç´¢åª’ä½“æ—¶å‡ºé”™: {e}")
@app.get("/api/media/debug-item/{item_id}")
def debug_get_item_details(item_id: str):
    config = app_config.load_app_config()
    server_conf = config.server_config
    if not all([server_conf.server, server_conf.api_key, server_conf.user_id]):
        raise HTTPException(status_code=400, detail="EmbyæœåŠ¡å™¨æœªé…ç½®")
    logging.info(f"ã€è°ƒè¯•ã€‘å¼€å§‹è·å– Item ID: {item_id} çš„å®Œæ•´æ•°æ®...")
    url = f"{server_conf.server}/Users/{server_conf.user_id}/Items/{item_id}"
    params = {"api_key": server_conf.api_key}
    try:
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨æ–°çš„ ProxyManager ---
        proxy_manager = ProxyManager(config)
        proxies = proxy_manager.get_proxies(url)
        # --- ç»“æŸä¿®æ”¹ ---

        response = requests.get(url, params=params, timeout=20, proxies=proxies)
        response.raise_for_status()
        data = response.json()
        logging.info(f"ã€è°ƒè¯•ã€‘æˆåŠŸè·å– Item ID: {item_id} çš„æ•°æ®ã€‚")
        return data
    except requests.RequestException as e:
        logging.error(f"ã€è°ƒè¯•ã€‘è·å– Item ID: {item_id} æ•°æ®æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=f"è¯·æ±‚EmbyæœåŠ¡å™¨å¤±è´¥: {e}")
    except Exception as e:
        logging.error(f"ã€è°ƒè¯•ã€‘è·å– Item ID: {item_id} æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

@app.post("/api/media/download-item")
def download_single_item_api(req: DownloadRequest):
    config = app_config.load_app_config()
    if not config.download_config.download_directory: raise HTTPException(status_code=400, detail="ä¸‹è½½ç›®å½•æœªé…ç½®")
    logging.info(f"æ”¶åˆ°å•é¡¹ä¸‹è½½è¯·æ±‚: Item ID={req.item_id}, å†…å®¹={req.content_types}")
    try:
        downloader = EmbyDownloader(config)
        result = downloader.download_for_item(req.item_id, req.content_types)
        return {"status": "success", "message": "ä¸‹è½½ä»»åŠ¡å®Œæˆ", "details": result}
    except Exception as e:
        logging.error(f"å•é¡¹ä¸‹è½½å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½æ—¶å‘ç”Ÿé”™è¯¯: {e}")
@app.post("/api/media/batch-download")
async def batch_download_api(req: BatchDownloadRequest):
    config = app_config.load_app_config()
    if not config.download_config.download_directory: raise HTTPException(status_code=400, detail="ä¸‹è½½ç›®å½•æœªé…ç½®")
    task_name = f"æ‰¹é‡ä¸‹è½½ ({req.mode})"
    task_id = task_manager.register_task(batch_download_task, task_name, config, req)
    return {"status": "success", "message": "æ‰¹é‡ä¸‹è½½ä»»åŠ¡å·²æˆåŠŸå¯åŠ¨", "task_id": task_id}
@app.get("/api/tasks")
def get_tasks_api(): return task_manager.get_all_tasks()
@app.post("/api/tasks/{task_id}/cancel")
def cancel_task_api(task_id: str):
    if task_manager.cancel_task(task_id): return {"status": "success", "message": f"ä»»åŠ¡ {task_id} æ­£åœ¨å–æ¶ˆä¸­ã€‚"}
    else: raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ {task_id} æœªæ‰¾åˆ°æˆ–å·²ç»“æŸã€‚")
class GenreMapping(BaseModel):
    mapping: Dict[str, str]
class GenrePreviewRequest(BaseModel):
    mapping: Dict[str, str]
    mode: Literal["byType", "byLibrary", "all"]
    media_type: Optional[Literal["Movie", "Series"]] = None
    library_ids: Optional[List[str]] = None
    blacklist: Optional[str] = None
class GenreApplyRequest(BaseModel):
    items_to_apply: List[Dict]

@app.post("/api/media/extract-local")
async def extract_local_media_api(req: LocalExtractRequest):
    config = app_config.load_app_config()
    if not config.download_config.download_directory:
        raise HTTPException(status_code=400, detail="å…¨å±€ä¸‹è½½ç›®å½•æœªé…ç½®ï¼Œè¯·å…ˆåœ¨â€œEmbyé…ç½®â€é¡µé¢è®¾ç½®ã€‚")
    if not os.path.isdir(req.source_path):
        raise HTTPException(status_code=400, detail=f"æŒ‡å®šçš„æºç›®å½• '{req.source_path}' æ— æ•ˆæˆ–ä¸å­˜åœ¨ã€‚")
    
    task_name = f"æœ¬åœ°æå– ({os.path.basename(req.source_path)})"
    task_id = task_manager.register_task(
        extract_local_media_task,
        task_name,
        config,
        req
    )
    return {"status": "success", "message": "æœ¬åœ°æå–ä»»åŠ¡å·²å¯åŠ¨", "task_id": task_id}

@app.get("/api/genres")
def get_all_genres():
    config = app_config.load_app_config()
    logic = GenreLogic(config)
    return logic.get_all_genres()
@app.get("/api/genres/standard-mapping")
def get_standard_genre_mapping():
    config = app_config.load_app_config()
    return getattr(config, 'genre_mapping', {})
@app.post("/api/genres/standard-mapping")
def save_standard_genre_mapping(mapping: GenreMapping):
    try:
        config = app_config.load_app_config()
        config.genre_mapping = mapping.mapping
        app_config.save_app_config(config)
        logging.info("æ ‡å‡†ç±»å‹æ˜ å°„è§„åˆ™å·²ä¿å­˜ã€‚")
        return {"status": "success", "message": "æ˜ å°„è§„åˆ™å·²ä¿å­˜"}
    except Exception as e:
        logging.error(f"ä¿å­˜æ ‡å‡†æ˜ å°„æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=str(e))
@app.post("/api/genres/preview")
def preview_genre_changes(req: GenrePreviewRequest):
    config = app_config.load_app_config()
    logic = GenreLogic(config)
    task_name = f"é¢„è§ˆç±»å‹æ›¿æ¢ ({req.mode})"
    task_id = task_manager.register_task(
        logic.preview_changes_task,
        task_name,
        req.mapping, req.mode, req.media_type, req.library_ids, req.blacklist
    )
    return {"status": "success", "message": "é¢„è§ˆä»»åŠ¡å·²å¯åŠ¨", "task_id": task_id}
@app.post("/api/genres/apply")
def apply_genre_changes(req: GenreApplyRequest):
    config = app_config.load_app_config()
    logic = GenreLogic(config)
    task_name = f"åº”ç”¨ç±»å‹æ›¿æ¢ (å…± {len(req.items_to_apply)} é¡¹)"
    task_id = task_manager.register_task(
        logic.apply_changes_task,
        task_name,
        req.items_to_apply
    )
    return {"status": "success", "message": "åº”ç”¨ç±»å‹æ›¿æ¢ä»»åŠ¡å·²å¯åŠ¨", "task_id": task_id}
@app.post("/api/config/actor-localizer")
def save_actor_localizer_config_api(config: ActorLocalizerConfig):
    try:
        logging.info("æ­£åœ¨ä¿å­˜æ¼”å‘˜ä¸­æ–‡åŒ–è®¾ç½®...")
        current_app_config = app_config.load_app_config()
        current_app_config.actor_localizer_config = config
        app_config.save_app_config(current_app_config)
        
        if scheduler.running:
            job_id = "actor_localizer_apply_job"
            existing_job = scheduler.get_job(job_id)
            if existing_job:
                scheduler.remove_job(job_id)
                logging.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘å·²ç§»é™¤æ—§çš„æ¼”å‘˜ä¸­æ–‡åŒ–è‡ªåŠ¨åº”ç”¨ä»»åŠ¡ã€‚")
            if config.apply_cron:
                try:
                    scheduler.add_job(trigger_actor_localizer_apply, CronTrigger.from_crontab(config.apply_cron), id=job_id, replace_existing=True)
                    logging.info(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘å·²æ›´æ–°æ¼”å‘˜ä¸­æ–‡åŒ–è‡ªåŠ¨åº”ç”¨ä»»åŠ¡ï¼Œæ–°CRONè¡¨è¾¾å¼: '{config.apply_cron}'")
                except Exception as e:
                    logging.error(f"ã€è°ƒåº¦ä»»åŠ¡ã€‘æ›´æ–°æ¼”å‘˜ä¸­æ–‡åŒ–ä»»åŠ¡å¤±è´¥: {e}")
                    raise HTTPException(status_code=400, detail=f"CRONè¡¨è¾¾å¼æ— æ•ˆ: {e}")

        logging.info("æ¼”å‘˜ä¸­æ–‡åŒ–è®¾ç½®ä¿å­˜æˆåŠŸï¼")
        return {"success": True, "message": "æ¼”å‘˜ä¸­æ–‡åŒ–è®¾ç½®å·²ä¿å­˜ï¼"}
    except Exception as e:
        logging.error(f"ä¿å­˜æ¼”å‘˜ä¸­æ–‡åŒ–è®¾ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜æ¼”å‘˜ä¸­æ–‡åŒ–è®¾ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
@app.post("/api/actor-localizer/preview")
def preview_actor_changes_api(req: ActorLocalizerPreviewRequest):
    config = app_config.load_app_config()
    logic = ActorLocalizerLogic(config)
    task_name = f"é¢„è§ˆæ¼”å‘˜ä¸­æ–‡åŒ– ({req.target.scope})"
    task_id = task_manager.register_task(
        logic.preview_actor_changes_task,
        task_name,
        req.target,
        req.config
    )
    return {"status": "success", "message": "æ¼”å‘˜ä¸­æ–‡åŒ–é¢„è§ˆä»»åŠ¡å·²å¯åŠ¨", "task_id": task_id}
@app.post("/api/actor-localizer/apply")
def apply_actor_changes_api(req: ActorLocalizerApplyRequest):
    config = app_config.load_app_config()
    logic = ActorLocalizerLogic(config)
    task_name = f"åº”ç”¨æ¼”å‘˜ä¸­æ–‡åŒ– (å…± {len(req.items)} é¡¹)"
    task_id = task_manager.register_task(
        logic.apply_actor_changes_task,
        task_name,
        req.items
    )
    return {"status": "success", "message": "åº”ç”¨æ¼”å‘˜ä¸­æ–‡åŒ–ä»»åŠ¡å·²å¯åŠ¨", "task_id": task_id}

# backend/main.py (ä¿®æ”¹ apply_actor_changes_directly_api å‡½æ•°)

@app.post("/api/actor-localizer/apply-directly")
def apply_actor_changes_directly_api():
    config = app_config.load_app_config()
    for task in task_manager.get_all_tasks():
        if task['name'].startswith("æ¼”å‘˜ä¸­æ–‡åŒ–"):
            raise HTTPException(status_code=409, detail=f"å·²æœ‰æ¼”å‘˜ä¸­æ–‡åŒ–ä»»åŠ¡(ID: {task['id']})æ­£åœ¨è¿è¡Œï¼Œè¯·å‹¿é‡å¤å¯åŠ¨ã€‚")
    
    logic = ActorLocalizerLogic(config)
    task_name = "æ¼”å‘˜ä¸­æ–‡åŒ–-æ‰‹åŠ¨è§¦å‘è‡ªåŠ¨åº”ç”¨"
    task_id = task_manager.register_task(
        logic.apply_actor_changes_directly_task,
        task_name,
        config.actor_localizer_config,
        task_category=task_name # ä¼ é€’ task_category
    )
    return {"status": "success", "message": "è‡ªåŠ¨åº”ç”¨ä»»åŠ¡å·²å¯åŠ¨", "task_id": task_id}

class TestTranslationRequest(BaseModel):
    mode: Literal["tencent", "siliconflow"]
    config: Dict
@app.post("/api/actor-localizer/test-translation")
def test_translation_api(req: TestTranslationRequest):
    try:
        logic = ActorLocalizerLogic(app_config.load_app_config())
        text_to_test = "John Doe"
        translated_text = ""
        if req.mode == "tencent":
            logging.info("æ­£åœ¨æµ‹è¯•è…¾è®¯äº‘ç¿»è¯‘API...")
            api_config = TencentApiConfig(**req.config)
            translated_text = logic.translate_with_tencent_api(text_to_test, api_config)
        elif req.mode == "siliconflow":
            logging.info("æ­£åœ¨æµ‹è¯•SiliconFlow API...")
            api_config = SiliconflowApiConfig(**req.config)
            translated_text = logic.translate_with_siliconflow_api(text_to_test, api_config)
        if translated_text and translated_text != text_to_test:
            return {"success": True, "message": f"æµ‹è¯•æˆåŠŸï¼ç¿»è¯‘ç»“æœ: '{translated_text}'"}
        else:
            raise Exception("ç¿»è¯‘ç»“æœä¸ºç©ºæˆ–ä¸åŸæ–‡ç›¸åŒï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–APIæƒé™ã€‚")
    except Exception as e:
        logging.error(f"ç¿»è¯‘APIæµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    

@app.get("/api/actor-localizer/media/{item_id}/people", response_model=List[Dict])
def get_media_people(item_id: str):
    try:
        config = app_config.load_app_config()
        logic = ActorLocalizerLogic(config)
        return logic.get_people_for_item(item_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/actor-localizer/suggest-roles", response_model=Dict[str, str])
def suggest_roles(req: SuggestRolesRequest):
    try:
        config = app_config.load_app_config()
        if not config.actor_localizer_config.siliconflow_config.api_key:
            raise HTTPException(status_code=400, detail="å°šæœªé…ç½®AIå¤§æ¨¡å‹API Keyï¼Œæ— æ³•ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚")
        logic = ActorLocalizerLogic(config)
        return logic.suggest_roles_with_ai(req.item_id, req.actor_names, config.actor_localizer_config)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/actor-localizer/update-roles")
def update_roles(req: UpdateRolesRequest):
    task_cat = "æ‰‹åŠ¨æ ¡æ­£-åº”ç”¨"
    try:
        config = app_config.load_app_config()
        logic = ActorLocalizerLogic(config)
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šå…ˆè·å–æ—§æ•°æ®è¿›è¡Œå¯¹æ¯” ---
        ui_logger.info(f"â¡ï¸ æ”¶åˆ°å¯¹åª’ä½“ (ID: {req.item_id}) çš„è§’è‰²æ›´æ–°è¯·æ±‚ï¼Œæ­£åœ¨è·å–å½“å‰æ•°æ®...", task_category=task_cat)
        full_item_json = logic._get_item_details(req.item_id, full_json=True)
        if not full_item_json:
            raise HTTPException(status_code=404, detail=f"æœªèƒ½æ‰¾åˆ°åª’ä½“é¡¹ {req.item_id}")
        
        original_people = full_item_json.get('People', [])
        
        # ç®€å•å¯¹æ¯”ï¼šç›´æ¥æ¯”è¾ƒä¸¤ä¸ªåˆ—è¡¨æ˜¯å¦ç›¸ç­‰ã€‚
        # æ³¨æ„ï¼šè¿™è¦æ±‚å‰ç«¯å‘é€çš„ people å¯¹è±¡ç»“æ„ä¸Embyè¿”å›çš„å®Œå…¨ä¸€è‡´ã€‚
        if original_people == req.people:
            ui_logger.info(f"âœ… æ£€æµ‹åˆ°è§’è‰²åˆ—è¡¨æ— ä»»ä½•å˜æ›´ï¼Œæ— éœ€æ›´æ–°ã€‚", task_category=task_cat)
            return {"success": True, "message": "è§’è‰²åˆ—è¡¨æ— ä»»ä½•å˜æ›´ï¼Œæ— éœ€æ›´æ–°ã€‚"}
        # --- ä¿®æ”¹ç»“æŸ ---

        ui_logger.info(f"ğŸ” æ£€æµ‹åˆ°è§’è‰²åˆ—è¡¨å­˜åœ¨å˜æ›´ï¼Œæ­£åœ¨åº”ç”¨åˆ° Emby...", task_category=task_cat)
        full_item_json['People'] = req.people
        
        if logic._update_item_on_server(req.item_id, full_item_json):
            ui_logger.info(f"âœ… è§’è‰²åå·²æˆåŠŸæ›´æ–°åˆ°Embyï¼", task_category=task_cat)
            return {"success": True, "message": "è§’è‰²åå·²æˆåŠŸæ›´æ–°åˆ°Embyï¼"}
        else:
            raise HTTPException(status_code=500, detail="æ›´æ–°åˆ°Embyæ—¶å‘ç”Ÿé”™è¯¯ã€‚")
    except Exception as e:
        ui_logger.error(f"âŒ æ›´æ–°è§’è‰²æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", task_category=task_cat, exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/api/config/scheduled-tasks")
def save_scheduled_tasks_config_api(config: ScheduledTasksConfig):
    try:
        logging.info("æ­£åœ¨ä¿å­˜é€šç”¨å®šæ—¶ä»»åŠ¡è®¾ç½®...")
        current_app_config = app_config.load_app_config()
        current_app_config.scheduled_tasks_config = config
        app_config.save_app_config(current_app_config)

        if scheduler.running:
            logging.info("ã€è°ƒåº¦ä»»åŠ¡ã€‘æ£€æµ‹åˆ°é…ç½®å˜æ›´ï¼Œæ­£åœ¨æ›´æ–°è°ƒåº¦å™¨...")
            for task in config.tasks:
                job_id = f"scheduled_{task.id}"
                existing_job = scheduler.get_job(job_id)
                if task.enabled and task.cron:
                    try:
                        scheduler.add_job(
                            trigger_scheduled_task,
                            CronTrigger.from_crontab(task.cron),
                            id=job_id,
                            replace_existing=True,
                            args=[task.id]
                        )
                        logging.info(f"  - å·²æ›´æ–°/æ·»åŠ ä»»åŠ¡ '{task.name}' (CRON: {task.cron})")
                    except Exception as e:
                        logging.error(f"  - æ›´æ–°ä»»åŠ¡ '{task.name}' å¤±è´¥: {e}")
                elif existing_job:
                    scheduler.remove_job(job_id)
                    logging.info(f"  - å·²ç§»é™¤ç¦ç”¨çš„ä»»åŠ¡ '{task.name}'")

        logging.info("é€šç”¨å®šæ—¶ä»»åŠ¡è®¾ç½®ä¿å­˜æˆåŠŸï¼")
        return {"success": True, "message": "å®šæ—¶ä»»åŠ¡è®¾ç½®å·²ä¿å­˜ï¼"}
    except Exception as e:
        logging.error(f"ä¿å­˜é€šç”¨å®šæ—¶ä»»åŠ¡è®¾ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜è®¾ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    

    
@app.post("/api/scheduled-tasks/{task_id}/trigger")
def trigger_scheduled_task_once_api(task_id: str):
    task_cat = "API-å®šæ—¶ä»»åŠ¡"
    ui_logger.info(f"æ”¶åˆ°ç«‹å³æ‰§è¡Œä»»åŠ¡çš„è¯·æ±‚: {task_id}", task_category=task_cat)
    config = app_config.load_app_config()
    defined_tasks = {task.id for task in config.scheduled_tasks_config.tasks}
    if task_id not in defined_tasks:
        raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°IDä¸º '{task_id}' çš„å®šæ—¶ä»»åŠ¡å®šä¹‰ã€‚")

    task_name_map = {
        "actor_localizer": "æ¼”å‘˜ä¸­æ–‡åŒ–",
        "douban_fixer": "è±†ç“£IDä¿®å¤å™¨"
    }
    task_display_name = task_name_map.get(task_id)
    if task_display_name:
        for task in task_manager.get_all_tasks():
            if task['name'].startswith(f"å®šæ—¶ä»»åŠ¡-{task_display_name}"):
                 raise HTTPException(status_code=409, detail=f"å·²æœ‰åŒç±»å®šæ—¶ä»»åŠ¡(ID: {task['id']})æ­£åœ¨è¿è¡Œï¼Œè¯·å‹¿é‡å¤å¯åŠ¨ã€‚")

    try:
        trigger_scheduled_task(task_id)
        return {"success": True, "message": f"ä»»åŠ¡ '{task_display_name or task_id}' å·²æˆåŠŸè§¦å‘ï¼Œè¯·åœ¨â€œè¿è¡Œä»»åŠ¡â€é¡µé¢æŸ¥çœ‹è¿›åº¦ã€‚"}
    except Exception as e:
        ui_logger.error(f"æ‰‹åŠ¨è§¦å‘å®šæ—¶ä»»åŠ¡ '{task_id}' æ—¶å¤±è´¥: {e}", task_category=task_cat, exc_info=True)
        raise HTTPException(status_code=500, detail=f"è§¦å‘ä»»åŠ¡æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}")

@app.post("/api/config/douban-poster-updater")
def save_douban_poster_updater_config_api(config: DoubanPosterUpdaterConfig):
    try:
        logging.info("æ­£åœ¨ä¿å­˜è±†ç“£æµ·æŠ¥æ›´æ–°å™¨è®¾ç½®...")
        current_app_config = app_config.load_app_config()
        current_app_config.douban_poster_updater_config = config
        app_config.save_app_config(current_app_config)
        logging.info("è±†ç“£æµ·æŠ¥æ›´æ–°å™¨è®¾ç½®ä¿å­˜æˆåŠŸï¼")
        return {"success": True, "message": "è®¾ç½®å·²ä¿å­˜ï¼"}
    except Exception as e:
        logging.error(f"ä¿å­˜è±†ç“£æµ·æŠ¥æ›´æ–°å™¨è®¾ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜è®¾ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    

@app.post("/api/config/webhook")
def save_webhook_config_api(config: WebhookConfig):
    try:
        logging.info("æ­£åœ¨ä¿å­˜ Webhook è®¾ç½®...")
        current_app_config = app_config.load_app_config()
        current_app_config.webhook_config = config
        app_config.save_app_config(current_app_config)
        logging.info("Webhook è®¾ç½®ä¿å­˜æˆåŠŸï¼")
        return {"success": True, "message": "è®¾ç½®å·²ä¿å­˜ï¼"}
    except Exception as e:
        logging.error(f"ä¿å­˜ Webhook è®¾ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜è®¾ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")

@app.post("/api/config/episode-refresher")
def save_episode_refresher_config_api(config: EpisodeRefresherConfig):
    try:
        logging.info("æ­£åœ¨ä¿å­˜å‰§é›†å…ƒæ•°æ®åˆ·æ–°å™¨è®¾ç½®...")
        current_app_config = app_config.load_app_config()
        current_app_config.episode_refresher_config = config
        app_config.save_app_config(current_app_config)
        logging.info("å‰§é›†å…ƒæ•°æ®åˆ·æ–°å™¨è®¾ç½®ä¿å­˜æˆåŠŸï¼")
        return {"success": True, "message": "è®¾ç½®å·²ä¿å­˜ï¼"}
    except Exception as e:
        logging.error(f"ä¿å­˜å‰§é›†å…ƒæ•°æ®åˆ·æ–°å™¨è®¾ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜è®¾ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
class ScreenshotBackupRequest(BaseModel):
    scope: ScheduledTasksTargetScope
    config: EpisodeRefresherConfig

@app.post("/api/episode-refresher/backup-screenshots")
def backup_screenshots_api(req: ScreenshotBackupRequest):
    task_cat = "API-æˆªå›¾å¤‡ä»½"
    ui_logger.info(f"æ”¶åˆ°æˆªå›¾å¤‡ä»½è¯·æ±‚ï¼ŒèŒƒå›´: {req.scope.mode}", task_category=task_cat)
    
    config = app_config.load_app_config()
    # ä½¿ç”¨è¯·æ±‚ä¸­ä¸´æ—¶çš„é…ç½®
    config.episode_refresher_config = req.config
    
    selector = MediaSelector(config)
    series_ids = selector.get_item_ids(req.scope, target_collection_type="tvshows")

    if not series_ids:
        raise HTTPException(status_code=404, detail="åœ¨æŒ‡å®šèŒƒå›´å†…æœªæ‰¾åˆ°ä»»ä½•å‰§é›†ã€‚")

    task_name = f"æ‰‹åŠ¨å¤‡ä»½æˆªå›¾ ({req.scope.mode})"
    task_id = task_manager.register_task(
        _episode_screenshot_backup_task_runner,
        task_name,
        series_ids=series_ids,
        config=config,
        task_name=task_name
    )
    return {"status": "success", "message": "æˆªå›¾å¤‡ä»½ä»»åŠ¡å·²å¯åŠ¨ã€‚", "task_id": task_id}

class GitHubBackupRequest(BaseModel):
    config: EpisodeRefresherConfig

@app.post("/api/episode-refresher/backup-to-github")
def backup_screenshots_to_github_api(req: GitHubBackupRequest):
    task_cat = "API-å¤‡ä»½åˆ°GitHub"
    ui_logger.info(f"æ”¶åˆ°å¤‡ä»½æˆªå›¾åˆ° GitHub çš„è¯·æ±‚...", task_category=task_cat)
    
    config = app_config.load_app_config()
    # ä½¿ç”¨è¯·æ±‚ä¸­ä¸´æ—¶çš„é…ç½®
    config.episode_refresher_config = req.config
    
    logic = EpisodeRefresherLogic(config)
    task_name = "å¤‡ä»½æˆªå›¾åˆ° GitHub"
    task_id = task_manager.register_task(
        logic.backup_screenshots_to_github_task,
        task_name,
        config=config.episode_refresher_config
    )
    return {"status": "success", "message": "å¤‡ä»½åˆ° GitHub çš„ä»»åŠ¡å·²å¯åŠ¨ã€‚", "task_id": task_id}
    
@app.post("/api/config/episode-renamer")
def save_episode_renamer_config_api(config: EpisodeRenamerConfig):
    try:
        logging.info("æ­£åœ¨ä¿å­˜å‰§é›†æ–‡ä»¶é‡å‘½åå™¨è®¾ç½®...")
        current_app_config = app_config.load_app_config()
        current_app_config.episode_renamer_config = config
        app_config.save_app_config(current_app_config)
        logging.info("å‰§é›†æ–‡ä»¶é‡å‘½åå™¨è®¾ç½®ä¿å­˜æˆåŠŸï¼")
        return {"success": True, "message": "è®¾ç½®å·²ä¿å­˜ï¼"}
    except Exception as e:
        logging.error(f"ä¿å­˜å‰§é›†æ–‡ä»¶é‡å‘½åå™¨è®¾ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜è®¾ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")


from models import TelegramConfig
from notification_manager import notification_manager, escape_markdown

@app.post("/api/config/telegram")
def save_telegram_config_api(config: TelegramConfig):
    """ä¿å­˜ Telegram é€šçŸ¥é…ç½®"""
    try:
        ui_logger.info("æ­£åœ¨ä¿å­˜ Telegram é€šçŸ¥è®¾ç½®...", task_category="ç³»ç»Ÿé…ç½®")
        current_app_config = app_config.load_app_config()
        current_app_config.telegram_config = config
        app_config.save_app_config(current_app_config)
        ui_logger.info("âœ… Telegram é€šçŸ¥è®¾ç½®ä¿å­˜æˆåŠŸï¼", task_category="ç³»ç»Ÿé…ç½®")
        return {"success": True, "message": "è®¾ç½®å·²ä¿å­˜ï¼"}
    except Exception as e:
        logging.error(f"ä¿å­˜ Telegram è®¾ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜è®¾ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}")

@app.post("/api/notification/test-telegram")
def test_telegram_api(config: TelegramConfig):
    """æµ‹è¯•å‘é€ä¸€æ¡ Telegram æ¶ˆæ¯"""
    # --- æ ¸å¿ƒä¿®æ”¹ï¼šåŠ è½½å®Œæ•´é…ç½®ï¼Œå¹¶å°†ä¸´æ—¶æµ‹è¯•é…ç½®è¦†ç›–ä¸Šå» ---
    current_app_config = app_config.load_app_config()
    current_app_config.telegram_config = config # ä½¿ç”¨ç”¨æˆ·æ­£åœ¨æµ‹è¯•çš„é…ç½®
    
    raw_message = "ğŸ‰ è¿™æ˜¯ä¸€æ¡æ¥è‡ª Emby-Toolkit çš„æµ‹è¯•æ¶ˆæ¯ï¼\nå¦‚æœèƒ½çœ‹åˆ°æˆ‘ï¼Œè¯´æ˜æ‚¨çš„é€šçŸ¥é…ç½®æ­£ç¡®æ— è¯¯ã€‚"
    test_message = escape_markdown(raw_message)
    
    result = notification_manager.send_telegram_message(test_message, current_app_config)
    # --- ä¿®æ”¹ç»“æŸ ---
    if result["success"]:
        return result
    else:
        raise HTTPException(status_code=500, detail=result["message"])

@app.get("/api/episodes/{series_id}")
def get_series_episodes(series_id: str):
    """è·å–æŒ‡å®šå‰§é›†çš„æ‰€æœ‰åˆ†é›†ä¿¡æ¯"""
    task_cat = "API-å‰§é›†åˆ·æ–°"
    try:
        config = app_config.load_app_config()
        session = requests.Session()
        episodes_url = f"{config.server_config.server}/Items"
        episodes_params = {
            "api_key": config.server_config.api_key,
            "ParentId": series_id,
            "IncludeItemTypes": "Episode",
            "Recursive": "true",
            "Fields": "Id,Name,IndexNumber,ParentIndexNumber,SeriesName,ProviderIds"
        }
        response = session.get(episodes_url, params=episodes_params, timeout=30)
        response.raise_for_status()
        return response.json().get("Items", [])
    except Exception as e:
        ui_logger.error(f"è·å–å‰§é›† {series_id} çš„åˆ†é›†åˆ—è¡¨æ—¶å¤±è´¥: {e}", task_category=task_cat, exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
    
@app.get("/api/episode-refresher/series/{series_id}/local-screenshots")
def get_local_screenshots_for_series_api(series_id: str):
    """è·å–æŒ‡å®šå‰§é›†åœ¨æœ¬åœ°å­˜åœ¨æˆªå›¾çš„åˆ†é›†åˆ—è¡¨"""
    task_cat = "API-å‰§é›†åˆ·æ–°"
    try:
        config = app_config.load_app_config()
        logic = EpisodeRefresherLogic(config)
        return logic.get_local_screenshots_for_series(series_id)
    except Exception as e:
        ui_logger.error(f"è·å–å‰§é›† {series_id} çš„æœ¬åœ°æˆªå›¾åˆ—è¡¨æ—¶å¤±è´¥: {e}", task_category=task_cat, exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/episode-refresher/precise-upload-from-local")
def precise_upload_from_local_api(req: PreciseScreenshotUpdateRequest):
    """å¯åŠ¨ä¸€ä¸ªä»»åŠ¡ï¼Œç”¨äºç²¾å‡†ä¸Šä¼ æœ¬åœ°å·²æœ‰çš„æˆªå›¾"""
    task_cat = "API-ç²¾å‡†è¦†ç›–"
    ui_logger.info(f"æ”¶åˆ°å¯¹å‰§é›†ã€Š{req.series_name}ã€‹çš„ç²¾å‡†ä¸Šä¼ è¯·æ±‚...", task_category=task_cat)
    
    config = app_config.load_app_config()
    # ä½¿ç”¨è¯·æ±‚ä¸­ä¸´æ—¶çš„é…ç½®
    config.episode_refresher_config = req.config
    
    logic = EpisodeRefresherLogic(config)
    task_name = f"ç²¾å‡†è¦†ç›–æˆªå›¾ - {req.series_name}"
    task_id = task_manager.register_task(
        logic.precise_upload_from_local_task,
        task_name,
        series_tmdb_id=req.series_tmdb_id,
        series_name=req.series_name,
        episodes=req.episodes,
        config=req.config
    )
    return {"status": "success", "message": "ç²¾å‡†è¦†ç›–ä»»åŠ¡å·²å¯åŠ¨ã€‚", "task_id": task_id}

@app.get("/api/episode-refresher/github-delete-log")
def get_github_delete_log_api():
    """è·å–å¾…åˆ é™¤æˆªå›¾çš„æ—¥å¿—ï¼Œå¹¶èšåˆEmbyä¿¡æ¯"""
    try:
        config = app_config.load_app_config()
        logic = EpisodeRefresherLogic(config)
        return logic.get_github_delete_log()
    except Exception as e:
        ui_logger.error(f"è·å–å¾…åˆ é™¤æ—¥å¿—å¤±è´¥: {e}", task_category="API-è¿œç¨‹æ¸…ç†", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/episode-refresher/github-delete-log")
def save_github_delete_log_api(payload: Dict):
    """ä¿å­˜ç”¨æˆ·å®¡æ ¸åçš„å¾…åˆ é™¤æ—¥å¿—"""
    try:
        config = app_config.load_app_config()
        logic = EpisodeRefresherLogic(config)
        if logic.save_github_delete_log(payload):
            return {"status": "success", "message": "å®¡æ ¸çŠ¶æ€å·²ä¿å­˜ï¼"}
        else:
            raise HTTPException(status_code=500, detail="ä¿å­˜æ—¥å¿—æ–‡ä»¶å¤±è´¥ã€‚")
    except Exception as e:
        ui_logger.error(f"ä¿å­˜å¾…åˆ é™¤æ—¥å¿—å¤±è´¥: {e}", task_category="API-è¿œç¨‹æ¸…ç†", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/episode-refresher/cleanup-github")
def cleanup_github_screenshots_api():
    """å¯åŠ¨ä¸€ä¸ªä»»åŠ¡ï¼Œæ¸…ç†è¿œç¨‹ä½œåºŸæˆªå›¾"""
    task_cat = "API-è¿œç¨‹æ¸…ç†"
    ui_logger.info(f"æ”¶åˆ°æ¸…ç†è¿œç¨‹ä½œåºŸæˆªå›¾çš„è¯·æ±‚...", task_category=task_cat)
    
    config = app_config.load_app_config()
    if not config.episode_refresher_config.github_config.personal_access_token:
        raise HTTPException(status_code=400, detail="æœªé…ç½® GitHub PATï¼Œæ— æ³•æ‰§è¡Œæ¸…ç†æ“ä½œã€‚")

    logic = EpisodeRefresherLogic(config)
    task_name = "æ¸…ç†è¿œç¨‹ä½œåºŸæˆªå›¾"
    task_id = task_manager.register_task(
        logic.cleanup_github_screenshots_task,
        task_name,
        config=config.episode_refresher_config
    )
    return {"status": "success", "message": "æ¸…ç†ä»»åŠ¡å·²å¯åŠ¨ã€‚", "task_id": task_id}

# backend/main.py (æ–°å¢è·¯ç”±)

class ScreenshotRestoreRequest(BaseModel):
    scope: ScheduledTasksTargetScope
    overwrite: bool

@app.post("/api/episode-refresher/restore-from-github")
def restore_screenshots_from_github_api(req: ScreenshotRestoreRequest):
    """å¯åŠ¨ä¸€ä¸ªä»»åŠ¡ï¼Œä» GitHub å¤‡ä»½åå‘æ¢å¤æˆªå›¾åˆ° Emby"""
    task_cat = "API-æˆªå›¾æ¢å¤(åå‘)"
    ui_logger.info(f"æ”¶åˆ°ä» GitHub æ¢å¤æˆªå›¾çš„è¯·æ±‚...", task_category=task_cat)
    
    config = app_config.load_app_config()
    logic = EpisodeRefresherLogic(config)
    task_name = f"ä»GitHubæ¢å¤æˆªå›¾ ({req.scope.mode})"
    task_id = task_manager.register_task(
        logic.restore_screenshots_from_github_task,
        task_name,
        scope=req.scope,
        overwrite=req.overwrite
    )
    return {"status": "success", "message": "ä»GitHubæ¢å¤æˆªå›¾çš„ä»»åŠ¡å·²å¯åŠ¨ã€‚", "task_id": task_id}

@app.post("/api/webhook/emby")
async def emby_webhook_receiver(payload: EmbyWebhookPayload):
    try:
        payload_json_str = payload.model_dump_json(indent=4, exclude_unset=True)
        logging.info(f"ã€Webhookè°ƒè¯•ã€‘æ”¶åˆ°æ¥è‡ª Emby çš„å®Œæ•´ Payload å†…å®¹:\n{payload_json_str}")
    except Exception as e:
        logging.error(f"ã€Webhookè°ƒè¯•ã€‘æ‰“å° Payload æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    task_cat = "Webhook"
    ui_logger.info(f"â¡ï¸ æ”¶åˆ°æ¥è‡ª Emby çš„é€šçŸ¥ï¼Œäº‹ä»¶: {payload.Event}", task_category=task_cat)
    
    config = app_config.load_app_config()
    if not config.webhook_config.enabled:
        ui_logger.info("ã€è·³è¿‡ã€‘Webhook åŠŸèƒ½æœªå¯ç”¨ï¼Œå¿½ç•¥æœ¬æ¬¡é€šçŸ¥ã€‚", task_category=task_cat)
        return {"status": "skipped", "message": "Webhook processing is disabled."}

    if not payload.Item:
        ui_logger.info("ã€è·³è¿‡ã€‘æ”¶åˆ°çš„é€šçŸ¥ä¸­ä¸åŒ…å«æœ‰æ•ˆçš„ Item ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯æµ‹è¯•é€šçŸ¥æˆ–æ— å…³äº‹ä»¶ã€‚", task_category=task_cat)
        return {"status": "success_test_skipped", "message": "Test notification received successfully."}

    target_item_id = None
    target_item_name = payload.Item.Name
    target_item_type = payload.Item.Type

    if payload.Event in ["item.add", "library.new"]:
        if target_item_type in ["Movie", "Series"]:
            target_item_id = payload.Item.Id
            ui_logger.info(f"  - [ä¸»æµç¨‹] æ£€æµ‹åˆ°æ–° [ç”µå½±/å‰§é›†] å…¥åº“: ã€{target_item_name}ã€‘ (ID: {target_item_id})", task_category=task_cat)
        
        elif target_item_type == "Episode":
            episode_id = payload.Item.Id
            ui_logger.info(f"  - [åˆ†é›†æµç¨‹] æ£€æµ‹åˆ°æ–° [å‰§é›†åˆ†é›†] å…¥åº“: ã€{target_item_name}ã€‘ (ID: {episode_id})", task_category=task_cat)
            
            try:
                server_conf = config.server_config
                url = f"{server_conf.server}/Users/{server_conf.user_id}/Items/{episode_id}"
                params = {"api_key": server_conf.api_key, "Fields": "SeriesId,SeriesName"}
                response = requests.get(url, params=params, timeout=15)
                response.raise_for_status()
                episode_details = response.json()
                
                series_id = episode_details.get("SeriesId")
                series_name = episode_details.get("SeriesName")

                if series_id and series_name:
                    ui_logger.info(f"  - [åˆ†é›†æµç¨‹] æˆåŠŸæ‰¾åˆ°æ‰€å±å‰§é›†: ã€{series_name}ã€‘ (ID: {series_id})", task_category=task_cat)
                    
                    # --- æ ¸å¿ƒæ–°å¢ï¼šäº‹ä»¶æ”¶é›†é€»è¾‘ ---
                    with episode_sync_queue_lock:
                        if series_id not in episode_sync_queue:
                            episode_sync_queue[series_id] = {
                                "episode_ids": set(),
                                "series_name": series_name,
                                "last_update": 0
                            }
                            ui_logger.info(f"    - [æ”¶é›†å™¨] é¦–æ¬¡è®°å½•å‰§é›†ã€Š{series_name}ã€‹ï¼Œå·²åˆ›å»ºæ–°çš„åŒæ­¥é˜Ÿåˆ—ã€‚", task_category=task_cat)
                        
                        episode_sync_queue[series_id]["episode_ids"].add(episode_id)
                        episode_sync_queue[series_id]["last_update"] = time.time()
                        
                        queue_size = len(episode_sync_queue[series_id]["episode_ids"])
                        ui_logger.info(f"    - [æ”¶é›†å™¨] å·²å°†åˆ†é›† {episode_id} æ·»åŠ åˆ°é˜Ÿåˆ—ã€‚å‰§é›†ã€Š{series_name}ã€‹å½“å‰å¾…åŒæ­¥åˆ†é›†æ•°: {queue_size}ã€‚é™é»˜å€’è®¡æ—¶å·²é‡ç½®ã€‚", task_category=task_cat)
                    # --- æ ¸å¿ƒæ–°å¢ç»“æŸ ---

                    # ä¸»æµç¨‹ä¾ç„¶åªå¤„ç†å‰§é›†ID
                    target_item_id = series_id
                    target_item_name = series_name
                else:
                    ui_logger.warning(f"  - âš ï¸ [åˆ†é›†æµç¨‹] æ— æ³•ä»åˆ†é›†ã€{target_item_name}ã€‘ä¸­æ‰¾åˆ°æ‰€å±å‰§é›†çš„IDï¼Œè·³è¿‡å¤„ç†ã€‚", task_category=task_cat)

            except requests.RequestException as e:
                ui_logger.error(f"  - âŒ [åˆ†é›†æµç¨‹] æŸ¥è¯¢å‰§é›†è¯¦æƒ…å¤±è´¥: {e}ï¼Œè·³è¿‡å¤„ç†ã€‚", task_category=task_cat)
        
    if target_item_id:
        if target_item_id in webhook_processing_set:
            ui_logger.info(f"  - [ä¸»æµç¨‹-è·³è¿‡] ä»»åŠ¡ã€{target_item_name}ã€‘(ID: {target_item_id}) å·²åœ¨ä¸»å¤„ç†é˜Ÿåˆ—ä¸­ï¼Œæœ¬æ¬¡é€šçŸ¥çš„ä¸»æµç¨‹éƒ¨åˆ†è·³è¿‡ã€‚", task_category=task_cat)
            return {"status": "skipped_in_queue", "message": "Task is already in the processing queue."}
        
        await webhook_queue.put((target_item_id, target_item_name))
        webhook_processing_set.add(target_item_id)
        ui_logger.info(f"  - [ä¸»æµç¨‹-å…¥é˜Ÿ] å·²å°†ä»»åŠ¡ã€{target_item_name}ã€‘(ID: {target_item_id}) æ·»åŠ åˆ°ä¸»å¤„ç†é˜Ÿåˆ—ã€‚å½“å‰é˜Ÿåˆ—é•¿åº¦: {webhook_queue.qsize()}", task_category=task_cat)
        
        return {"status": "success_queued", "message": f"Task for item {target_item_id} has been queued."}
    
    ui_logger.info(f"ã€è·³è¿‡ã€‘äº‹ä»¶ '{payload.Event}' æˆ–ç±»å‹ '{target_item_type}' æ— éœ€å¤„ç†ã€‚", task_category=task_cat)
    return {"status": "skipped", "message": "Event not applicable"}