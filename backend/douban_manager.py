

import os
import json
import logging
import threading
from typing import List
from datetime import datetime
from task_manager import TaskManager
import config as app_config
from models import DoubanCacheStatus
from log_manager import ui_logger

DOUBAN_CACHE_FILE = os.path.join('/app/data', 'douban_data.json')

def _parse_folder_name(folder_name):
    douban_id = 'N/A'
    imdb_id = 'N/A'
    if '_' in folder_name:
        parts = folder_name.split('_')
        db_part = parts[0]
        imdb_part = parts[1] if len(parts) > 1 else ''
        if db_part.isdigit() and db_part != '0':
            douban_id = db_part
        if imdb_part.startswith('tt'):
            imdb_id = imdb_part
    elif folder_name.isdigit():
        douban_id = folder_name
    return douban_id, imdb_id

# backend/douban_manager.py (å‡½æ•°æ›¿æ¢)

def scan_douban_directory_task(directory: str, extra_fields: List[str], cancellation_event: threading.Event, task_id: str, task_manager: TaskManager):
    task_cat = "è±†ç“£æ‰«æ"
    ui_logger.info("â¡ï¸ ã€æ­¥éª¤ 1/5ã€‘ä»»åŠ¡å¯åŠ¨ï¼Œå¼€å§‹æ‰«æè±†ç“£æ•°æ®ç›®å½•...", task_category=task_cat)
    
    movie_dir = os.path.join(directory, 'douban-movies')
    series_dir = os.path.join(directory, 'douban-tv')
    
    media_folders = []
    if os.path.isdir(movie_dir):
        for folder in os.listdir(movie_dir):
            full_path = os.path.join(movie_dir, folder)
            if os.path.isdir(full_path):
                media_folders.append({'path': full_path, 'type': 'Movie'})
    
    if os.path.isdir(series_dir):
        for folder in os.listdir(series_dir):
            full_path = os.path.join(series_dir, folder)
            if os.path.isdir(full_path):
                media_folders.append({'path': full_path, 'type': 'TVShow'})

    total_folders = len(media_folders)
    task_manager.update_task_progress(task_id, 0, total_folders)
    
    ui_logger.info(f"âœ… ã€æ­¥éª¤ 2/5ã€‘ç›®å½•æ‰«æå®Œæˆï¼Œå…±æ‰¾åˆ° {total_folders} ä¸ªåª’ä½“æ–‡ä»¶å¤¹ã€‚", task_category=task_cat)
    ui_logger.info("â¡ï¸ ã€æ­¥éª¤ 3/5ã€‘å¼€å§‹è§£æå…ƒæ•°æ®æ–‡ä»¶ï¼Œè¿‡ç¨‹ä¸­çš„è¯¦ç»†æ—¥å¿—å°†å†™å…¥åç«¯æ—¥å¿—æ–‡ä»¶...", task_category=task_cat)
    
    final_data = {}
    found_count = 0
    skipped_no_json_count = 0
    skipped_no_id_count = 0
    error_json_count = 0
    error_unknown_count = 0

    for i, folder_info in enumerate(media_folders):
        if cancellation_event.is_set():
            ui_logger.warning("âš ï¸ ä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆã€‚", task_category=task_cat)
            return

        folder_path = folder_info['path']
        folder_name = os.path.basename(folder_path)
        media_type = folder_info['type']
        
        # ä»…åœ¨è¿›åº¦æ¡æ›´æ–°æ—¶ä½¿ç”¨ logging.debug è®°å½•è¯¦ç»†ä¿¡æ¯åˆ°åç«¯æ—¥å¿—
        if (i + 1) % 100 == 0 or (i + 1) == total_folders:
            logging.debug(f"ã€è±†ç“£æ‰«æã€‘è¿›åº¦ {i+1}/{total_folders}: æ­£åœ¨å¤„ç†ã€{folder_name}ã€‘")
        
        task_manager.update_task_progress(task_id, i + 1, total_folders)

        json_filename = 'all.json' if media_type == 'Movie' else 'series.json'
        json_path = os.path.join(folder_path, json_filename)

        if not os.path.isfile(json_path):
            logging.warning(f"ã€è±†ç“£æ‰«æ-è·³è¿‡ã€‘åœ¨ç›®å½•ã€{folder_path}ã€‘ä¸­æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶ {json_filename}ã€‚")
            skipped_no_json_count += 1
            continue

        try:
            douban_id, imdb_id = _parse_folder_name(folder_name)
            if douban_id == 'N/A':
                logging.warning(f"ã€è±†ç“£æ‰«æ-è·³è¿‡ã€‘æ— æ³•ä»æ–‡ä»¶å¤¹åã€{folder_name}ã€‘è§£æå‡ºè±†ç“£IDã€‚")
                skipped_no_id_count += 1
                continue

            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)


            #ä¸‹æ–¹genresåç§»é™¤ç®€ä»‹  'intro': data.get('intro', ''),
            item_data = {
                'type': media_type,
                'title': data.get('title', 'N/A'),
                'year': data.get('year', ''),
                'genres': data.get('genres', []),
                'pic': data.get('pic', {}),
                'actors': [
                    {
                        'id': actor.get('id'),
                        'name': actor.get('name'),
                        'latin_name': actor.get('latin_name'),
                        'character': actor.get('character'),
                        'avatar': actor.get('avatar', {})
                    } for actor in data.get('actors', [])
                ],
                'imdb_id': imdb_id,
                'countries': data.get('countries', [])
            }

            if 'rating' in extra_fields:
                item_data['rating'] = data.get('rating', {}).get('value')
            if 'pubdate' in extra_fields:
                item_data['pubdate'] = data.get('pubdate', [])
            if 'card_subtitle' in extra_fields:
                item_data['card_subtitle'] = data.get('card_subtitle', '')
            if 'languages' in extra_fields:
                item_data['languages'] = data.get('languages', [])
            if 'durations' in extra_fields and media_type == 'Movie':
                item_data['durations'] = data.get('durations', [])
            
            final_data[douban_id] = item_data
            found_count += 1
        except json.JSONDecodeError:
            logging.error(f"ã€è±†ç“£æ‰«æ-é”™è¯¯ã€‘è§£æJSONæ–‡ä»¶å¤±è´¥: {json_path}")
            error_json_count += 1
        except Exception as e:
            logging.error(f"ã€è±†ç“£æ‰«æ-é”™è¯¯ã€‘å¤„ç†æ–‡ä»¶å¤¹ã€{folder_path}ã€‘æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            error_unknown_count += 1

    ui_logger.info(f"âœ… ã€æ­¥éª¤ 4/5ã€‘å…ƒæ•°æ®è§£æå®Œæˆï¼", task_category=task_cat)
    # --- æ±‡æ€»æŠ¥å‘Š ---
    ui_logger.info(f"""
    - - - - - - - - æ‰«æç»“æœæ±‡æ€» - - - - - - - -
    âœ… æˆåŠŸè§£æ: {found_count} æ¡
    âš ï¸ è·³è¿‡ (ç¼ºå°‘å…ƒæ•°æ®æ–‡ä»¶): {skipped_no_json_count} æ¡
    âš ï¸ è·³è¿‡ (æ— æ³•è§£æID): {skipped_no_id_count} æ¡
    âŒ å¤±è´¥ (JSONæ ¼å¼é”™è¯¯): {error_json_count} æ¡
    âŒ å¤±è´¥ (å…¶ä»–æœªçŸ¥é”™è¯¯): {error_unknown_count} æ¡
    - - - - - - - - - - - - - - - - - - - - - - -
    """, task_category=task_cat)
    
    try:
        cache_dir = os.path.dirname(DOUBAN_CACHE_FILE)
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir, exist_ok=True)

        old_data_keys = set()
        if os.path.exists(DOUBAN_CACHE_FILE):
            try:
                with open(DOUBAN_CACHE_FILE, 'r', encoding='utf-8') as f:
                    old_data = json.load(f)
                old_data_keys = set(old_data.keys())
            except (IOError, json.JSONDecodeError):
                ui_logger.warning("âš ï¸ è¯»å–æ—§ç¼“å­˜æ–‡ä»¶å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œæ•°æ®å¯¹æ¯”ã€‚", task_category=task_cat)

        new_data_keys = set(final_data.keys())
        added_count = len(new_data_keys - old_data_keys)
        removed_count = len(old_data_keys - new_data_keys)
        
        if old_data_keys:
            ui_logger.info(f"ğŸ”„ æ•°æ®å¯¹æ¯”ï¼šæ–°å¢ {added_count} æ¡ï¼Œç§»é™¤ {removed_count} æ¡ã€‚", task_category=task_cat)

        ui_logger.info(f"â¡ï¸ ã€æ­¥éª¤ 5/5ã€‘æ­£åœ¨å°† {found_count} æ¡æ•°æ®å†™å…¥ç¼“å­˜æ–‡ä»¶...", task_category=task_cat)
        with open(DOUBAN_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=4)
        ui_logger.info("âœ… ã€æ­¥éª¤ 5/5ã€‘ç¼“å­˜æ–‡ä»¶å†™å…¥æˆåŠŸï¼", task_category=task_cat)

        config = app_config.load_app_config()
        mtime = os.path.getmtime(DOUBAN_CACHE_FILE)
        last_modified = datetime.fromtimestamp(mtime).isoformat()
        
        config.douban_cache_status = DoubanCacheStatus(
            exists=True,
            item_count=found_count,
            last_modified=last_modified,
            is_scanning=False
        )
        app_config.save_app_config(config)

        logging.info("ã€è±†ç“£æ‰«æã€‘å·²æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„ç¼“å­˜çŠ¶æ€ã€‚")

    except Exception as e:
        ui_logger.error(f"âŒ å†™å…¥ç¼“å­˜æˆ–æ›´æ–°é…ç½®å¤±è´¥: {e}", task_category=task_cat, exc_info=True)

    return {"found_count": found_count}